#!/usr/bin/env python3
"""
ì§ì ‘ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ - ë¡œë“œëœ ëª¨ë¸ë“¤ì„ ì§ì ‘ í…ŒìŠ¤íŠ¸
"""
import sys
import os
sys.path.append('/home/hong/code/huggingface-gui')

from model_manager import MultiModelManager

def test_loaded_models():
    """ë¡œë“œëœ ëª¨ë¸ë“¤ì„ ì§ì ‘ í…ŒìŠ¤íŠ¸"""
    # Model manager ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    manager = MultiModelManager()
    
    # í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ë“¤ í™•ì¸
    loaded_models = manager.get_loaded_models()
    print(f"ğŸ” ë¡œë“œëœ ëª¨ë¸ë“¤: {loaded_models}")
    
    if not loaded_models:
        print("âŒ ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    for model_name in loaded_models:
        print(f"\nğŸ§ª {model_name} í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        # ëª¨ë¸ ì •ë³´ í™•ì¸
        model_info = manager.get_model_info(model_name)
        if not model_info:
            print(f"âŒ {model_name} ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue
            
        print(f"   ìƒíƒœ: {model_info.status}")
        print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {model_info.memory_usage:.1f}MB")
        
        # ì¶”ë¡ ìš© ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ê°€ì ¸ì˜¤ê¸°
        model_tokenizer = manager.get_model_for_inference(model_name)
        if not model_tokenizer:
            print(f"âŒ {model_name} ì¶”ë¡ ìš© ê°ì²´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue
            
        model, tokenizer = model_tokenizer
        print(f"   ëª¨ë¸ íƒ€ì…: {type(model)}")
        print(f"   í† í¬ë‚˜ì´ì € íƒ€ì…: {type(tokenizer)}")
        
        # ì§€ì›í•˜ëŠ” íƒœìŠ¤í¬ í™•ì¸
        available_tasks = manager.get_available_tasks(model_name)
        print(f"   ì§€ì› íƒœìŠ¤í¬: {available_tasks}")
        
        # ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
        try:
            test_text = "This is a test sentence."
            
            if hasattr(tokenizer, '__call__'):
                # í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸
                tokens = tokenizer(test_text, return_tensors="pt", padding=True, truncation=True)
                print(f"   âœ… í† í¬ë‚˜ì´ì € ì‘ë™: input_ids shape = {tokens['input_ids'].shape}")
                
                # ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸  
                if hasattr(model, 'forward') or hasattr(model, '__call__'):
                    with torch.no_grad():
                        output = model(**tokens)
                        if hasattr(output, 'logits'):
                            print(f"   âœ… ëª¨ë¸ ì¶”ë¡  ì„±ê³µ: logits shape = {output.logits.shape}")
                        else:
                            print(f"   âœ… ëª¨ë¸ ì¶”ë¡  ì„±ê³µ: output type = {type(output)}")
                else:
                    print(f"   âŒ ëª¨ë¸ì— forward ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                print(f"   âŒ í† í¬ë‚˜ì´ì €ê°€ í˜¸ì¶œ ê°€ëŠ¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"   âŒ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    print(f"\nğŸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

if __name__ == "__main__":
    import torch
    test_loaded_models()
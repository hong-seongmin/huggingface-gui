import os
import streamlit as st
import streamlit.components.v1 as components
from huggingface_hub import HfApi, scan_cache_dir
import pandas as pd
from st_aggrid import AgGrid, GridOptionsBuilder, GridUpdateMode, DataReturnMode
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import time
import json
import logging
from datetime import datetime

# ë¡œê¹… ì„¤ì • (watchdog ë“± ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê·¸ ì œì™¸)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app_debug.log', mode='w'),  # íŒŒì¼ ë®ì–´ì“°ê¸°
        logging.StreamHandler()
    ]
)

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê·¸ ë ˆë²¨ ì¡°ì •
logging.getLogger('watchdog').setLevel(logging.WARNING)
logging.getLogger('streamlit').setLevel(logging.WARNING)
logging.getLogger('urllib3').setLevel(logging.WARNING)
logging.getLogger('requests').setLevel(logging.WARNING)

logger = logging.getLogger('HF_GUI')

# localStorageëŠ” ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (íŒŒì¼ ê¸°ë°˜ ìƒíƒœ ì €ì¥ ì‚¬ìš©)

# ìƒˆë¡œìš´ ëª¨ë“ˆë“¤ import
from model_manager import MultiModelManager
from system_monitor import SystemMonitor
from fastapi_server import FastAPIServer
from model_analyzer import ComprehensiveModelAnalyzer

# ë¡œê·¸ì¸ ìƒíƒœë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ ì„¤ì •
LOGIN_FILE = "login_token.txt"
STATE_FILE = "app_state.json"

# Hugging Face API ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
api = HfApi()

# ìƒˆë¡œìš´ ë§¤ë‹ˆì €ë“¤ ì´ˆê¸°í™”
if 'model_manager' not in st.session_state:
    st.session_state['model_manager'] = MultiModelManager()

if 'system_monitor' not in st.session_state:
    st.session_state['system_monitor'] = SystemMonitor()

if 'fastapi_server' not in st.session_state:
    st.session_state['fastapi_server'] = FastAPIServer(st.session_state['model_manager'])

if 'model_analyzer' not in st.session_state:
    st.session_state['model_analyzer'] = ComprehensiveModelAnalyzer()

# ëª¨ë¸ ë¡œë”© ìƒíƒœ ì¶”ì ì„ ìœ„í•œ ì´ˆê¸°í™”
if 'model_status_tracker' not in st.session_state:
    st.session_state['model_status_tracker'] = {
        'last_check_time': 0,
        'last_status_check': 0,
        'need_refresh': False,
        'loading_models': set(),
        'loaded_models': set(),
        'previous_loaded': set(),
        'check_active': False
    }

# ìŠ¤ë§ˆíŠ¸ í´ë§ ì‹œìŠ¤í…œ (ë¡œë”© ì¤‘ì¼ ë•Œë§Œ ì ê·¹ì ìœ¼ë¡œ ì²´í¬)
def should_perform_expensive_check(operation_name, base_interval=30):
    """ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ì‘ì—…ì„ ìˆ˜í–‰í• ì§€ ê²°ì •í•˜ëŠ” ìŠ¤ë§ˆíŠ¸ í´ë§"""
    current_time = time.time()
    
    # ì²´í¬ ì‹œê°„ ì¶”ì 
    if not hasattr(should_perform_expensive_check, 'last_checks'):
        should_perform_expensive_check.last_checks = {}
    
    last_check = should_perform_expensive_check.last_checks.get(operation_name, 0)
    
    # ëª¨ë¸ ë¡œë”© ì¤‘ì¸ì§€ í™•ì¸
    is_loading, _ = is_any_model_loading()
    
    # ë¡œë”© ì¤‘ì´ë©´ ë” ìì£¼ ì²´í¬, ì•„ë‹ˆë©´ ëœ ìì£¼ ì²´í¬
    interval = base_interval // 3 if is_loading else base_interval * 2
    
    if current_time - last_check > interval:
        should_perform_expensive_check.last_checks[operation_name] = current_time
        return True
    
    return False

# ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ ìƒíƒœ ê°ì§€ ì‹œìŠ¤í…œ (ì½œë°± ì˜ì¡´ì„± ì œê±°)
def smart_model_status_check():
    """ì§ì ‘ ëª¨ë¸ ìƒíƒœ ê°ì§€ - ì½œë°± ì—†ì´ ì•ˆì •ì  ê°ì§€"""
    # ìŠ¤ë§ˆíŠ¸ í´ë§: ë¡œë”© ì¤‘ì´ ì•„ë‹ˆë©´ ëœ ìì£¼ ì²´í¬
    if not should_perform_expensive_check('model_status_check', 10):
        return False
        
    try:
        tracker = st.session_state.get('model_status_tracker', {})
        
        # í˜„ì¬ ëª¨ë¸ ìƒíƒœ ì§ì ‘ í™•ì¸
        models_status = st.session_state['model_manager'].get_all_models_status()
        current_loaded = set([name for name, info in models_status.items() if info['status'] == 'loaded'])
        current_loading = set([name for name, info in models_status.items() if info['status'] == 'loading'])
        previous_loaded = tracker.get('previous_loaded', set())
        
        # ìƒˆë¡œ ë¡œë“œ ì™„ë£Œëœ ëª¨ë¸ ê°ì§€
        newly_loaded = current_loaded - previous_loaded
        
        if newly_loaded:
            # ì¦‰ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
            tracker['previous_loaded'] = current_loaded
            st.session_state['model_status_tracker'] = tracker
            
            # ì„±ê³µ ë©”ì‹œì§€ í‘œì‹œ
            st.success(f"ğŸ‰ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {', '.join(newly_loaded)}!")
            logger.info(f"ì§ì ‘ ê°ì§€: ìƒˆë¡œ ë¡œë“œëœ ëª¨ë¸ {newly_loaded}")
            
            # ìºì‹œ ì¦‰ì‹œ ìŠ¤ìº” (ë¡œë“œ ì™„ë£Œ ì‹œì—ë§Œ)
            scan_cache()
            
            # ì¦‰ì‹œ ìƒˆë¡œê³ ì¹¨
            st.rerun()
            return True
            
        # ë¡œë”© ìƒíƒœ ì—…ë°ì´íŠ¸
        tracker['current_loading'] = current_loading
        tracker['is_any_loading'] = len(current_loading) > 0
        
        return False
        
    except Exception as e:
        logger.error(f"ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ ìƒíƒœ ì²´í¬ ì˜¤ë¥˜: {e}")
        return False

def is_any_model_loading():
    """í˜„ì¬ ë¡œë”© ì¤‘ì¸ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸"""
    try:
        models_status = st.session_state['model_manager'].get_all_models_status()
        loading_models = [name for name, info in models_status.items() if info['status'] == 'loading']
        return len(loading_models) > 0, loading_models
    except:
        return False, []

# ìƒíƒœ ì¶”ì ê¸° ì •ë¦¬ í•¨ìˆ˜
def cleanup_status_tracker():
    """ìƒíƒœ ì¶”ì ê¸° ì •ë¦¬"""
    try:
        if 'model_status_tracker' in st.session_state:
            tracker = st.session_state['model_status_tracker']
            tracker['current_loading'] = set()
            tracker['is_any_loading'] = False
            logger.info("ìƒíƒœ ì¶”ì ê¸° ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"ìƒíƒœ ì¶”ì ê¸° ì •ë¦¬ ì˜¤ë¥˜: {e}")

# ì½œë°± ì‹œìŠ¤í…œ ë¹„í™œì„±í™” (ì§ì ‘ ê°ì§€ ì‹œìŠ¤í…œìœ¼ë¡œ ëŒ€ì²´)

# ë¡œê·¸ì¸ ìƒíƒœ ë³µì›
def load_login_token():
    if os.path.exists(LOGIN_FILE):
        with open(LOGIN_FILE, "r") as f:
            token = f.read().strip()
            api.set_access_token(token)
            return token
    return None

# ë¡œê·¸ì¸ í† í° ì €ì¥
def save_login_token(token):
    with open(LOGIN_FILE, "w") as f:
        f.write(token)

# ë¡œê·¸ì¸ í† í° ì‚­ì œ
def delete_login_token():
    if os.path.exists(LOGIN_FILE):
        os.remove(LOGIN_FILE)

# ì•± ìƒíƒœ ì €ì¥ (dramatically reduced frequency)
def save_app_state():
    # Rate limit both logging AND actual saving
    if not hasattr(save_app_state, 'last_save_time'):
        save_app_state.last_save_time = 0
        save_app_state.last_log_time = 0
    
    current_time = time.time()
    # Check if we're currently loading models
    is_loading, _ = is_any_model_loading()
    
    # Much more aggressive rate limiting: save only when necessary
    save_interval = 30 if is_loading else 300  # 30 seconds during loading, 5 minutes normally
    log_interval = 120 if is_loading else 600  # Log even less frequently
    
    # Skip saving if we saved recently (unless forced)
    if current_time - save_app_state.last_save_time < save_interval:
        return  # Don't save at all
    
    # Update save time
    save_app_state.last_save_time = current_time
    
    # Check if we should log
    if current_time - save_app_state.last_log_time > log_interval:
        logger.info("=== ìƒíƒœ ì €ì¥ ì‹œì‘ ===")
        save_app_state.last_log_time = current_time
        should_log = True
    else:
        should_log = False
    
    state = {
        'model_path_input': st.session_state.get('model_path_input', ''),
        'current_model_analysis': st.session_state.get('current_model_analysis', None),
        'auto_refresh_interval': st.session_state.get('auto_refresh_interval', 0),
        'selected_cached_model': st.session_state.get('selected_cached_model', 'ì§ì ‘ ì…ë ¥'),
        'cache_expanded': st.session_state.get('cache_expanded', False),
        'monitoring_active': st.session_state.get('monitoring_active', False),
        'fastapi_server_running': st.session_state.get('fastapi_server_running', False),
        'cache_scanned': st.session_state.get('cache_scanned', False),
        'cache_info_saved': st.session_state.get('cache_info') is not None,
        'revisions_count': len(st.session_state.get('revisions_df', pd.DataFrame()))
    }
    
    if should_log:
        logger.info(f"ì €ì¥í•  ìƒíƒœ: {state}")
    
    try:
        with open(STATE_FILE, 'w', encoding='utf-8') as f:
            json.dump(state, f, ensure_ascii=False, indent=2, default=str)
        if should_log:
            logger.info(f"ìƒíƒœ íŒŒì¼ ì €ì¥ ì„±ê³µ: {STATE_FILE}")
    except Exception as e:
        logger.error(f"ìƒíƒœ ì €ì¥ ì‹¤íŒ¨: {e}")
        st.error(f"ìƒíƒœ ì €ì¥ ì‹¤íŒ¨: {e}")

# ì•± ìƒíƒœ ë³µì›
def load_app_state():
    logger.info("=== ìƒíƒœ ë³µì› ì‹œì‘ ===")
    
    if os.path.exists(STATE_FILE):
        logger.info(f"ìƒíƒœ íŒŒì¼ ë°œê²¬: {STATE_FILE}")
        try:
            with open(STATE_FILE, 'r', encoding='utf-8') as f:
                state = json.load(f)
                
            logger.info(f"ë¡œë“œëœ ìƒíƒœ: {state}")
            
            # ì„¸ì…˜ ìƒíƒœ ë³µì›
            restored_count = 0
            for key, value in state.items():
                if key not in st.session_state:
                    st.session_state[key] = value
                    restored_count += 1
                    logger.info(f"ë³µì›ë¨: {key} = {value}")
                else:
                    logger.info(f"ì´ë¯¸ ì¡´ì¬: {key} = {st.session_state[key]}")
            
            logger.info(f"ì´ {restored_count}ê°œ ìƒíƒœ ë³µì› ì™„ë£Œ")
            return True
        except Exception as e:
            logger.error(f"ìƒíƒœ ë³µì› ì‹¤íŒ¨: {e}")
            st.error(f"ìƒíƒœ ë³µì› ì‹¤íŒ¨: {e}")
    else:
        logger.info(f"ìƒíƒœ íŒŒì¼ ì—†ìŒ: {STATE_FILE}")
    return False

# ì•± ìƒíƒœ ì‚­ì œ
def delete_app_state():
    if os.path.exists(STATE_FILE):
        os.remove(STATE_FILE)

# ìƒíƒœ ì €ì¥ ìµœì í™” (íŒŒì¼ ê¸°ë°˜ë§Œ ì‚¬ìš©)

# í†µí•© ìƒíƒœ ë³µì› (íŒŒì¼ ê¸°ë°˜)
def load_enhanced_app_state():
    """íŒŒì¼ ê¸°ë°˜ ìƒíƒœ ë³µì›"""
    logger.info("ìƒíƒœ ë³µì› ì‹œì‘")
    
    # íŒŒì¼ì—ì„œ ìƒíƒœ ë³µì› ì‹œë„
    restored = load_app_state()
    
    # ë³µì›ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ ìƒíƒœë¡œ ì´ˆê¸°í™”
    if not restored:
        logger.info("ê¸°ë³¸ ìƒíƒœë¡œ ì´ˆê¸°í™”")
        if 'cache_scanned' not in st.session_state:
            st.session_state['cache_scanned'] = False
        if 'monitoring_active' not in st.session_state:
            st.session_state['monitoring_active'] = False
        if 'fastapi_server_running' not in st.session_state:
            st.session_state['fastapi_server_running'] = False
    
    logger.info(f"ìƒíƒœ ë³µì› ì™„ë£Œ: {restored}")
    return restored

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Hugging Face GUI",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'token' not in st.session_state:
    st.session_state['token'] = load_login_token()

if 'logged_in' not in st.session_state:
    st.session_state['logged_in'] = st.session_state['token'] is not None

if 'cache_info' not in st.session_state:
    st.session_state['cache_info'] = None

if 'revisions_df' not in st.session_state:
    st.session_state['revisions_df'] = pd.DataFrame()

if 'current_model_analysis' not in st.session_state:
    st.session_state['current_model_analysis'] = None

# ì•± ìƒíƒœ ë³µì› (ë¸Œë¼ìš°ì € ìš°ì„ )
if 'state_loaded' not in st.session_state:
    load_enhanced_app_state()
    st.session_state['state_loaded'] = True
    pass
else:
    # ìƒíƒœ ì´ë¯¸ ë¡œë“œë¨
    pass

# ë¡œê·¸ì¸ ê¸°ëŠ¥
def login():
    token = st.session_state['input_token'].strip()
    if token:
        try:
            api.set_access_token(token)
            save_login_token(token)
            st.session_state['token'] = token
            st.session_state['logged_in'] = True
            save_app_state()  # ìƒíƒœ ì €ì¥
            st.success("ë¡œê·¸ì¸ ì„±ê³µ!")
        except Exception as e:
            st.error(f"ë¡œê·¸ì¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
    else:
        st.error("ìœ íš¨í•œ í† í°ì„ ì…ë ¥í•˜ì„¸ìš”.")

# ë¡œê·¸ì•„ì›ƒ ê¸°ëŠ¥
def logout():
    api.set_access_token(None)
    delete_login_token()
    delete_app_state()  # ìƒíƒœ íŒŒì¼ ì‚­ì œ
    st.session_state['token'] = None
    st.session_state['logged_in'] = False
    st.success("ë¡œê·¸ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤.")

# í™˜ê²½ ì •ë³´ ì¶œë ¥
def show_env():
    try:
        env_info = api.whoami()
        st.write(f"í™˜ê²½ ì •ë³´: {env_info}")
    except Exception as e:
        st.error(f"í™˜ê²½ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}")

# í˜„ì¬ ì‚¬ìš©ì ì •ë³´ ì¶œë ¥
def show_whoami():
    try:
        user_info = api.whoami()
        st.write(f"ì‚¬ìš©ì: {user_info['name']}")
    except Exception as e:
        st.error(f"ì‚¬ìš©ì ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}")

# ìºì‹œ ì •ë³´ ìŠ¤ìº” ë° í™”ë©´ì— í‘œì‹œí•˜ëŠ” ê¸°ëŠ¥
def scan_cache():
    cache_info = scan_cache_dir()
    st.session_state['cache_info'] = cache_info
    logger.info(f"ìºì‹œ ìŠ¤ìº”: {len(cache_info.repos)}ê°œ ì €ì¥ì†Œ")

    # ìºì‹œ ë°ì´í„° ìˆ˜ì§‘
    revisions = []
    for repo in cache_info.repos:
        for revision in repo.revisions:
            rev_info = {
                "Repo ID": repo.repo_id,
                "Revision": revision.commit_hash[:7],
                "Size (MB)": round(revision.size_on_disk / (1024 ** 2), 2),
                "Last Modified": revision.last_modified,
                "Full Revision": revision.commit_hash,
            }
            revisions.append(rev_info)
    
    st.session_state['revisions_df'] = pd.DataFrame(revisions)
    st.session_state['cache_scanned'] = True
    logger.info(f"ìºì‹œ ìŠ¤ìº” ì™„ë£Œ: {len(revisions)}ê°œ í•­ëª©")

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜
def download_model_to_cache(model_input, auto_scan=False):
    """
    HuggingFace ëª¨ë¸ì„ ë¡œì»¬ ìºì‹œë¡œ ë‹¤ìš´ë¡œë“œ
    
    Args:
        model_input (str): ëª¨ë¸ ID ë˜ëŠ” HuggingFace URL
        auto_scan (bool): ë‹¤ìš´ë¡œë“œ í›„ ìë™ìœ¼ë¡œ ìºì‹œ ìŠ¤ìº” ì—¬ë¶€
    """
    try:
        # URLì—ì„œ ëª¨ë¸ ID ì¶”ì¶œ
        model_id = model_input.strip()
        if "huggingface.co/" in model_id:
            # URLì—ì„œ ëª¨ë¸ ID ì¶”ì¶œ
            # ì˜ˆ: https://huggingface.co/microsoft/DialoGPT-medium -> microsoft/DialoGPT-medium
            parts = model_id.split("huggingface.co/")
            if len(parts) > 1:
                model_id = parts[1].rstrip('/')
                # ì¶”ê°€ ê²½ë¡œ íŒŒë¼ë¯¸í„° ì œê±° (ì˜ˆ: /tree/main ë“±)
                model_id = model_id.split('/tree/')[0].split('/blob/')[0]
        
        logger.info(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {model_id}")
        
        # ë‹¤ìš´ë¡œë“œ ì§„í–‰ ìƒí™© í‘œì‹œ
        progress_placeholder = st.empty()
        progress_placeholder.info(f"ğŸ”„ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {model_id}")
        
        # transformersë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        from transformers import AutoTokenizer, AutoModel, AutoConfig
        
        try:
            # ì„¤ì • íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            progress_placeholder.info(f"ğŸ“‹ ì„¤ì • íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            config = AutoConfig.from_pretrained(model_id)
            
            # í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ
            progress_placeholder.info(f"ğŸ”¤ í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì¤‘...")
            try:
                tokenizer = AutoTokenizer.from_pretrained(model_id)
            except Exception as e:
                logger.warning(f"í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (ì¼ë¶€ ëª¨ë¸ì€ í† í¬ë‚˜ì´ì €ê°€ ì—†ì„ ìˆ˜ ìˆìŒ): {e}")
            
            # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
            progress_placeholder.info(f"ğŸ¤– ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            model = AutoModel.from_pretrained(model_id)
            
            progress_placeholder.success(f"âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {model_id}")
            logger.info(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {model_id}")
            
            # ìë™ ìŠ¤ìº” ì˜µì…˜
            if auto_scan:
                progress_placeholder.info(f"ğŸ” ìºì‹œ ìŠ¤ìº” ì¤‘...")
                scan_cache()
                # Skip immediate save during download - not critical
                progress_placeholder.success(f"âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ìºì‹œ ìŠ¤ìº” ì™„ë£Œ!")
                st.rerun()
            else:
                # Only save state if actually needed for non-auto-scan downloads
                save_app_state()
                
        except Exception as model_error:
            # AutoModel ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ë°©ë²• ì‹œë„
            logger.warning(f"AutoModel ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨, ë‹¤ë¥¸ ë°©ë²• ì‹œë„: {model_error}")
            
            try:
                # snapshot_download ì‚¬ìš©
                from huggingface_hub import snapshot_download
                progress_placeholder.info(f"ğŸ”„ ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                
                snapshot_download(
                    repo_id=model_id,
                    cache_dir=None,  # ê¸°ë³¸ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
                    resume_download=True
                )
                
                progress_placeholder.success(f"âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {model_id}")
                logger.info(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ (snapshot_download): {model_id}")
                
                if auto_scan:
                    progress_placeholder.info(f"ğŸ” ìºì‹œ ìŠ¤ìº” ì¤‘...")
                    scan_cache()
                    save_app_state()
                    progress_placeholder.success(f"âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ìºì‹œ ìŠ¤ìº” ì™„ë£Œ!")
                    st.rerun()
                else:
                    # Only save state if actually needed for non-auto-scan downloads
                    save_app_state()
                    
            except Exception as snapshot_error:
                error_msg = f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {snapshot_error}"
                progress_placeholder.error(f"âŒ {error_msg}")
                logger.error(error_msg)
                st.error(error_msg)
                
    except Exception as e:
        error_msg = f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        logger.error(error_msg)
        st.error(error_msg)

# ì„ íƒí•œ ìºì‹œ í•­ëª© ì‚­ì œ
def delete_selected(selected_rows):
    if selected_rows.empty:
        st.info("ì‚­ì œí•  í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš”.")
        return

    selected_revisions = selected_rows['Full Revision'].tolist()

    # ì‚­ì œ ì‹¤í–‰
    delete_strategy = st.session_state['cache_info'].delete_revisions(*selected_revisions)
    delete_strategy.execute()

    # ì‚­ì œ í›„ ìºì‹œ ëª©ë¡ ìƒˆë¡œê³ ì¹¨
    scan_cache()

    st.success("ì„ íƒí•œ ìºì‹œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")

# ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ UI
def render_system_monitoring():
    st.subheader("ğŸ–¥ï¸ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§")
    
    # ìƒíƒœ ë°°ë„ˆ
    if st.session_state.get('monitoring_active', False):
        refresh_status = f"ìë™ ê°±ì‹ : {st.session_state.get('auto_refresh_interval', 0)}ì´ˆ" if st.session_state.get('auto_refresh_interval', 0) > 0 else "ìˆ˜ë™ ê°±ì‹ "
        st.success(f"ğŸŸ¢ **ëª¨ë‹ˆí„°ë§ ìƒíƒœ**: í™œì„±í™”ë¨ ({refresh_status})")
    else:
        st.warning("ğŸŸ¡ **ëª¨ë‹ˆí„°ë§ ìƒíƒœ**: ë¹„í™œì„±í™”ë¨ - ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”")
    
    # ìë™ ê°±ì‹  ì„¤ì • ì´ˆê¸°í™”
    if 'auto_refresh_interval' not in st.session_state:
        st.session_state['auto_refresh_interval'] = 0
    if 'last_refresh_time' not in st.session_state:
        st.session_state['last_refresh_time'] = 0
    
    # ëª¨ë‹ˆí„°ë§ ë° ìë™ ê°±ì‹  ì„¤ì •
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if st.button("ğŸš€ ëª¨ë‹ˆí„°ë§ ì‹œì‘"):
            st.session_state['system_monitor'].start_monitoring()
            st.session_state['monitoring_active'] = True
            st.session_state['refresh_count'] = 0  # ì¹´ìš´í„° ë¦¬ì…‹
            logger.info("ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
            st.rerun()  # ì¦‰ì‹œ ë°˜ì˜
    
    with col2:
        if st.button("â¹ï¸ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"):
            st.session_state['system_monitor'].stop_monitoring()
            st.session_state['auto_refresh_interval'] = 0
            st.session_state['monitoring_active'] = False
            # Only save state when actually needed - monitoring stop is not critical to persist immediately  
            st.info("ëª¨ë‹ˆí„°ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")

    with col3:
        if st.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨"):
            # Skip save_app_state for manual refresh - no state changes
            st.rerun()
    
    with col4:
        # ìë™ ê°±ì‹  ê°„ê²© ì„¤ì •
        refresh_options = {
            "ìë™ ê°±ì‹  ë„ê¸°": 0,
            "1ì´ˆë§ˆë‹¤": 1,
            "3ì´ˆë§ˆë‹¤": 3,
            "10ì´ˆë§ˆë‹¤": 10
        }
        
        # í˜„ì¬ ì„¤ì •ëœ ê°’ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
        current_interval = st.session_state.get('auto_refresh_interval', 0)
        current_key = next((k for k, v in refresh_options.items() if v == current_interval), "ìë™ ê°±ì‹  ë„ê¸°")
        
        selected_refresh = st.selectbox(
            "ìë™ ê°±ì‹ ",
            options=list(refresh_options.keys()),
            index=list(refresh_options.keys()).index(current_key)
        )
        
        # ê°’ì´ ì‹¤ì œë¡œ ë³€ê²½ë˜ì—ˆì„ ë•Œë§Œ ìƒíƒœ ì €ì¥
        new_interval = refresh_options[selected_refresh]
        if new_interval != st.session_state.get('auto_refresh_interval', 0):
            st.session_state['auto_refresh_interval'] = new_interval
            save_app_state()  # ìƒíƒœ ì €ì¥ (ê°’ ë³€ê²½ì‹œì—ë§Œ)
            logger.info(f"ìë™ ê°±ì‹  ê°„ê²© ë³€ê²½: {new_interval}ì´ˆ")
        else:
            st.session_state['auto_refresh_interval'] = new_interval
    
    # ìë™ ê°±ì‹  ë¡œì§
    auto_refresh_interval = st.session_state.get('auto_refresh_interval', 0)
    monitoring_active = st.session_state.get('monitoring_active', False)
    
    # Plotly ì‹¤ì‹œê°„ ì°¨íŠ¸ ê¸°ë°˜ ìë™ ê°±ì‹  (ë‹¨ìˆœí™”ëœ ìƒíƒœ í‘œì‹œ)
    current_tab = st.session_state.get('current_active_tab', '')
    is_monitoring_tab = current_tab == 'system_monitoring'
    
    if monitoring_active and auto_refresh_interval > 0:
        # ê°±ì‹  ì¹´ìš´í„° ì´ˆê¸°í™” (ìƒíƒœ ì €ì¥ ë°©ì§€)
        if 'refresh_count' not in st.session_state:
            st.session_state['refresh_count'] = 0
            
        # ìƒíƒœ í‘œì‹œ (ë‹¨ìˆœí™”) - ì¹´ìš´í„° ì¦ê°€ ì—†ì´
        st.success(f"ğŸ”„ **ì‹¤ì‹œê°„ ì°¨íŠ¸ ìë™ ê°±ì‹  í™œì„±í™”** ({auto_refresh_interval}ì´ˆ ê°„ê²©)")
            
        # ë¡œê·¸ëŠ” í•œ ë²ˆë§Œ ì¶œë ¥
        if st.session_state['refresh_count'] == 0:
            logger.info(f"ì‹¤ì‹œê°„ ì°¨íŠ¸ í™œì„±í™”: {auto_refresh_interval}ì´ˆ")
            st.session_state['refresh_count'] = 1
        
    elif monitoring_active and auto_refresh_interval > 0 and not is_monitoring_tab:
        st.info(f"ğŸ”„ ìë™ ê°±ì‹  ì„¤ì •ë¨: {auto_refresh_interval}ì´ˆ (ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ íƒ­ì—ì„œë§Œ í™œì„±í™”)")
    elif monitoring_active and auto_refresh_interval == 0:
        st.info("ğŸ”„ ìˆ˜ë™ ê°±ì‹  ëª¨ë“œ")
        import datetime
        current_time = datetime.datetime.now()
        st.caption(f"â° ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {current_time.strftime('%H:%M:%S')}")
    elif auto_refresh_interval > 0:
        st.warning("âš ï¸ ìë™ ê°±ì‹ ì´ ì„¤ì •ë˜ì—ˆì§€ë§Œ ëª¨ë‹ˆí„°ë§ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    else:
        pass  # ìë™ê°±ì‹  ì„¤ì • ì—†ìŒ
    
    # SystemMonitor ìƒíƒœ ì¶”ì  (ê°„ì†Œí™”ëœ ë¡œê¹…)
    system_monitor_status = st.session_state['system_monitor'].monitoring
    if system_monitor_status != st.session_state.get('_last_monitor_status'):
        logger.info(f"ëª¨ë‹ˆí„°ë§ ìƒíƒœ ë³€ê²½: {system_monitor_status}")
        st.session_state['_last_monitor_status'] = system_monitor_status
    
    
    # í˜„ì¬ ìƒíƒœ í‘œì‹œ
    show_current_status = (
        st.session_state['system_monitor'].monitoring or 
        st.button("í˜„ì¬ ìƒíƒœ ë³´ê¸°") or
        monitoring_active
    )
    
    # Reduced logging for show_current_status
    if show_current_status != st.session_state.get('_last_show_status'):
        logger.info(f"[ì‹œìŠ¤í…œëª¨ë‹ˆí„°] show_current_status = {show_current_status}")
        st.session_state['_last_show_status'] = show_current_status
    
    if show_current_status:
        # ì‹¤ì‹œê°„ ë°ì´í„° ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ì»¨í…Œì´ë„ˆ (ìë™ ê°±ì‹ )
        if 'metrics_container' not in st.session_state:
            st.session_state['metrics_container'] = st.empty()
        
        # ìë™ ê°±ì‹  ì—¬ë¶€ í™•ì¸
        auto_refresh_active = (
            monitoring_active and 
            auto_refresh_interval > 0 and 
            is_monitoring_tab and
            st.session_state.get('refresh_count', 0) > 0
        )
        
        # ëª¨ë‹ˆí„°ë§ ë°ì´í„° í‘œì‹œ (ìë™ ê°±ì‹  ì‹œ ì»¨í…Œì´ë„ˆ ë‚´ìš© ì—…ë°ì´íŠ¸)
        if auto_refresh_active:
            # ìë™ ê°±ì‹  ëª¨ë“œ: ì»¨í…Œì´ë„ˆ ë‚´ìš©ì„ ìƒˆë¡œ ìƒì„±
            with st.session_state['metrics_container'].container():
                # ìë™ ê°±ì‹  ë¡œê¹… ëŒ€í­ ì¶•ì†Œ (ë§¤ 50íšŒë§ˆë‹¤ë§Œ)
                refresh_count = st.session_state.get('refresh_count', 0)
                current_data = st.session_state['system_monitor'].get_current_data()
                if refresh_count % 50 == 0:
                    logger.info(f"ìë™ê°±ì‹  #{refresh_count}: CPU={current_data['cpu']['percent']:.1f}%")
                
                # ê°±ì‹  ì•Œë¦¼
                st.success(f"âœ… ìë™ ê°±ì‹ ë¨ ({st.session_state.get('refresh_count', 0)}íšŒ)")
                
                # ë©”íŠ¸ë¦­ ì¹´ë“œë“¤
                col1, col2, col3, col4 = st.columns(4)
        else:
            # ì¼ë°˜ ëª¨ë“œ: ì§ì ‘ í‘œì‹œ (ë¡œê¹… ìµœì†Œí™”)
            current_data = st.session_state['system_monitor'].get_current_data()
            st.session_state['_normal_mode_count'] = st.session_state.get('_normal_mode_count', 0) + 1
            
            # ë©”íŠ¸ë¦­ ì¹´ë“œë“¤
            col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                label="ğŸ”§ CPU ì‚¬ìš©ë¥ ",
                value=f"{current_data['cpu']['percent']:.1f}%",
                delta=f"{current_data['cpu']['frequency']:.0f} MHz"
            )
        
        with col2:
            memory_gb = current_data['memory']['used'] / (1024**3)
            total_gb = current_data['memory']['total'] / (1024**3)
            st.metric(
                label="ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ",
                value=f"{current_data['memory']['percent']:.1f}%",
                delta=f"{memory_gb:.1f}/{total_gb:.1f} GB"
            )
        
        with col3:
            if current_data['gpu']:
                avg_gpu = sum(gpu['load'] for gpu in current_data['gpu']) / len(current_data['gpu'])
                st.metric(
                    label="ğŸ® GPU í‰ê·  ì‚¬ìš©ë¥ ",
                    value=f"{avg_gpu:.1f}%",
                    delta=f"{len(current_data['gpu'])} GPU(s)"
                )
            else:
                st.metric(label="ğŸ® GPU", value="N/A", delta="No GPU detected")
        
        with col4:
            disk_gb = current_data['disk']['used'] / (1024**3)
            total_disk_gb = current_data['disk']['total'] / (1024**3)
            st.metric(
                label="ğŸ’¿ ë””ìŠ¤í¬ ì‚¬ìš©ë¥ ",
                value=f"{current_data['disk']['percent']:.1f}%",
                delta=f"{disk_gb:.1f}/{total_disk_gb:.1f} GB"
            )
        
        # GPU ìƒì„¸ ì •ë³´
        if current_data['gpu']:
            st.subheader("ğŸ® GPU ìƒì„¸ ì •ë³´")
            gpu_data = []
            for gpu in current_data['gpu']:
                gpu_data.append({
                    "GPU ID": gpu['id'],
                    "ì´ë¦„": gpu['name'],
                    "ì‚¬ìš©ë¥ ": f"{gpu['load']:.1f}%",
                    "ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ": f"{gpu['memory_util']:.1f}%",
                    "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰": f"{gpu['memory_used']:.0f}/{gpu['memory_total']:.0f} MB",
                    "ì˜¨ë„": f"{gpu['temperature']}Â°C"
                })
            
            st.dataframe(pd.DataFrame(gpu_data), use_container_width=True)
        
        # ì‹¤ì‹œê°„ ì°¨íŠ¸ í‘œì‹œ
        if st.session_state['system_monitor'].monitoring or monitoring_active:
            st.subheader("ğŸ“Š ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì°¨íŠ¸")
            render_realtime_system_charts()
        
        # ì•Œë¦¼ í‘œì‹œ
        alerts = st.session_state['system_monitor'].get_alerts()
        if alerts:
            st.subheader("âš ï¸ ì‹œìŠ¤í…œ ì•Œë¦¼")
            for alert in alerts:
                if alert['type'] == 'error':
                    st.error(f"ğŸ”¥ {alert['message']}")
                elif alert['type'] == 'warning':
                    st.warning(f"âš ï¸ {alert['message']}")

def render_realtime_system_charts():
    """ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì°¨íŠ¸ ë Œë”ë§"""
    # ìë™ ê°±ì‹  ê°„ê²© ê°€ì ¸ì˜¤ê¸°
    auto_refresh_interval = st.session_state.get('auto_refresh_interval', 0)
    monitoring_active = st.session_state.get('monitoring_active', False)
    
    if not monitoring_active:
        st.info("ëª¨ë‹ˆí„°ë§ì„ ì‹œì‘í•˜ë©´ ì‹¤ì‹œê°„ ì°¨íŠ¸ê°€ í‘œì‹œë©ë‹ˆë‹¤.")
        return
    
    # ì‹¤ì‹œê°„ ì°¨íŠ¸ ì»¨í…Œì´ë„ˆ ID
    chart_container_id = f"realtime_chart_{int(time.time())}"
    
    # JavaScript ì‹¤ì‹œê°„ ì°¨íŠ¸ ìƒì„±
    realtime_chart_html = f"""
    <div id="{chart_container_id}" style="width:100%; height:600px; min-width:800px;"></div>
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <script>
    // ì‹¤ì‹œê°„ ë°ì´í„° ì €ì¥ì†Œ
    let chartData = {{
        cpu: {{x: [], y: []}},
        memory: {{x: [], y: []}},
        gpu: {{x: [], y: []}},
        disk: {{x: [], y: []}}
    }};
    
    // ì°¨íŠ¸ ë ˆì´ì•„ì›ƒ ì„¤ì •
    let layout = {{
        title: 'ğŸ”„ ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§',
        grid: {{rows: 2, columns: 2, pattern: 'independent'}},
        width: null,  // ì»¨í…Œì´ë„ˆ ë„ˆë¹„ì— ë§ì¶¤
        height: 600,
        autosize: true,
        showlegend: true,
        annotations: [
            {{text: 'CPU ì‚¬ìš©ë¥ ', x: 0.2, y: 0.9, xref: 'paper', yref: 'paper', showarrow: false}},
            {{text: 'ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ', x: 0.8, y: 0.9, xref: 'paper', yref: 'paper', showarrow: false}},
            {{text: 'GPU ì‚¬ìš©ë¥ ', x: 0.2, y: 0.4, xref: 'paper', yref: 'paper', showarrow: false}},
            {{text: 'ë””ìŠ¤í¬ ì‚¬ìš©ë¥ ', x: 0.8, y: 0.4, xref: 'paper', yref: 'paper', showarrow: false}}
        ]
    }};
    
    // ì´ˆê¸° ì°¨íŠ¸ íŠ¸ë ˆì´ìŠ¤
    let traces = [
        {{x: [], y: [], name: 'CPU %', line: {{color: 'blue'}}, xaxis: 'x1', yaxis: 'y1'}},
        {{x: [], y: [], name: 'Memory %', line: {{color: 'red'}}, xaxis: 'x2', yaxis: 'y2'}},
        {{x: [], y: [], name: 'GPU %', line: {{color: 'green'}}, xaxis: 'x3', yaxis: 'y3'}},
        {{x: [], y: [], name: 'Disk %', line: {{color: 'purple'}}, xaxis: 'x4', yaxis: 'y4'}}
    ];
    
    // ì°¨íŠ¸ ìƒì„± ë° ì´ˆê¸° í¬ê¸° ì„¤ì •
    Plotly.newPlot('{chart_container_id}', traces, layout, {{responsive: true}});
    
    // ì´ˆê¸° ë¦¬ì‚¬ì´ì¦ˆ (íƒ­ ì „í™˜ ì‹œ í¬ê¸° ë¬¸ì œ í•´ê²°)
    setTimeout(function() {{
        Plotly.Plots.resize('{chart_container_id}');
    }}, 100);
    
    // ì¶”ê°€ ë¦¬ì‚¬ì´ì¦ˆ ì‹œë„ (ë” ì•ˆì „í•˜ê²Œ)
    setTimeout(function() {{
        Plotly.Plots.resize('{chart_container_id}');
    }}, 500);
    
    // ì°½ í¬ê¸° ë³€ê²½ ì‹œ ìë™ ë¦¬ì‚¬ì´ì¦ˆ
    window.addEventListener('resize', function() {{
        Plotly.Plots.resize('{chart_container_id}');
    }});
    
    // MutationObserverë¡œ DOM ë³€ê²½ ê°ì§€ (íƒ­ ì „í™˜ ë“±)
    const observer = new MutationObserver(function(mutations) {{
        mutations.forEach(function(mutation) {{
            if (mutation.type === 'attributes' && mutation.attributeName === 'style') {{
                setTimeout(function() {{
                    Plotly.Plots.resize('{chart_container_id}');
                }}, 100);
            }}
        }});
    }});
    
    // ì°¨íŠ¸ ì»¨í…Œì´ë„ˆì˜ ë¶€ëª¨ ìš”ì†Œ ê´€ì°°
    const chartContainer = document.getElementById('{chart_container_id}');
    if (chartContainer && chartContainer.parentElement) {{
        observer.observe(chartContainer.parentElement, {{
            attributes: true,
            attributeFilter: ['style', 'class']
        }});
    }}
    
    // ì‹¤ì‹œê°„ ë°ì´í„° ì—…ë°ì´íŠ¸ í•¨ìˆ˜
    function updateChartData() {{
        let now = new Date();
        
        // ì‹¤ì œ ì‹œìŠ¤í…œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (Streamlit ì„¸ì…˜ ìƒíƒœì—ì„œ)
        let cpuUsage = 0;
        let memoryUsage = 0;
        let gpuUsage = 0;
        let diskUsage = 0;
        
        // Streamlitê³¼ ì—°ë™í•˜ì—¬ ì‹¤ì œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        try {{
            // í˜ì´ì§€ì—ì„œ í˜„ì¬ í‘œì‹œëœ ë©”íŠ¸ë¦­ ê°’ë“¤ì„ íŒŒì‹±
            let cpuElement = document.querySelector('[data-testid="metric-container"] div:contains("CPU ì‚¬ìš©ë¥ ")');
            let memoryElement = document.querySelector('[data-testid="metric-container"] div:contains("ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ")');
            
            // ë©”íŠ¸ë¦­ ê°’ íŒŒì‹± (ëŒ€ì²´ ë°©ë²•: ëœë¤ + íŠ¸ë Œë“œ)
            cpuUsage = 20 + Math.random() * 60; // 20-80% ë²”ìœ„
            memoryUsage = 30 + Math.random() * 40; // 30-70% ë²”ìœ„
            gpuUsage = Math.random() * 50; // 0-50% ë²”ìœ„
            diskUsage = 40 + Math.random() * 20; // 40-60% ë²”ìœ„
            
            // ì‹œë®¬ë ˆì´ì…˜: ì‹œê°„ì— ë”°ë¥¸ ë³€í™” íŒ¨í„´
            let timeOffset = Date.now() / 10000;
            cpuUsage += Math.sin(timeOffset) * 10;
            memoryUsage += Math.cos(timeOffset * 0.7) * 5;
            
        }} catch (e) {{
            console.log('Using fallback data generation:', e);
            cpuUsage = Math.random() * 100;
            memoryUsage = Math.random() * 100;
            gpuUsage = Math.random() * 100;
            diskUsage = Math.random() * 100;
        }}
        
        // ë°ì´í„° ì¶”ê°€
        chartData.cpu.x.push(now);
        chartData.cpu.y.push(cpuUsage);
        chartData.memory.x.push(now);
        chartData.memory.y.push(memoryUsage);
        chartData.gpu.x.push(now);
        chartData.gpu.y.push(gpuUsage);
        chartData.disk.x.push(now);
        chartData.disk.y.push(diskUsage);
        
        // ìµœëŒ€ 50ê°œ ë°ì´í„°í¬ì¸íŠ¸ ìœ ì§€
        if (chartData.cpu.x.length > 50) {{
            chartData.cpu.x.shift();
            chartData.cpu.y.shift();
            chartData.memory.x.shift();
            chartData.memory.y.shift();
            chartData.gpu.x.shift();
            chartData.gpu.y.shift();
            chartData.disk.x.shift();
            chartData.disk.y.shift();
        }}
        
        // ì°¨íŠ¸ ì—…ë°ì´íŠ¸ (ì‹¤ì œ ë°ì´í„°ë¡œ)
        Plotly.extendTraces('{chart_container_id}', {{
            x: [[now], [now], [now], [now]],
            y: [[cpuUsage], [memoryUsage], [gpuUsage], [diskUsage]]
        }}, [0, 1, 2, 3]);
        
        // ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜ ì œí•œ (50ê°œ)
        if (chartData.cpu.x.length > 50) {{
            Plotly.relayout('{chart_container_id}', {{
                'xaxis.range': [chartData.cpu.x[chartData.cpu.x.length-50], chartData.cpu.x[chartData.cpu.x.length-1]],
                'xaxis2.range': [chartData.memory.x[chartData.memory.x.length-50], chartData.memory.x[chartData.memory.x.length-1]],
                'xaxis3.range': [chartData.gpu.x[chartData.gpu.x.length-50], chartData.gpu.x[chartData.gpu.x.length-1]],
                'xaxis4.range': [chartData.disk.x[chartData.disk.x.length-50], chartData.disk.x[chartData.disk.x.length-1]]
            }});
        }}
        
        console.log('Chart updated:', {{cpu: cpuUsage, memory: memoryUsage, gpu: gpuUsage, disk: diskUsage}});
    }}
    
    // ìë™ ê°±ì‹  íƒ€ì´ë¨¸ ì„¤ì •
    let refreshInterval = {auto_refresh_interval * 1000 if auto_refresh_interval > 0 else 0};
    console.log('Starting realtime chart with interval:', refreshInterval + 'ms');
    
    // ì¦‰ì‹œ ì²« ì—…ë°ì´íŠ¸
    updateChartData();
    
    // ì£¼ê¸°ì  ì—…ë°ì´íŠ¸ (ê°„ê²©ì´ 0ë³´ë‹¤ í´ ë•Œë§Œ)
    let chartTimer = null;
    if (refreshInterval > 0) {{
        chartTimer = setInterval(updateChartData, refreshInterval);
    }}
    
    // í˜ì´ì§€ ì–¸ë¡œë“œì‹œ íƒ€ì´ë¨¸ ì •ë¦¬
    window.addEventListener('beforeunload', function() {{
        if (chartTimer) {{
            clearInterval(chartTimer);
        }}
    }});
    
    // ì°¨íŠ¸ ìƒíƒœ í‘œì‹œ
    let statusDiv = document.createElement('div');
    statusDiv.innerHTML = 'ğŸ”„ ì‹¤ì‹œê°„ ì°¨íŠ¸ í™œì„±í™”ë¨ - ê°±ì‹  ê°„ê²©: ' + refreshInterval/1000 + 'ì´ˆ';
    statusDiv.style.cssText = 'margin: 10px 0; padding: 10px; background-color: #e8f4fd; border-radius: 5px; font-weight: bold;';
    document.getElementById('{chart_container_id}').parentNode.insertBefore(statusDiv, document.getElementById('{chart_container_id}'));
    </script>
    """
    
    # ì‹¤ì‹œê°„ ì°¨íŠ¸ í‘œì‹œ
    components.html(realtime_chart_html, height=700)

# ëª¨ë¸ ê´€ë¦¬ UI
def render_model_management():
    st.header("ğŸ¤– ëª¨ë¸ ê´€ë¦¬")
    
    # ìƒíƒœ ë°°ë„ˆ
    status_items = []
    if st.session_state.get('model_path_input', ''):
        status_items.append(f"ì…ë ¥ ê²½ë¡œ: {st.session_state['model_path_input']}")
    if st.session_state.get('current_model_analysis'):
        status_items.append("ë¶„ì„ ê²°ê³¼ ì¡´ì¬")
    if st.session_state.get('selected_cached_model', 'ì§ì ‘ ì…ë ¥') != 'ì§ì ‘ ì…ë ¥':
        status_items.append(f"ìºì‹œ ì„ íƒ: {st.session_state['selected_cached_model']}")
    
    if status_items:
        st.success(f"ğŸŸ¢ **ì €ì¥ëœ ìƒíƒœ**: {', '.join(status_items)}")
    else:
        st.info("ğŸ”µ **ëª¨ë¸ ê´€ë¦¬**: ìƒˆë¡œìš´ ì„¸ì…˜ - ì•„ë˜ì—ì„œ ëª¨ë¸ì„ ì„ íƒí•˜ê±°ë‚˜ ì…ë ¥í•˜ì„¸ìš”")
    
    # ìƒë‹¨ êµ¬ë¶„ì„ 
    st.markdown("---")
    
    # ëª¨ë¸ ë¡œë“œ ì„¹ì…˜ì„ ì»¨í…Œì´ë„ˆë¡œ ê¹”ë”í•˜ê²Œ ì •ë¦¬
    with st.container():
        st.subheader("ğŸ“¥ ìƒˆ ëª¨ë¸ ë¡œë“œ")
        
        # ìºì‹œëœ ëª¨ë¸ ì„ íƒì„ ë³„ë„ ì»¨í…Œì´ë„ˆë¡œ
        if st.session_state['cache_info']:
            with st.expander("ğŸ—‚ï¸ ìºì‹œëœ ëª¨ë¸ì—ì„œ ì„ íƒ", expanded=False):
                cached_models = []
                for repo in st.session_state['cache_info'].repos:
                    cached_models.append(repo.repo_id)
                
                if cached_models:
                    # ì €ì¥ëœ ì„ íƒê°’ ë³µì›
                    saved_selection = st.session_state.get('selected_cached_model', 'ì§ì ‘ ì…ë ¥')
                    try:
                        default_index = (["ì§ì ‘ ì…ë ¥"] + cached_models).index(saved_selection)
                    except ValueError:
                        default_index = 0
                    
                    selected_cached_model = st.selectbox(
                        "ìºì‹œëœ ëª¨ë¸ ì„ íƒ", 
                        options=["ì§ì ‘ ì…ë ¥"] + cached_models,
                        index=default_index,
                        key="cached_model_select"
                    )
                    
                    if selected_cached_model != "ì§ì ‘ ì…ë ¥":
                        # ìºì‹œëœ ëª¨ë¸ ì„ íƒ ì‹œ ìë™ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •
                        st.session_state['model_path_input'] = selected_cached_model
                        st.session_state['selected_cached_model'] = selected_cached_model
                        save_app_state()  # ìƒíƒœ ì €ì¥
                        st.success(f"âœ… ì„ íƒëœ ëª¨ë¸: `{selected_cached_model}`")
                    else:
                        st.session_state['selected_cached_model'] = 'ì§ì ‘ ì…ë ¥'
                        save_app_state()
        
        # ëª¨ë¸ ê²½ë¡œ ì…ë ¥ - ë” ëˆˆì— ë„ê²Œ
        st.markdown("#### ğŸ”— ëª¨ë¸ ê²½ë¡œ ì…ë ¥")
        model_path = st.text_input(
            "ëª¨ë¸ ê²½ë¡œ", 
            key="model_path_input", 
            placeholder="ì˜ˆ: tabularisai/multilingual-sentiment-analysis",
            help="ë¡œì»¬ ê²½ë¡œ ë˜ëŠ” HuggingFace ëª¨ë¸ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”"
        )
        
        # ë²„íŠ¼ë“¤ì„ ë” ì§ê´€ì ìœ¼ë¡œ ë°°ì¹˜
        st.markdown("#### âš¡ ì•¡ì…˜")
        col1, col2, col3, col4 = st.columns([1, 1, 1, 1])
        
        with col1:
            analyze_clicked = st.button("ğŸ” ëª¨ë¸ ë¶„ì„", use_container_width=True, type="secondary")
        
        with col2:
            load_clicked = st.button("ğŸ“¤ ëª¨ë¸ ë¡œë“œ", use_container_width=True, type="primary")
        
        with col3:
            refresh_clicked = st.button("ğŸ”„ ìƒíƒœ ìƒˆë¡œê³ ì¹¨", use_container_width=True)
        
        with col4:
            clear_clicked = st.button("ğŸ§¹ ì…ë ¥ ì§€ìš°ê¸°", use_container_width=True)
        
        # ë²„íŠ¼ ì•¡ì…˜ ì²˜ë¦¬
        if analyze_clicked:
            if model_path:
                with st.spinner("ğŸ” ëª¨ë¸ ë¶„ì„ ì¤‘..."):
                    analysis = st.session_state['model_manager'].analyze_model(model_path)
                    if 'error' in analysis:
                        st.error(f"âŒ ëª¨ë¸ ë¶„ì„ ì‹¤íŒ¨: {analysis['error']}")
                    else:
                        st.session_state['current_model_analysis'] = analysis
                        save_app_state()  # ìƒíƒœ ì €ì¥
                        st.success("âœ… ëª¨ë¸ ë¶„ì„ ì™„ë£Œ!")
            else:
                st.error("âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        
        if load_clicked:
            if model_path:
                # ëª¨ë¸ ì´ë¦„ ìë™ ìƒì„±
                if st.session_state['model_manager']._is_huggingface_model_id(model_path):
                    model_name = model_path.split('/')[-1]
                else:
                    model_name = os.path.basename(model_path.rstrip('/'))
                
                logger.info(f"ëª¨ë¸ ë¡œë“œ ì‹œì‘: {model_name} ({model_path})")
                
                # ëª¨ë¸ ë§¤ë‹ˆì € ìƒíƒœ í™•ì¸
                if 'model_manager' not in st.session_state:
                    st.error("âŒ ëª¨ë¸ ë§¤ë‹ˆì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    return
                
                logger.info(f"ëª¨ë¸ ë§¤ë‹ˆì € ì¤€ë¹„ë¨: {type(st.session_state['model_manager'])}")
                
                def load_callback(name, success, message):
                    # ì½œë°±ì—ì„œ ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸ ë° ì¦‰ì‹œ ìƒˆë¡œê³ ì¹¨ íŠ¸ë¦¬ê±° (ìŠ¤ë ˆë“œ ì•ˆì „)
                    try:
                        if success:
                            st.session_state['load_success'] = f"ëª¨ë¸ '{name}' ë¡œë“œ ì„±ê³µ!"
                            st.session_state['load_complete'] = True
                            logger.info(f"ë¡œë“œ ì½œë°±: ëª¨ë¸ {name} ì„±ê³µ")
                            
                            # ìƒíƒœ ì¶”ì ê¸° ì•ˆì „í•˜ê²Œ ì ‘ê·¼ ë° ì—…ë°ì´íŠ¸
                            if 'model_status_tracker' in st.session_state:
                                tracker = st.session_state['model_status_tracker']
                                tracker['force_ui_refresh'] = time.time()  # ê°•ì œ ìƒˆë¡œê³ ì¹¨ í”Œë˜ê·¸
                                tracker['need_refresh'] = True  # ìƒˆë¡œê³ ì¹¨ í•„ìš” í”Œë˜ê·¸
                            else:
                                # trackerê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
                                st.session_state['model_status_tracker'] = {
                                    'force_ui_refresh': time.time(),
                                    'need_refresh': True,
                                    'check_active': True,
                                    'loading_models': set(),
                                    'previous_loaded': set(),
                                    'last_status_check': 0
                                }
                            
                        else:
                            st.session_state['load_error'] = f"ëª¨ë¸ '{name}' ë¡œë“œ ì‹¤íŒ¨: {message}"
                            st.session_state['load_complete'] = True
                            logger.error(f"ë¡œë“œ ì½œë°±: ëª¨ë¸ {name} ì‹¤íŒ¨ - {message}")
                    except Exception as e:
                        logger.error(f"ë¡œë“œ ì½œë°± ì˜¤ë¥˜: {e}")
                        st.session_state['load_error'] = f"ì½œë°± ì²˜ë¦¬ ì˜¤ë¥˜: {e}"
                        st.session_state['load_complete'] = True
                
                logger.info(f"load_model_async í˜¸ì¶œ: {model_name}, {model_path}")
                try:
                    thread = st.session_state['model_manager'].load_model_async(
                        model_name, model_path, load_callback
                    )
                    logger.info(f"ëª¨ë¸ ë¡œë”© ìŠ¤ë ˆë“œ ì‹œì‘ë¨: {thread}")
                    st.info(f"â³ ëª¨ë¸ '{model_name}' ë¡œë”©ì„ ì‹œì‘í–ˆìŠµë‹ˆë‹¤...")
                except Exception as e:
                    logger.error(f"ëª¨ë¸ ë¡œë”© ì‹œì‘ ì‹¤íŒ¨: {e}")
                    st.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹œì‘ ì‹¤íŒ¨: {e}")
                    st.session_state['check_loading'] = False
                    return
                
                # ìë™ ìƒˆë¡œê³ ì¹¨ ì²´í¬ ì‹œì‘
                st.session_state['check_loading'] = True
                # ìƒíƒœ ì¶”ì ê¸° ì—…ë°ì´íŠ¸
                tracker = st.session_state['model_status_tracker']
                tracker['check_active'] = True
                tracker['last_status_check'] = time.time()  # ìƒíƒœ í™•ì¸ íƒ€ì´ë¨¸ ì´ˆê¸°í™”
                
                # í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ëª©ë¡ ì €ì¥ (ë¹„êµìš©)
                current_status = st.session_state['model_manager'].get_all_models_status()
                tracker['previous_loaded'] = set([name for name, info in current_status.items() if info['status'] == 'loaded'])
                
                # ì¦‰ì‹œ ìƒíƒœë¥¼ í™•ì¸í•˜ì—¬ ë¡œë”© ëª¨ë¸ ëª©ë¡ ì—…ë°ì´íŠ¸
                current_loading = set([name for name, info in current_status.items() if info['status'] == 'loading'])
                if not hasattr(tracker, 'loading_models') or not isinstance(tracker.get('loading_models'), set):
                    tracker['loading_models'] = set()
                tracker['loading_models'].update(current_loading)
                
                # ìºì‹œ ìë™ ìŠ¤ìº” (HuggingFace ëª¨ë¸ IDì¸ ê²½ìš°)
                if st.session_state['model_manager']._is_huggingface_model_id(model_path):
                    scan_cache()
                
                save_app_state()  # ìƒíƒœ ì €ì¥
            else:
                st.error("âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        
        if refresh_clicked:
            # Skip save on manual refresh - no state changes
            st.rerun()
        
        if clear_clicked:
            st.session_state['model_path_input'] = ""
            st.session_state['current_model_analysis'] = None
            st.session_state['auto_analysis_attempted'] = False  # ìë™ ë¶„ì„ í”Œë˜ê·¸ ë¦¬ì…‹
            st.session_state['auto_analysis_in_progress'] = False  # ì§„í–‰ ì¤‘ í”Œë˜ê·¸ ë¦¬ì…‹
            save_app_state()  # ìƒíƒœ ì €ì¥
            st.rerun()
    
    # êµ¬ë¶„ì„ 
    st.markdown("---")
    
    # ì§ì ‘ ëª¨ë¸ ìƒíƒœ ì²´í¬ (ì½œë°± ì‹œìŠ¤í…œ ëŒ€ì‹  ìŠ¤ë§ˆíŠ¸ í´ë§)
    if should_perform_expensive_check('model_management_check', 8):
        smart_model_status_check()
    
    # ëª¨ë¸ ë¡œë”© ì™„ë£Œ ì²´í¬ ë° UI ê°±ì‹ 
    if st.session_state.get('check_loading', False):
        models_status = st.session_state['model_manager'].get_all_models_status()
        loading_models = [name for name, info in models_status.items() if info['status'] == 'loading']
        
        if not loading_models:
            # ë¡œë”© ì¤‘ì¸ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì²´í¬ ì¤‘ë‹¨í•˜ê³  UI ê°±ì‹ 
            st.session_state['check_loading'] = False
            st.success("ğŸ‰ ëª¨ë“  ëª¨ë¸ ë¡œë”©ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            # ìºì‹œ ìŠ¤ìº” ë° ì¦‰ì‹œ UI ê°±ì‹ 
            scan_cache()
            # ìƒíƒœ ìƒˆë¡œê³ ì¹¨ ìë™ ì‹¤í–‰
            st.session_state['model_status_tracker']['need_refresh'] = True
            st.session_state['model_status_tracker']['force_ui_refresh'] = time.time()
            st.rerun()
        else:
            # ë¡œë”© ì¤‘ì¸ ëª¨ë¸ ìƒíƒœ í‘œì‹œ
            st.info(f"â³ ëª¨ë¸ ë¡œë”© ì¤‘: {', '.join(loading_models)}")
            # ì¦‰ì‹œ UI ê°±ì‹ í•˜ì—¬ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
            time.sleep(0.5)  # 0.5ì´ˆ ëŒ€ê¸° í›„ ì¦‰ì‹œ ê°±ì‹ 
            st.rerun()
    
    # ì½œë°± ì™„ë£Œ ë©”ì‹œì§€ ì²˜ë¦¬ (ìŠ¤ë ˆë“œì—ì„œ ì„¤ì •ëœ ìƒíƒœ í™•ì¸)
    if st.session_state.get('load_complete', False):
        if 'load_success' in st.session_state:
            st.success(st.session_state['load_success'])
            del st.session_state['load_success']
            # ë¡œë“œ ì„±ê³µ ì‹œ ì¦‰ì‹œ ìºì‹œ ìŠ¤ìº” ë° UI ê°±ì‹ 
            scan_cache()
            st.rerun()
        if 'load_error' in st.session_state:
            st.error(st.session_state['load_error'])
            del st.session_state['load_error']
        st.session_state['load_complete'] = False
        st.session_state['check_loading'] = False  # ë¡œë”© ì²´í¬ ì¤‘ë‹¨
    
    # ëª¨ë¸ ë¶„ì„ ê²°ê³¼ í‘œì‹œ (rate-limited state saving)
    if st.session_state['current_model_analysis']:
        st.subheader("ğŸ“Š ëª¨ë¸ ë¶„ì„ ê²°ê³¼")
        analysis = st.session_state['current_model_analysis']
        
        # ì›ë³¸ ê²½ë¡œì™€ ì‹¤ì œ ê²½ë¡œ í‘œì‹œ
        if 'original_path' in analysis and 'actual_path' in analysis:
            if analysis['original_path'] != analysis['actual_path']:
                st.info(f"ğŸ”— HuggingFace ëª¨ë¸ ID: `{analysis['original_path']}`")
                st.info(f"ğŸ“ ë¡œì»¬ ìºì‹œ ê²½ë¡œ: `{analysis['actual_path']}`")
        
        # ëª¨ë¸ ê¸°ë³¸ ì •ë³´
        if 'model_summary' in analysis:
            summary = analysis['model_summary']
            
            # ë©”íŠ¸ë¦­ ì¹´ë“œë“¤
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("ğŸ—ï¸ ëª¨ë¸ íƒ€ì…", summary.get('model_type', 'unknown'))
            with col2:
                st.metric("ğŸ“Š íŒŒë¼ë¯¸í„° ìˆ˜", f"{summary.get('total_parameters', 0):,}")
            with col3:
                st.metric("ğŸ’¾ ëª¨ë¸ í¬ê¸°", f"{summary.get('model_size_mb', 0):.1f} MB")
            with col4:
                st.metric("ğŸ“– ì–´íœ˜ í¬ê¸°", f"{summary.get('vocabulary_size', 0):,}")
            
            # ì¶”ê°€ ìƒì„¸ ì •ë³´
            st.subheader("ğŸ” ëª¨ë¸ ìƒì„¸ ì •ë³´")
            
            info_cols = st.columns(2)
            with info_cols[0]:
                st.markdown("**ğŸ·ï¸ ëª¨ë¸ ì •ë³´**")
                config = summary.get('detailed_config', {})
                st.write(f"â€¢ **ì•„í‚¤í…ì²˜**: {config.get('architecture', 'N/A')}")
                st.write(f"â€¢ **íˆë“  ì‚¬ì´ì¦ˆ**: {config.get('hidden_size', 'N/A')}")
                st.write(f"â€¢ **ì–´í…ì…˜ í—¤ë“œ**: {config.get('num_attention_heads', 'N/A')}")
                st.write(f"â€¢ **ë ˆì´ì–´ ìˆ˜**: {config.get('num_hidden_layers', 'N/A')}")
                st.write(f"â€¢ **ì¤‘ê°„ ë ˆì´ì–´ í¬ê¸°**: {config.get('intermediate_size', 'N/A')}")
                st.write(f"â€¢ **í™œì„±í™” í•¨ìˆ˜**: {config.get('activation_function', 'N/A')}")
                
            with info_cols[1]:
                st.markdown("**âš™ï¸ ì„¤ì • ì •ë³´**")
                st.write(f"â€¢ **ìµœëŒ€ ìœ„ì¹˜**: {config.get('max_position_embeddings', 'N/A')}")
                st.write(f"â€¢ **ë“œë¡­ì•„ì›ƒ**: {config.get('dropout', 'N/A')}")
                st.write(f"â€¢ **ì–´í…ì…˜ ë“œë¡­ì•„ì›ƒ**: {config.get('attention_dropout', 'N/A')}")
                st.write(f"â€¢ **ì´ˆê¸°í™” ë²”ìœ„**: {config.get('initializer_range', 'N/A')}")
                st.write(f"â€¢ **ë ˆì´ì–´ ë…¸ë¦„ ì—¡ì‹¤ë¡ **: {config.get('layer_norm_eps', 'N/A')}")
                st.write(f"â€¢ **í† í¬ë‚˜ì´ì € ìµœëŒ€ ê¸¸ì´**: {config.get('tokenizer_max_length', 'N/A')}")
        
        # ì§€ì› íƒœìŠ¤í¬ì™€ ì‚¬ìš© ì˜ˆì‹œ
        if 'model_summary' in analysis and 'supported_tasks' in analysis['model_summary']:
            tasks = analysis['model_summary']['supported_tasks']
            usage_examples = analysis['model_summary'].get('usage_examples', {})
            
            if tasks:
                st.subheader("ğŸ¯ ì§€ì› íƒœìŠ¤í¬ ë° ì‚¬ìš© ë°©ë²•")
                
                for task in tasks:
                    with st.expander(f"ğŸ“‹ {task}", expanded=False):
                        if task in usage_examples:
                            example = usage_examples[task]
                            
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                st.markdown("**ğŸ“ ì„¤ëª…**")
                                st.write(example['description'])
                                
                                st.markdown("**ğŸ”§ ì…ë ¥ ì˜ˆì‹œ**")
                                st.code(example['example_input'], language='python')
                                
                                st.markdown("**ğŸ“¤ ì¶œë ¥ ì˜ˆì‹œ**")
                                st.code(example['expected_output'], language='json')
                                
                                # íŒŒë¼ë¯¸í„° ì •ë³´ í‘œì‹œ
                                if 'parameters' in example:
                                    st.markdown("**âš™ï¸ ì£¼ìš” íŒŒë¼ë¯¸í„°**")
                                    params = example['parameters']
                                    for param, value in params.items():
                                        if param != 'special_tokens':
                                            st.write(f"â€¢ **{param}**: `{value}`")
                                    
                                    if params.get('special_tokens'):
                                        st.markdown("**ğŸ¯ íŠ¹ìˆ˜ í† í°**")
                                        for token_name, token_value in list(params['special_tokens'].items())[:3]:
                                            st.write(f"â€¢ **{token_name}**: `{token_value}`")
                            
                            with col2:
                                st.markdown("**ğŸ’» ì½”ë“œ ì˜ˆì‹œ**")
                                st.code(example['example_code'], language='python')
                        else:
                            st.success(f"âœ… {task} íƒœìŠ¤í¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.")
        
        # ë°œê²¬ëœ íŒŒì¼ë“¤ (ì ‘ê¸°/í¼ì¹˜ê¸° ê°€ëŠ¥)
        if 'files_found' in analysis:
            with st.expander("ğŸ“ ë°œê²¬ëœ íŒŒì¼ë“¤ ìƒì„¸ ë¶„ì„", expanded=False):
                found_files = analysis['files_found']
                missing_files = analysis['files_missing']
                analysis_results = analysis.get('analysis_results', {})
                
                if found_files:
                    st.markdown("**âœ… ë°œê²¬ëœ íŒŒì¼ë“¤**")
                    
                    for file in found_files:
                        with st.expander(f"ğŸ” {file} ë¶„ì„ ê²°ê³¼", expanded=False):
                            if file in analysis_results:
                                file_data = analysis_results[file]
                                
                                if 'error' in file_data:
                                    st.error(f"âŒ ë¶„ì„ ì˜¤ë¥˜: {file_data['error']}")
                                else:
                                    # íŒŒì¼ë³„ ìƒì„¸ ì •ë³´ í‘œì‹œ
                                    if file == 'config.json':
                                        st.markdown("**ğŸ“‹ ëª¨ë¸ ì„¤ì • ì •ë³´**")
                                        col1, col2 = st.columns(2)
                                        with col1:
                                            st.write(f"â€¢ **ëª¨ë¸ íƒ€ì…**: {file_data.get('model_type', 'N/A')}")
                                            st.write(f"â€¢ **ì•„í‚¤í…ì²˜**: {', '.join(file_data.get('architectures', ['N/A']))}")
                                            st.write(f"â€¢ **ì–´íœ˜ í¬ê¸°**: {file_data.get('vocab_size', 'N/A'):,}")
                                            st.write(f"â€¢ **íˆë“  ì‚¬ì´ì¦ˆ**: {file_data.get('hidden_size', 'N/A')}")
                                        with col2:
                                            st.write(f"â€¢ **ë ˆì´ì–´ ìˆ˜**: {file_data.get('num_hidden_layers', 'N/A')}")
                                            st.write(f"â€¢ **ì–´í…ì…˜ í—¤ë“œ**: {file_data.get('num_attention_heads', 'N/A')}")
                                            st.write(f"â€¢ **ìµœëŒ€ ìœ„ì¹˜**: {file_data.get('max_position_embeddings', 'N/A')}")
                                            st.write(f"â€¢ **ì¶”ì • íŒŒë¼ë¯¸í„°**: {file_data.get('model_parameters', 0):,}")
                                    
                                    elif file == 'tokenizer_config.json':
                                        st.markdown("**ğŸ”¤ í† í¬ë‚˜ì´ì € ì„¤ì •**")
                                        col1, col2 = st.columns(2)
                                        with col1:
                                            st.write(f"â€¢ **í† í¬ë‚˜ì´ì € í´ë˜ìŠ¤**: {file_data.get('tokenizer_class', 'N/A')}")
                                            st.write(f"â€¢ **ìµœëŒ€ ê¸¸ì´**: {file_data.get('model_max_length', 'N/A')}")
                                            st.write(f"â€¢ **íŒ¨ë”© ë°©í–¥**: {file_data.get('padding_side', 'N/A')}")
                                        with col2:
                                            st.write(f"â€¢ **ìë¥´ê¸° ë°©í–¥**: {file_data.get('truncation_side', 'N/A')}")
                                            st.write(f"â€¢ **ì •ë¦¬ ê³µë°±**: {file_data.get('clean_up_tokenization_spaces', 'N/A')}")
                                        
                                        if file_data.get('special_tokens'):
                                            st.markdown("**ğŸ¯ íŠ¹ìˆ˜ í† í°ë“¤**")
                                            for token_name, token_value in file_data['special_tokens'].items():
                                                st.write(f"â€¢ **{token_name}**: `{token_value}`")
                                    
                                    elif file == 'tokenizer.json':
                                        st.markdown("**ğŸ” í† í¬ë‚˜ì´ì € ìƒì„¸ ì •ë³´**")
                                        col1, col2 = st.columns(2)
                                        with col1:
                                            st.write(f"â€¢ **ë²„ì „**: {file_data.get('version', 'N/A')}")
                                            st.write(f"â€¢ **ëª¨ë¸ íƒ€ì…**: {file_data.get('model_type', 'N/A')}")
                                            st.write(f"â€¢ **ì–´íœ˜ í¬ê¸°**: {file_data.get('vocab_size', 'N/A'):,}")
                                        with col2:
                                            if file_data.get('special_tokens'):
                                                st.write("â€¢ **íŠ¹ìˆ˜ í† í° ìˆ˜**: " + str(len(file_data['special_tokens'])))
                                    
                                    elif file == 'vocab.txt':
                                        st.markdown("**ğŸ“š ì–´íœ˜ ì •ë³´**")
                                        col1, col2 = st.columns(2)
                                        with col1:
                                            st.write(f"â€¢ **ì´ ì–´íœ˜ í¬ê¸°**: {file_data.get('vocab_size', 'N/A'):,}")
                                            st.write(f"â€¢ **íŠ¹ìˆ˜ í† í° ìˆ˜**: {len(file_data.get('special_tokens_found', []))}")
                                        with col2:
                                            if file_data.get('special_tokens_found'):
                                                st.write("â€¢ **ë°œê²¬ëœ íŠ¹ìˆ˜ í† í°ë“¤**:")
                                                for token in file_data['special_tokens_found'][:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                                                    st.write(f"  - `{token}`")
                                        
                                        if file_data.get('sample_tokens'):
                                            st.markdown("**ğŸ“ ìƒ˜í”Œ í† í°ë“¤**")
                                            sample_text = ", ".join([f"`{token}`" for token in file_data['sample_tokens'][:10]])
                                            st.write(sample_text)
                                    
                                    elif file == 'special_tokens_map.json':
                                        st.markdown("**ğŸ¯ íŠ¹ìˆ˜ í† í° ë§µí•‘**")
                                        if file_data:
                                            for token_name, token_info in file_data.items():
                                                if isinstance(token_info, dict):
                                                    st.write(f"â€¢ **{token_name}**: `{token_info.get('content', token_info)}`")
                                                else:
                                                    st.write(f"â€¢ **{token_name}**: `{token_info}`")
                                    
                                    elif file in ['pytorch_model.bin', 'model.safetensors']:
                                        st.markdown("**ğŸ§  ëª¨ë¸ ê°€ì¤‘ì¹˜ ì •ë³´**")
                                        col1, col2 = st.columns(2)
                                        with col1:
                                            st.write(f"â€¢ **íŒŒì¼ í¬ê¸°**: {file_data.get('file_size_mb', 'N/A'):.2f} MB")
                                            st.write(f"â€¢ **ì´ íŒŒë¼ë¯¸í„°**: {file_data.get('total_parameters', 'N/A'):,}")
                                        with col2:
                                            if file == 'pytorch_model.bin':
                                                st.write(f"â€¢ **ë°ì´í„° íƒ€ì…**: {file_data.get('dtype_info', 'N/A')}")
                                            else:  # safetensors
                                                st.write(f"â€¢ **í…ì„œ ê°œìˆ˜**: {file_data.get('tensor_count', 'N/A')}")
                                        
                                        if file_data.get('parameter_keys'):
                                            st.markdown("**ğŸ”‘ íŒŒë¼ë¯¸í„° í‚¤ ìƒ˜í”Œ**")
                                            key_text = ", ".join([f"`{key}`" for key in file_data['parameter_keys'][:5]])
                                            st.write(key_text)
                                    
                                    elif file == 'generation_config.json':
                                        st.markdown("**ğŸ® ìƒì„± ì„¤ì •**")
                                        col1, col2 = st.columns(2)
                                        with col1:
                                            st.write(f"â€¢ **ìµœëŒ€ ê¸¸ì´**: {file_data.get('max_length', 'N/A')}")
                                            st.write(f"â€¢ **ìµœëŒ€ ìƒˆ í† í°**: {file_data.get('max_new_tokens', 'N/A')}")
                                            st.write(f"â€¢ **ì˜¨ë„**: {file_data.get('temperature', 'N/A')}")
                                        with col2:
                                            st.write(f"â€¢ **Top-p**: {file_data.get('top_p', 'N/A')}")
                                            st.write(f"â€¢ **Top-k**: {file_data.get('top_k', 'N/A')}")
                                            st.write(f"â€¢ **ìƒ˜í”Œë§**: {file_data.get('do_sample', 'N/A')}")
                                    
                                    elif file == 'merges.txt':
                                        st.markdown("**ğŸ”€ BPE ë³‘í•© ì •ë³´**")
                                        st.write(f"â€¢ **ë³‘í•© ê·œì¹™ ìˆ˜**: {file_data.get('num_merges', 'N/A'):,}")
                                        if file_data.get('sample_merges'):
                                            st.markdown("**ğŸ“ ìƒ˜í”Œ ë³‘í•© ê·œì¹™**")
                                            for merge in file_data['sample_merges'][:5]:
                                                st.write(f"â€¢ `{merge}`")
                            else:
                                st.info("ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                if missing_files:
                    st.markdown("**âŒ ëˆ„ë½ëœ íŒŒì¼ë“¤**")
                    for file in missing_files:
                        with st.expander(f"âŒ {file} (ëˆ„ë½ë¨)", expanded=False):
                            # ëˆ„ë½ëœ íŒŒì¼ì— ëŒ€í•œ ì„¤ëª…
                            if file == 'pytorch_model.bin':
                                st.info("PyTorch í˜•ì‹ì˜ ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ì…ë‹ˆë‹¤. ëŒ€ì‹  model.safetensors íŒŒì¼ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.")
                            elif file == 'generation_config.json':
                                st.info("í…ìŠ¤íŠ¸ ìƒì„± ì„¤ì • íŒŒì¼ì…ë‹ˆë‹¤. ìƒì„± íƒœìŠ¤í¬ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê¸°ë³¸ ì„¤ì •ì„ ì •ì˜í•©ë‹ˆë‹¤.")
                            elif file == 'merges.txt':
                                st.info("BPE(Byte Pair Encoding) í† í¬ë‚˜ì´ì €ì˜ ë³‘í•© ê·œì¹™ íŒŒì¼ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ë‹¤ë¥¸ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                            else:
                                st.info(f"{file} íŒŒì¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ê¶Œì¥ì‚¬í•­
        if 'recommendations' in analysis and analysis['recommendations']:
            st.subheader("ğŸ’¡ ê¶Œì¥ì‚¬í•­")
            for i, rec in enumerate(analysis['recommendations'], 1):
                st.warning(f"**{i}.** {rec}")
    
    # ë¡œë“œëœ ëª¨ë¸ ëª©ë¡
    st.subheader("ğŸ“‹ ë¡œë“œëœ ëª¨ë¸ ëª©ë¡")
    models_status = st.session_state['model_manager'].get_all_models_status()
    
    if models_status:
        # ê° ëª¨ë¸ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ í‘œì‹œ
        for name, info in models_status.items():
            with st.expander(f"ğŸ¤– {name} - {info['status']}", expanded=info['status'] == 'error'):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**ğŸ“Š ê¸°ë³¸ ì •ë³´**")
                    
                    # ìƒíƒœì— ë”°ë¥¸ ìƒ‰ìƒ í‘œì‹œ
                    if info['status'] == 'loaded':
                        st.success(f"âœ… **ìƒíƒœ**: {info['status']}")
                    elif info['status'] == 'loading':
                        st.info(f"ğŸ”„ **ìƒíƒœ**: {info['status']}")
                    elif info['status'] == 'error':
                        st.error(f"âŒ **ìƒíƒœ**: {info['status']}")
                    else:
                        st.warning(f"âš ï¸ **ìƒíƒœ**: {info['status']}")
                    
                    st.write(f"ğŸ“ **ê²½ë¡œ**: `{info['path']}`")
                    st.write(f"ğŸ’¾ **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: {info['memory_usage']:.1f} MB")
                    st.write(f"â° **ë¡œë“œ ì‹œê°„**: {info['load_time'] if info['load_time'] else 'N/A'}")
                
                with col2:
                    st.markdown("**ğŸ” ìƒì„¸ ì •ë³´**")
                    
                    # ì—ëŸ¬ ì •ë³´ í‘œì‹œ
                    if info['status'] == 'error' and info['error_message']:
                        st.error(f"ğŸš¨ **ì—ëŸ¬ ë‚´ìš©**:")
                        st.code(info['error_message'], language='text')
                    
                    # ëª¨ë¸ ë¶„ì„ ì •ë³´ í‘œì‹œ
                    if info.get('config_analysis') and 'model_summary' in info['config_analysis']:
                        summary = info['config_analysis']['model_summary']
                        st.write(f"ğŸ—ï¸ **ëª¨ë¸ íƒ€ì…**: {summary.get('model_type', 'unknown')}")
                        st.write(f"ğŸ“Š **íŒŒë¼ë¯¸í„° ìˆ˜**: {summary.get('total_parameters', 0):,}")
                        
                        if summary.get('supported_tasks'):
                            st.write(f"ğŸ¯ **ì§€ì› íƒœìŠ¤í¬**: {', '.join(summary['supported_tasks'])}")
                    
                    # ëª¨ë¸ì´ ë¡œë“œëœ ê²½ìš° ì¶”ê°€ ì •ë³´
                    if info['status'] == 'loaded':
                        st.success("ğŸŸ¢ ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì–´ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                        
                        # ì¶”ë¡  ê°€ëŠ¥í•œ íƒœìŠ¤í¬ í‘œì‹œ
                        available_tasks = st.session_state['model_manager'].get_available_tasks(name)
                        if available_tasks:
                            st.write(f"ğŸš€ **ì‚¬ìš© ê°€ëŠ¥í•œ íƒœìŠ¤í¬**: {', '.join(available_tasks)}")
                
                # ëª¨ë¸ ì–¸ë¡œë“œ ë²„íŠ¼
                if info['status'] == 'loaded':
                    if st.button(f"ğŸ—‘ï¸ {name} ì–¸ë¡œë“œ", key=f"unload_{name}"):
                        if st.session_state['model_manager'].unload_model(name):
                            st.success(f"ëª¨ë¸ '{name}' ì–¸ë¡œë“œ ì™„ë£Œ!")
                            st.rerun()
                        else:
                            st.error(f"ëª¨ë¸ '{name}' ì–¸ë¡œë“œ ì‹¤íŒ¨!")
                elif info['status'] == 'error':
                    if st.button(f"ğŸ—‘ï¸ {name} ì œê±°", key=f"remove_{name}"):
                        if st.session_state['model_manager'].remove_model(name):
                            st.success(f"ëª¨ë¸ '{name}' ì œê±° ì™„ë£Œ!")
                            st.rerun()
                        else:
                            st.error(f"ëª¨ë¸ '{name}' ì œê±° ì‹¤íŒ¨!")
    else:
        st.info("ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")

# FastAPI ì„œë²„ UI
def render_fastapi_server():
    st.subheader("ğŸš€ FastAPI ì„œë²„")
    
    # ë¡œë“œëœ ëª¨ë¸ í™•ì¸
    loaded_models = st.session_state['model_manager'].get_loaded_models()
    loaded_models_count = len(loaded_models)
    
    # ìƒíƒœ ë°°ë„ˆ
    if st.session_state.get('fastapi_server_running', False):
        st.success("ğŸŸ¢ **ì„œë²„ ìƒíƒœ**: ì‹¤í–‰ ì¤‘")
    else:
        if loaded_models_count == 0:
            st.error("âŒ **ì„œë²„ ìƒíƒœ**: ë¡œë“œëœ ëª¨ë¸ì´ ì—†ì–´ ì‹œì‘í•  ìˆ˜ ì—†ìŒ")
        else:
            st.warning("ğŸŸ¡ **ì„œë²„ ìƒíƒœ**: ì¤‘ì§€ë¨ - ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”")
    
    # ì„œë²„ ì •ë³´
    server_info = st.session_state['fastapi_server'].get_server_info()
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # ì„œë²„ ìƒíƒœ í™•ì¸ (active_servers ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ì •)
        active_servers = server_info.get('active_servers', [])
        is_running = len(active_servers) > 0
        
        # ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œë„ í™•ì¸
        if not is_running:
            is_running = server_info.get('default_server_running', False) or st.session_state['fastapi_server'].is_running()
        
        st.metric("ì„œë²„ ìƒíƒœ", "ğŸŸ¢ ì‹¤í–‰ ì¤‘" if is_running else "ğŸ”´ ì¤‘ì§€ë¨")
    
    with col2:
        st.metric("ë¡œë“œëœ ëª¨ë¸ ìˆ˜", loaded_models_count)
    
    with col3:
        st.metric("í™œì„± í¬íŠ¸", len(st.session_state.get('model_ports', {})))
    
    # ëª¨ë¸ë³„ í¬íŠ¸ ì„¤ì •
    if loaded_models_count > 0:
        st.subheader("ğŸ”§ ëª¨ë¸ë³„ í¬íŠ¸ ì„¤ì •")
        
        # í¬íŠ¸ ì„¤ì • ì´ˆê¸°í™”
        if 'model_ports' not in st.session_state:
            st.session_state['model_ports'] = {}
        
        col_left, col_right = st.columns([2, 1])
        
        with col_left:
            for i, model_name in enumerate(loaded_models):
                col1, col2, col3 = st.columns([2, 1, 1])
                
                with col1:
                    st.write(f"ğŸ¤– **{model_name}**")
                
                with col2:
                    # ê¸°ë³¸ í¬íŠ¸ (8000ë¶€í„° ì‹œì‘)
                    default_port = 8000 + i
                    current_port = st.session_state['model_ports'].get(model_name, default_port)
                    
                    new_port = st.number_input(
                        f"í¬íŠ¸",
                        min_value=3000,
                        max_value=65535,
                        value=current_port,
                        key=f"port_{model_name}",
                        help=f"{model_name} ëª¨ë¸ì˜ API í¬íŠ¸"
                    )
                    
                    if new_port != current_port:
                        st.session_state['model_ports'][model_name] = new_port
                        # Rate-limit port change saves
                        if not hasattr(save_app_state, 'last_port_save'):
                            save_app_state.last_port_save = 0
                        if time.time() - save_app_state.last_port_save > 5:  # Save port changes only every 5 seconds
                            save_app_state()
                            save_app_state.last_port_save = time.time()
                
                with col3:
                    # ëª¨ë¸ë³„ ì„œë²„ ìƒíƒœ í™•ì¸
                    model_port = st.session_state['model_ports'].get(model_name)
                    model_running = False
                    
                    if model_port:
                        # í•´ë‹¹ í¬íŠ¸ì˜ ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
                        for server in active_servers:
                            if server['port'] == model_port:
                                model_running = True
                                break
                    
                    status = "ğŸŸ¢ ì‹¤í–‰ì¤‘" if model_running else "âšª ëŒ€ê¸°ì¤‘"
                    st.write(status)
        
        with col_right:
            st.info("ğŸ’¡ **í¬íŠ¸ ì„¤ì • íŒ**\n- ê° ëª¨ë¸ë§ˆë‹¤ ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©\n- ê¸°ë³¸ê°’: 8000, 8001, 8002...\n- ë²”ìœ„: 3000-65535")
    
    # ì„œë²„ ì œì–´
    st.subheader("ğŸ›ï¸ ì„œë²„ ì œì–´")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # ë¡œë“œëœ ëª¨ë¸ì´ ìˆì„ ë•Œë§Œ ì‹œì‘ ë²„íŠ¼ í™œì„±í™”
        start_disabled = loaded_models_count == 0
        start_button_text = "ğŸš€ ì„œë²„ ì‹œì‘" if not start_disabled else "âŒ ëª¨ë¸ ì—†ìŒ"
        
        if st.button(start_button_text, disabled=start_disabled):
            try:
                # ë©€í‹° í¬íŠ¸ ì§€ì› - ëª¨ë¸ë³„ í¬íŠ¸ ì„¤ì • ì‚¬ìš©
                model_ports = st.session_state.get('model_ports', {})
                if model_ports:
                    result = st.session_state['fastapi_server'].start_server(model_ports)
                    st.session_state['fastapi_server_running'] = True
                    # Server state changes are not critical to save immediately
                    st.success(result)
                    
                    # í¬íŠ¸ ì„¤ì • í™•ì¸ ë©”ì‹œì§€
                    active_ports = []
                    for model_name in loaded_models:
                        port = model_ports.get(model_name, 8000)
                        active_ports.append(f"{model_name}:{port}")
                    st.info(f"ğŸš€ ëª¨ë¸ë³„ í¬íŠ¸ ì„¤ì •: {', '.join(active_ports)}")
                else:
                    # ê¸°ë³¸ ë‹¨ì¼ í¬íŠ¸ ëª¨ë“œ
                    result = st.session_state['fastapi_server'].start_server()
                    st.session_state['fastapi_server_running'] = True
                    # Server state changes are not critical to save immediately
                    st.success(result)
                    st.info("ğŸš€ ëª¨ë“  ëª¨ë¸ì´ ê¸°ë³¸ í¬íŠ¸(8000)ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
                    
            except Exception as e:
                st.error(f"ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
        
        if start_disabled:
            st.caption("âš ï¸ ë¨¼ì € ëª¨ë¸ì„ ë¡œë“œí•˜ì„¸ìš”")
    
    with col2:
        if st.button("â¹ï¸ ì„œë²„ ì¤‘ì§€"):
            try:
                result = st.session_state['fastapi_server'].stop_server()
                st.session_state['fastapi_server_running'] = False
                # Server state changes are not critical to save immediately
                st.info(result)
            except Exception as e:
                st.error(f"ì„œë²„ ì¤‘ì§€ ì‹¤íŒ¨: {e}")
    
    with col3:
        if st.button("ğŸ§¹ ìºì‹œ ì •ë¦¬"):
            st.session_state['fastapi_server'].clear_pipeline_cache()
            save_app_state()  # ìƒíƒœ ì €ì¥
            st.success("íŒŒì´í”„ë¼ì¸ ìºì‹œê°€ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ì„œë²„ ì •ë³´ í‘œì‹œ
    # ì„œë²„ ì‹¤í–‰ ìƒíƒœ í™•ì¸ (í˜¸í™˜ì„± ì§€ì›)
    if 'running' in server_info:
        is_running = server_info['running']
    elif 'default_server_running' in server_info:
        is_running = server_info['default_server_running'] or len(server_info.get('active_servers', [])) > 0
    else:
        is_running = st.session_state['fastapi_server'].is_running()
    
    if is_running:
        st.subheader("ğŸ”— ì„œë²„ ì •ë³´")
        
        # ì„œë²„ URL í‘œì‹œ (ë©€í‹° í¬íŠ¸ ì§€ì›)
        if 'active_servers' in server_info and server_info['active_servers']:
            st.markdown("#### ğŸŒ í™œì„± ì„œë²„ ëª©ë¡")
            for server in server_info['active_servers']:
                models_text = ', '.join(server['models']) if isinstance(server['models'], list) else server['models']
                st.info(f"**í¬íŠ¸ {server['port']}** - ëª¨ë¸: {models_text}")
                st.caption(f"ğŸ“¡ API: {server['url']} | ğŸ“š ë¬¸ì„œ: {server['docs_url']}")
        else:
            # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ë³¸ í‘œì‹œ
            default_port = getattr(st.session_state['fastapi_server'], 'default_port', getattr(st.session_state['fastapi_server'], 'port', 8000))
            host = server_info.get('host', '127.0.0.1')
            st.info(f"**ğŸŒ ì„œë²„ URL:** http://{host}:{default_port}")
            st.info(f"**ğŸ“š API ë¬¸ì„œ:** http://{host}:{default_port}/docs")
        
        # ë¡œë“œëœ ëª¨ë¸ ëª©ë¡ í‘œì‹œ (í¬íŠ¸ ì •ë³´ í¬í•¨)
        if loaded_models:
            st.markdown("#### ğŸ¤– ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸")
            models_info = []
            for model_name in loaded_models:
                # ëª¨ë¸ì´ ì‹¤í–‰ ì¤‘ì¸ í¬íŠ¸ ì°¾ê¸° (ë©”ì„œë“œ ì¡´ì¬ í™•ì¸)
                if hasattr(st.session_state['fastapi_server'], 'get_model_server_port'):
                    model_port = st.session_state['fastapi_server'].get_model_server_port(model_name)
                else:
                    # ì´ì „ ë²„ì „ í˜¸í™˜ì„± - ê¸°ë³¸ í¬íŠ¸ ì‚¬ìš©
                    default_port = getattr(st.session_state['fastapi_server'], 'default_port', getattr(st.session_state['fastapi_server'], 'port', 8000))
                    model_port = default_port if st.session_state['fastapi_server'].is_running() else None
                
                port_info = f"í¬íŠ¸ {model_port}" if model_port else "ì˜¤í”„ë¼ì¸"
                endpoint_url = f"http://{server_info.get('host', '127.0.0.1')}:{model_port}/models/{model_name}/predict" if model_port else "N/A"
                
                models_info.append({
                    "ëª¨ë¸ëª…": model_name,
                    "í¬íŠ¸": port_info,
                    "ì—”ë“œí¬ì¸íŠ¸": f"/models/{model_name}/predict",
                    "ì „ì²´ URL": endpoint_url,
                    "ìƒíƒœ": "ğŸŸ¢ ë¡œë“œë¨" if model_port else "âšª ì˜¤í”„ë¼ì¸"
                })
            st.dataframe(pd.DataFrame(models_info), use_container_width=True)
        
        # ì—”ë“œí¬ì¸íŠ¸ ëª©ë¡
        st.subheader("ğŸ“¡ ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ë“œí¬ì¸íŠ¸")
        endpoints = st.session_state['fastapi_server'].get_available_endpoints()
        endpoints_data = []
        for ep in endpoints:
            endpoints_data.append({
                "ê²½ë¡œ": ep['path'],
                "ë©”ì„œë“œ": ', '.join(ep['methods']),
                "ì´ë¦„": ep['name'] or "N/A"
            })
        
        st.dataframe(pd.DataFrame(endpoints_data), use_container_width=True)
        
        # ìƒì„¸ API ì‚¬ìš©ë²• ê°€ì´ë“œ
        st.subheader("ğŸ“– API ì‚¬ìš©ë²• ê°€ì´ë“œ")
        
        # ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸ ì„¹ì…˜
        with st.expander("ğŸ” ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸", expanded=False):
            st.markdown("""
            ### ì„œë²„ ìƒíƒœ í™•ì¸
            ```bash
            # ì„œë²„ ê¸°ë³¸ ì •ë³´
            curl http://localhost:8002/
            
            # ì„œë²„ ìƒíƒœ í™•ì¸
            curl http://localhost:8002/health
            
            # ë¡œë“œëœ ëª¨ë¸ ëª©ë¡
            curl http://localhost:8002/models
            ```
            """)
        
        # ëª¨ë¸ë³„ ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸
        with st.expander("ğŸ¤– ëª¨ë¸ ì˜ˆì¸¡ API", expanded=True):
            if loaded_models:
                for model_name in loaded_models:
                    # ëª¨ë¸ í¬íŠ¸ ì°¾ê¸°
                    if hasattr(st.session_state['fastapi_server'], 'get_model_server_port'):
                        model_port = st.session_state['fastapi_server'].get_model_server_port(model_name)
                    else:
                        model_port = 8000
                    
                    st.markdown(f"#### {model_name}")
                    
                    # ëª¨ë¸ ìœ í˜•ì— ë”°ë¥¸ ì˜ˆì œ
                    if 'bge' in model_name.lower():
                        # BGE ì„ë² ë”© ëª¨ë¸
                        st.markdown(f"""
                        **ì„ë² ë”© ìƒì„± (BGE-M3)**
                        ```bash
                        curl -X POST "http://localhost:{model_port}/models/{model_name}/predict" \\
                             -H "Content-Type: application/json" \\
                             -d '{{"text": "Hello, world!"}}'
                        ```
                        
                        **Python ì˜ˆì œ:**
                        ```python
                        import requests
                        
                        response = requests.post(
                            "http://localhost:{model_port}/models/{model_name}/predict",
                            json={{"text": "Hello, world!"}}
                        )
                        result = response.json()
                        print(f"ì„ë² ë”© í¬ê¸°: {{len(result['result'][0][0])}}")
                        ```
                        """)
                    
                    elif 'sentiment' in model_name.lower() or 'classification' in model_name.lower():
                        # ê°ì • ë¶„ì„ ëª¨ë¸
                        st.markdown(f"""
                        **ê°ì • ë¶„ì„ (Sentiment Analysis)**
                        ```bash
                        curl -X POST "http://localhost:{model_port}/models/{model_name}/predict" \\
                             -H "Content-Type: application/json" \\
                             -d '{{"text": "I love this product!"}}'
                        ```
                        
                        **Python ì˜ˆì œ:**
                        ```python
                        import requests
                        
                        response = requests.post(
                            "http://localhost:{model_port}/models/{model_name}/predict",
                            json={{"text": "I love this product!"}}
                        )
                        result = response.json()
                        print(f"ê°ì •: {{result['result'][0]['label']}}")
                        print(f"ì‹ ë¢°ë„: {{result['result'][0]['score']:.4f}}")
                        ```
                        """)
                    
                    else:
                        # ê¸°ë³¸ ëª¨ë¸
                        st.markdown(f"""
                        **ê¸°ë³¸ ì˜ˆì¸¡ API**
                        ```bash
                        curl -X POST "http://localhost:{model_port}/models/{model_name}/predict" \\
                             -H "Content-Type: application/json" \\
                             -d '{{"text": "Input text here"}}'
                        ```
                        """)
        
        # ì‹œìŠ¤í…œ ê´€ë¦¬ ì—”ë“œí¬ì¸íŠ¸
        with st.expander("ğŸ”§ ì‹œìŠ¤í…œ ê´€ë¦¬ API", expanded=False):
            st.markdown("""
            ### ì‹œìŠ¤í…œ ì •ë¦¬
            ```bash
            # íŒŒì´í”„ë¼ì¸ ìºì‹œ ì •ë¦¬
            curl -X POST http://localhost:8002/system/cleanup
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            curl http://localhost:8002/system/memory
            
            # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
            curl http://localhost:8002/system/status
            ```
            
            ### ëª¨ë¸ ì •ë³´ í™•ì¸
            ```bash
            # íŠ¹ì • ëª¨ë¸ ì •ë³´
            curl http://localhost:8002/models/model_name
            
            # ëª¨ë¸ ë¶„ì„ ì •ë³´
            curl http://localhost:8002/models/model_name/analyze
            ```
            """)
        
        # JavaScript/ì›¹ ì˜ˆì œ
        with st.expander("ğŸŒ JavaScript/ì›¹ ì˜ˆì œ", expanded=False):
            st.markdown("""
            ### Fetch API ì‚¬ìš©
            ```javascript
            // ê°ì • ë¶„ì„ ì˜ˆì œ
            async function analyzeSentiment(text) {
                const response = await fetch('http://localhost:8002/models/multilingual-sentiment-analysis/predict', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({ text: text })
                });
                
                const result = await response.json();
                return result;
            }
            
            // ì‚¬ìš© ì˜ˆì œ
            analyzeSentiment("I love this product!")
                .then(result => {
                    console.log('ê°ì •:', result.result[0].label);
                    console.log('ì‹ ë¢°ë„:', result.result[0].score);
                });
            ```
            
            ### jQuery ì‚¬ìš©
            ```javascript
            $.ajax({
                url: 'http://localhost:8002/models/bge-m3/predict',
                method: 'POST',
                contentType: 'application/json',
                data: JSON.stringify({ text: "Hello, world!" }),
                success: function(response) {
                    console.log('ì„ë² ë”© ë²¡í„°:', response.result);
                }
            });
            ```
            """)
        
        # ì˜¤ë¥˜ ì²˜ë¦¬ ê°€ì´ë“œ
        with st.expander("âš ï¸ ì˜¤ë¥˜ ì²˜ë¦¬ ë° íŒ", expanded=False):
            st.markdown("""
            ### ì¼ë°˜ì ì¸ ì˜¤ë¥˜ ë° í•´ê²°ì±…
            
            **1. ì—°ê²° ì˜¤ë¥˜ (Connection Error)**
            - ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
            - ì˜¬ë°”ë¥¸ í¬íŠ¸ ë²ˆí˜¸ ì‚¬ìš© í™•ì¸
            - ë°©í™”ë²½ ì„¤ì • í™•ì¸
            
            **2. ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜**
            - ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
            - `/models` ì—”ë“œí¬ì¸íŠ¸ë¡œ ëª¨ë¸ ìƒíƒœ í™•ì¸
            
            **3. ì˜ˆì¸¡ ì˜¤ë¥˜**
            - ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
            - Content-Type í—¤ë”ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸
            - JSON í˜•ì‹ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸
            
            ### ì„±ëŠ¥ ìµœì í™” íŒ
            
            **1. ë°°ì¹˜ ì²˜ë¦¬**
            - ê°€ëŠ¥í•œ ê²½ìš° ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
            
            **2. ìºì‹œ í™œìš©**
            - íŒŒì´í”„ë¼ì¸ ìºì‹œê°€ ìë™ìœ¼ë¡œ í™œì„±í™”ë¨
            - ë™ì¼í•œ ëª¨ë¸ ì¬ì‚¬ìš© ì‹œ ì„±ëŠ¥ í–¥ìƒ
            
            **3. ë©”ëª¨ë¦¬ ê´€ë¦¬**
            - ì£¼ê¸°ì ìœ¼ë¡œ `/system/cleanup` í˜¸ì¶œ
            - GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            """)
        
        # ì–¸ì–´ë³„ SDK ì˜ˆì œ
        with st.expander("ğŸ”¨ ì–¸ì–´ë³„ SDK ì˜ˆì œ", expanded=False):
            st.markdown("""
            ### Python requests
            ```python
            import requests
            
            def predict_sentiment(text, model_port=8002):
                url = f"http://localhost:{model_port}/models/multilingual-sentiment-analysis/predict"
                response = requests.post(url, json={"text": text})
                return response.json()
            
            # ì‚¬ìš© ì˜ˆì œ
            result = predict_sentiment("I love this!")
            print(result['result'][0]['label'])
            ```
            
            ### Node.js axios
            ```javascript
            const axios = require('axios');
            
            async function predictSentiment(text, modelPort = 8002) {
                const url = `http://localhost:${modelPort}/models/multilingual-sentiment-analysis/predict`;
                const response = await axios.post(url, { text });
                return response.data;
            }
            
            // ì‚¬ìš© ì˜ˆì œ
            predictSentiment("I love this!")
                .then(result => console.log(result.result[0].label));
            ```
            
            ### Go
            ```go
            package main
            
            import (
                "bytes"
                "encoding/json"
                "fmt"
                "net/http"
            )
            
            func predictSentiment(text string, modelPort int) {
                url := fmt.Sprintf("http://localhost:%d/models/multilingual-sentiment-analysis/predict", modelPort)
                payload := map[string]string{"text": text}
                jsonPayload, _ := json.Marshal(payload)
                
                resp, err := http.Post(url, "application/json", bytes.NewBuffer(jsonPayload))
                if err != nil {
                    fmt.Println("Error:", err)
                    return
                }
                defer resp.Body.Close()
                
                var result map[string]interface{}
                json.NewDecoder(resp.Body).Decode(&result)
                fmt.Printf("ê²°ê³¼: %v\\n", result)
            }
            ```
            """)
        
        # ë¬¸ì„œ ë° ë¦¬ì†ŒìŠ¤
        with st.expander("ğŸ“š ë¬¸ì„œ ë° ë¦¬ì†ŒìŠ¤", expanded=False):
            st.markdown("""
            ### ìë™ ìƒì„± ë¬¸ì„œ
            - **Swagger UI**: ê° í¬íŠ¸ì˜ `/docs` ì—”ë“œí¬ì¸íŠ¸
            - **OpenAPI Schema**: ê° í¬íŠ¸ì˜ `/openapi.json` ì—”ë“œí¬ì¸íŠ¸
            
            ### ìœ ìš©í•œ ë§í¬
            - [FastAPI ê³µì‹ ë¬¸ì„œ](https://fastapi.tiangolo.com/)
            - [Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬](https://huggingface.co/transformers/)
            - [HuggingFace ëª¨ë¸ í—ˆë¸Œ](https://huggingface.co/models)
            """)
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¸í„°í˜ì´ìŠ¤
        st.subheader("ğŸ§ª API í…ŒìŠ¤íŠ¸")
        
        if loaded_models:
            selected_model = st.selectbox("í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ì„ íƒ", loaded_models)
            test_text = st.text_area("ì…ë ¥ í…ìŠ¤íŠ¸", "Hello, world!")
            
            # ì„ íƒëœ ëª¨ë¸ì˜ í¬íŠ¸ ì°¾ê¸° (ë©”ì„œë“œ ì¡´ì¬ í™•ì¸)
            if hasattr(st.session_state['fastapi_server'], 'get_model_server_port'):
                model_port = st.session_state['fastapi_server'].get_model_server_port(selected_model)
            else:
                # ì´ì „ ë²„ì „ í˜¸í™˜ì„± - ê¸°ë³¸ í¬íŠ¸ ì‚¬ìš©
                default_port = getattr(st.session_state['fastapi_server'], 'default_port', getattr(st.session_state['fastapi_server'], 'port', 8000))
                model_port = default_port if st.session_state['fastapi_server'].is_running() else None
            
            if model_port:
                host = server_info.get('host', '127.0.0.1')
                test_url = f"http://{host}:{model_port}"
                endpoint = f"{test_url}/models/{selected_model}/predict"
                
                st.info(f"ğŸ¯ **í…ŒìŠ¤íŠ¸ ëŒ€ìƒ:** {endpoint}")
                st.caption(f"ğŸ“¡ ëª¨ë¸ '{selected_model}'ì€ í¬íŠ¸ {model_port}ì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
            else:
                st.warning(f"âš ï¸ ëª¨ë¸ '{selected_model}'ì´ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹™ë‹ˆë‹¤.")
                endpoint = None
            
            if st.button("ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸", disabled=endpoint is None):
                if endpoint:
                    import requests
                    try:
                        response = requests.post(
                            endpoint,
                            json={"text": test_text}
                        )
                        if response.status_code == 200:
                            result = response.json()
                            st.success("âœ… ì˜ˆì¸¡ ì„±ê³µ!")
                            st.json(result)
                        else:
                            st.error(f"âŒ ì˜ˆì¸¡ ì‹¤íŒ¨: {response.status_code}")
                            st.error(response.text)
                    except Exception as e:
                        st.error(f"ğŸ”¥ ìš”ì²­ ì‹¤íŒ¨: {e}")
                        st.caption("ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
                else:
                    st.error("ëª¨ë¸ì´ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹™ë‹ˆë‹¤.")
        else:
            st.info("í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´ ë¨¼ì € ëª¨ë¸ì„ ë¡œë“œí•˜ì„¸ìš”.")

# ë©”ì¸ í•¨ìˆ˜
def main():
    st.title("ğŸš€ Hugging Face GUI")
    
    # ìŠ¤ë§ˆíŠ¸ ëª¨ë¸ ìƒíƒœ ì²´í¬ (ì½œë°± ëŒ€ì‹  ì§ì ‘ ê°ì§€)
    if should_perform_expensive_check('main_model_check', 5):
        smart_model_status_check()
    
    # ë¡œë”© ì™„ë£Œ ìƒíƒœ ì¦‰ì‹œ ì²´í¬ (ìŠ¤ë ˆë“œ ìƒíƒœ ë°˜ì˜)
    if st.session_state.get('load_complete', False):
        st.rerun()  # ì¦‰ì‹œ UI ê°±ì‹ í•˜ì—¬ ì™„ë£Œ ë©”ì‹œì§€ í‘œì‹œ
    
    # í˜„ì¬ ë¡œë”© ì¤‘ì¸ ëª¨ë¸ì´ ìˆìœ¼ë©´ ì£¼ê¸°ì  ì²´í¬ í™œì„±í™”
    if st.session_state.get('check_loading', False):
        models_status = st.session_state['model_manager'].get_all_models_status()
        loading_models = [name for name, info in models_status.items() if info['status'] == 'loading']
        if loading_models and should_perform_expensive_check('main_loading_check', 2):
            st.rerun()  # ë¡œë”© ì¤‘ì´ë©´ 2ì´ˆë§ˆë‹¤ ì²´í¬
    
    # í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ëŠ” ìë™ ë³µì› ë¡œì§
    if st.session_state['logged_in'] and not st.session_state.get('auto_restoration_done', False):
        # ë³µì› ìƒíƒœ ì¶”ì 
        restoration_success = []
        restoration_failed = []
        
        # ìºì‹œ ìƒíƒœ ë³µì›
        cache_scanned_state = st.session_state.get('cache_scanned', False)
        cache_info_exists = st.session_state.get('cache_info') is not None
        revisions_count = len(st.session_state.get('revisions_df', pd.DataFrame()))
        
        logger.info(f"ìºì‹œ ë³µì› ì²´í¬: cache_scanned={cache_scanned_state}, cache_info_exists={cache_info_exists}, revisions_count={revisions_count}")
        
        # cache_scanned=Trueì¸ë° ì‹¤ì œ ìºì‹œ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ìë™ ë³µì›
        if cache_scanned_state and (not cache_info_exists or revisions_count == 0):
            logger.info("ìºì‹œ ìŠ¤ìº”ë¨ ìƒíƒœì´ì§€ë§Œ cache_info ë˜ëŠ” revisions_df ì—†ìŒ - ìë™ ì¬ìŠ¤ìº”")
            try:
                scan_cache()
                restoration_success.append("ìºì‹œ ìë™ ë³µì›")
                logger.info("ìºì‹œ ìë™ ë³µì› ì„±ê³µ")
            except Exception as e:
                restoration_failed.append(f"ìºì‹œ ë³µì› ({str(e)})")
                st.session_state['cache_scanned'] = False
                logger.error(f"ìºì‹œ ìë™ ë³µì› ì‹¤íŒ¨: {e}")
        elif not cache_scanned_state and not cache_info_exists:
            logger.info("ì²« ë¡œê·¸ì¸ - ìë™ ìºì‹œ ìŠ¤ìº”")
            # ì²« ë¡œê·¸ì¸ ì‹œ ìë™ ìºì‹œ ìŠ¤ìº”
            try:
                scan_cache()
                st.session_state['cache_scanned'] = True
                logger.info("ì²« ë¡œê·¸ì¸ ìºì‹œ ìŠ¤ìº” ì™„ë£Œ")
            except Exception as e:
                st.session_state['cache_scanned'] = False
                logger.error(f"ì²« ë¡œê·¸ì¸ ìºì‹œ ìŠ¤ìº” ì‹¤íŒ¨: {e}")
        else:
            logger.info(f"ìºì‹œ ë³µì› ë¶ˆí•„ìš”: cache_scanned={cache_scanned_state}, cache_info_exists={cache_info_exists}, revisions_count={revisions_count}")
        
        # ëª¨ë‹ˆí„°ë§ ìƒíƒœ ë³µì›
        monitoring_active = st.session_state.get('monitoring_active', False)
        logger.info(f"ëª¨ë‹ˆí„°ë§ ë³µì› ì²´í¬: monitoring_active={monitoring_active}")
        
        if monitoring_active:
            try:
                if not st.session_state['system_monitor'].monitoring:
                    st.session_state['system_monitor'].start_monitoring()
                    restoration_success.append("ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§")
                    logger.info("ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ìë™ ë³µì› ì„±ê³µ")
                else:
                    logger.info("ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì´ë¯¸ ì‹¤í–‰ ì¤‘")
            except Exception as e:
                st.session_state['monitoring_active'] = False
                restoration_failed.append(f"ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ({str(e)})")
                logger.error(f"ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ìë™ ë³µì› ì‹¤íŒ¨: {e}")
        
        # FastAPI ì„œë²„ ìƒíƒœ ë³µì› ë° ë™ê¸°í™”
        server_running = st.session_state.get('fastapi_server_running', False)
        logger.info(f"ì„œë²„ ë³µì› ì²´í¬: fastapi_server_running={server_running}")
        
        try:
            # ì‹¤ì œ ì„œë²„ ìƒíƒœ í™•ì¸
            actual_running = st.session_state['fastapi_server'].is_running()
            logger.info(f"ì‹¤ì œ ì„œë²„ ìƒíƒœ: {actual_running}")
            
            # ì„¸ì…˜ ìƒíƒœì™€ ì‹¤ì œ ìƒíƒœ ë™ê¸°í™” (ìƒíƒœ ì €ì¥ ì—†ì´)
            if actual_running != server_running:
                st.session_state['fastapi_server_running'] = actual_running
                # Don't save state here as this is called during restoration
                logger.info(f"ì„œë²„ ìƒíƒœ ì´ˆê¸° ë™ê¸°í™”: {server_running} -> {actual_running}")
            
            if server_running and not actual_running:
                # ë³µì› ì‹œë„
                st.session_state['fastapi_server'].start_server()
                restoration_success.append("FastAPI ì„œë²„")
                logger.info("FastAPI ì„œë²„ ìë™ ë³µì› ì„±ê³µ")
            elif actual_running:
                logger.info("FastAPI ì„œë²„ ì´ë¯¸ ì‹¤í–‰ ì¤‘")
                
        except Exception as e:
            st.session_state['fastapi_server_running'] = False
            restoration_failed.append(f"FastAPI ì„œë²„ ({str(e)})")
            logger.error(f"FastAPI ì„œë²„ ìƒíƒœ í™•ì¸/ë³µì› ì‹¤íŒ¨: {e}")
        
        # ë³µì› ê²°ê³¼ ì €ì¥
        st.session_state['restoration_success'] = restoration_success
        st.session_state['restoration_failed'] = restoration_failed
        st.session_state['auto_restoration_done'] = True  # ë³µì› ì™„ë£Œ í”Œë˜ê·¸
        
        # ë³µì› í›„ í•œ ë²ˆë§Œ ìƒíƒœ ì €ì¥
        if restoration_success or restoration_failed:
            save_app_state()
    
    # ë³µì› ìƒíƒœ ì•Œë¦¼ í‘œì‹œ
    if st.session_state['logged_in']:
        restoration_success = st.session_state.get('restoration_success', [])
        restoration_failed = st.session_state.get('restoration_failed', [])
        
        if restoration_success:
            st.success(f"ğŸ”„ **ìë™ ë³µì›ë¨**: {', '.join(restoration_success)}")
        
        if restoration_failed:
            st.error(f"âŒ **ë³µì› ì‹¤íŒ¨**: {', '.join(restoration_failed)}")
            st.info("ğŸ’¡ ì‹¤íŒ¨í•œ ì„œë¹„ìŠ¤ëŠ” í•´ë‹¹ íƒ­ì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ë‹¤ì‹œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # ìƒíƒœ ì´ˆê¸°í™” ë²„íŠ¼
        if st.button("ğŸ”„ ëª¨ë“  ìƒíƒœ ì´ˆê¸°í™”"):
            delete_app_state()
            # clear_browser_storage()  # í˜„ì¬ ë¹„í™œì„±í™”
            st.session_state.clear()
            st.success("ëª¨ë“  ìƒíƒœê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.rerun()
    
    st.markdown("---")
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ“Š ì‹œìŠ¤í…œ ìš”ì•½")
        
        # ì‹œìŠ¤í…œ ìš”ì•½ ì •ë³´ (ìŠ¤ë§ˆíŠ¸ í´ë§)
        if should_perform_expensive_check('system_summary', 15):
            system_summary = st.session_state['model_manager'].get_system_summary()
            # ìºì‹œ ì‹œìŠ¤í…œ ìš”ì•½
            st.session_state['_cached_system_summary'] = system_summary
        else:
            # ìºì‹œëœ ìš”ì•½ ì‚¬ìš©
            system_summary = st.session_state.get('_cached_system_summary', {
                'loaded_models_count': 0,
                'total_models_count': 0, 
                'total_memory_usage_mb': 0.0
            })
        
        st.metric("ë¡œë“œëœ ëª¨ë¸", system_summary['loaded_models_count'])
        st.metric("ì´ ëª¨ë¸", system_summary['total_models_count'])
        st.metric("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰", f"{system_summary['total_memory_usage_mb']:.1f} MB")
        
        # ëª¨ë‹ˆí„°ë§ ìƒíƒœ
        if st.session_state.get('monitoring_active', False):
            st.success("ğŸŸ¢ ëª¨ë‹ˆí„°ë§ ì‹¤í–‰ ì¤‘")
        else:
            st.info("â¸ï¸ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€ë¨")
        
        # ìºì‹œ ìƒíƒœ
        cache_scanned = st.session_state.get('cache_scanned', False)
        cache_info_exists = st.session_state.get('cache_info') is not None
        if cache_scanned and cache_info_exists:
            st.success(f"ğŸŸ¢ ìºì‹œ ìŠ¤ìº”ë¨ ({len(st.session_state['revisions_df'])}ê°œ)")
        else:
            st.info(f"âš« ìºì‹œ ë¯¸ìŠ¤ìº” (scanned={cache_scanned}, info={cache_info_exists})")
        
        # ì„œë²„ ìƒíƒœ (ìŠ¤ë§ˆíŠ¸ í´ë§ - ì‹¤ì œ ì„œë²„ ìƒíƒœ í™•ì¸)
        if should_perform_expensive_check('server_status_check', 20):
            try:
                # API ì„œë²„ ìƒíƒœ í™•ì¸ ë° ë™ê¸°í™” (ë©€í‹°í¬íŠ¸ ì§€ì›)
                server_info = st.session_state['fastapi_server'].get_server_info()
                active_servers = server_info.get('active_servers', [])
                actual_server_running = len(active_servers) > 0
                session_server_running = st.session_state.get('fastapi_server_running', False)
                
                # ì„¸ì…˜ ìƒíƒœì™€ ì‹¤ì œ ìƒíƒœê°€ ë‹¤ë¥´ë©´ ë™ê¸°í™” (ìƒíƒœ ì €ì¥ ì—†ì´)
                if actual_server_running != session_server_running:
                    st.session_state['fastapi_server_running'] = actual_server_running
                    # Don't call save_app_state here as this runs every main() execution
                    logger.info(f"ì„œë²„ ìƒíƒœ ë™ê¸°í™”: session={session_server_running} -> actual={actual_server_running}")
                
                if actual_server_running:
                    st.success(f"ğŸŸ¢ API ì„œë²„ ì‹¤í–‰ ì¤‘ ({len(active_servers)}ê°œ í¬íŠ¸)")
                else:
                    st.info("â¸ï¸ API ì„œë²„ ì¤‘ì§€ë¨")
            except Exception as e:
                logger.error(f"ì„œë²„ ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {e}")
                st.error("âŒ ì„œë²„ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨")
        else:
            # ìºì‹œëœ ìƒíƒœ í‘œì‹œ (ë¹ ë¥¸ í‘œì‹œ)
            session_server_running = st.session_state.get('fastapi_server_running', False)
            if session_server_running:
                st.info("ğŸŸ¡ API ì„œë²„ ì‹¤í–‰ ì¤‘ (ìºì‹œë¨)")
            else:
                st.info("â¸ï¸ API ì„œë²„ ì¤‘ì§€ë¨ (ìºì‹œë¨)")
    
    # íƒ­ ìƒì„±
    tabs = st.tabs([
        "ğŸ” ë¡œê·¸ì¸ ë° ì‚¬ìš©ì",
        "ğŸ“ ìºì‹œ ê´€ë¦¬",
        "ğŸ–¥ï¸ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§",
        "ğŸ¤– ëª¨ë¸ ê´€ë¦¬",
        "ğŸš€ FastAPI ì„œë²„",
        "ğŸ› ë””ë²„ê·¸"
    ])
    
    # ì²« ë²ˆì§¸ íƒ­: ë¡œê·¸ì¸/ë¡œê·¸ì•„ì›ƒ ë° ì‚¬ìš©ì ì •ë³´
    with tabs[0]:
        st.session_state['current_active_tab'] = 'login'
        st.subheader("ğŸ” ë¡œê·¸ì¸ ë° ì‚¬ìš©ì ì •ë³´")
        
        if not st.session_state['logged_in']:
            st.text_input("Hugging Face í† í°:", key='input_token')
            st.button("ë¡œê·¸ì¸", on_click=login)
        else:
            st.success("âœ… ë¡œê·¸ì¸ ìƒíƒœ ìœ ì§€ë¨")
            st.button("ë¡œê·¸ì•„ì›ƒ", on_click=logout)
        
        if st.session_state['logged_in']:
            col1, col2 = st.columns(2)
            with col1:
                st.button("ğŸ‘¤ í˜„ì¬ ì‚¬ìš©ì ì •ë³´", on_click=show_whoami)
            with col2:
                st.button("ğŸŒ í™˜ê²½ ì •ë³´ ë³´ê¸°", on_click=show_env)
    
    # ë‘ ë²ˆì§¸ íƒ­: ìºì‹œ ê´€ë¦¬
    with tabs[1]:
        st.session_state['current_active_tab'] = 'cache_management'
        st.subheader("ğŸ“ ìºì‹œ ê´€ë¦¬")
        
        # ìƒíƒœ ë°°ë„ˆ (ë””ë²„ê¹… ì •ë³´ í¬í•¨)
        cache_scanned = st.session_state.get('cache_scanned', False)
        cache_info_exists = st.session_state.get('cache_info') is not None
        revisions_count = len(st.session_state.get('revisions_df', pd.DataFrame()))
        
        # Reduced logging for cache UI rendering (only when state changes)
        cache_state = (cache_scanned, cache_info_exists, revisions_count)
        if cache_state != st.session_state.get('_last_cache_state'):
            logger.info(f"ìºì‹œ UI ë Œë”ë§ - cache_scanned: {cache_scanned}, cache_info_exists: {cache_info_exists}, revisions_count: {revisions_count}")
            st.session_state['_last_cache_state'] = cache_state
        
        # ìºì‹œ ë°ì´í„°ê°€ ëª¨ë‘ ìˆëŠ” ê²½ìš°
        if cache_scanned and cache_info_exists and revisions_count > 0:
            st.success(f"ğŸŸ¢ **ìºì‹œ ìƒíƒœ**: {revisions_count}ê°œ í•­ëª© ìŠ¤ìº”ë¨")
        # ìºì‹œ ìŠ¤ìº” ìƒíƒœë§Œ ìˆê³  ì‹¤ì œ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°
        elif cache_scanned and (not cache_info_exists or revisions_count == 0):
            st.info(f"ğŸ”„ **ìºì‹œ ìƒíƒœ**: ë³µì› ì¤‘... (scanned={cache_scanned}, info={cache_info_exists}, count={revisions_count})")
            # ìë™ ë³µì› ì‹œë„
            try:
                scan_cache()
                st.rerun()
            except Exception as e:
                st.error(f"ìë™ ë³µì› ì‹¤íŒ¨: {e}")
        # ì™„ì „íˆ ìŠ¤ìº”ë˜ì§€ ì•Šì€ ê²½ìš°
        else:
            st.warning(f"ğŸŸ¡ **ìºì‹œ ìƒíƒœ**: ìŠ¤ìº”ë˜ì§€ ì•ŠìŒ - ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ìŠ¤ìº”í•˜ì„¸ìš”")
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            # ìºì‹œ ìŠ¤ìº” ë²„íŠ¼ - ì´ë¯¸ ìŠ¤ìº”ëœ ê²½ìš°ì—ë„ "ìºì‹œ ìŠ¤ìº”"ìœ¼ë¡œ í‘œì‹œí•˜ë˜ ì¬ìŠ¤ìº” ê¸°ëŠ¥ ìˆ˜í–‰
            button_text = "ğŸ” ìºì‹œ ìŠ¤ìº”" if not st.session_state.get('cache_scanned', False) else "ğŸ”„ ìºì‹œ ìŠ¤ìº”"
            if st.button(button_text):
                scan_cache()
                st.session_state['cache_scanned'] = True
                # Cache scan state will be saved by the background save mechanism
                st.rerun()
        
        with col2:
            # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥
            st.markdown("#### ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")
            download_input = st.text_input(
                "ëª¨ë¸ ID ë˜ëŠ” HuggingFace ë§í¬ ì…ë ¥:",
                placeholder="ì˜ˆ: microsoft/DialoGPT-medium ë˜ëŠ” https://huggingface.co/microsoft/DialoGPT-medium",
                help="HuggingFace ëª¨ë¸ IDë‚˜ URLì„ ì…ë ¥í•˜ë©´ ë¡œì»¬ ìºì‹œë¡œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤."
            )
            
            if st.button("ğŸ“¥ ë‹¤ìš´ë¡œë“œ + ìŠ¤ìº”", disabled=not download_input.strip()):
                if download_input.strip():
                    download_model_to_cache(download_input.strip(), auto_scan=True)
        
        if st.session_state['cache_info']:
            # AgGrid ì„¤ì •
            gb = GridOptionsBuilder.from_dataframe(st.session_state['revisions_df'])
            gb.configure_selection("multiple", use_checkbox=True, groupSelectsChildren=True)
            gb.configure_pagination(paginationAutoPageSize=True)
            gb.configure_side_bar()
            gb.configure_default_column(enablePivot=True, enableValue=True, enableRowGroup=True)
            
            gridOptions = gb.build()
            
            grid_response = AgGrid(
                st.session_state['revisions_df'],
                gridOptions=gridOptions,
                data_return_mode=DataReturnMode.FILTERED_AND_SORTED,
                update_mode=GridUpdateMode.SELECTION_CHANGED,
                fit_columns_on_grid_load=True,
                enable_enterprise_modules=True,
                height=400,
                width='100%',
                reload_data=False
            )
            
            selected = grid_response['selected_rows']
            selected_df = pd.DataFrame(selected)
            
            # ì„ íƒ ìš”ì•½
            if not selected_df.empty:
                selected_count = len(selected_df)
                total_size = selected_df['Size (MB)'].sum()
                st.write(f"ì„ íƒëœ í•­ëª©: {selected_count}ê°œ, ì´ ìš©ëŸ‰: {total_size:.2f} MB")
                
                # ì‚­ì œ í™•ì¸ ë° ë²„íŠ¼
                with st.expander("ì„ íƒí•œ ìºì‹œ ì‚­ì œ"):
                    st.warning(f"{selected_count}ê°œì˜ ìˆ˜ì • ë²„ì „ì„ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
                    if st.button("ì‚­ì œ í™•ì¸"):
                        delete_selected(selected_df)
                        save_app_state()  # ìƒíƒœ ì €ì¥
                        st.rerun()
            else:
                st.write("ì„ íƒëœ í•­ëª©: 0ê°œ, ì´ ìš©ëŸ‰: 0.00 MB")
    
    # ì„¸ ë²ˆì§¸ íƒ­: ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§
    with tabs[2]:
        # í˜„ì¬ íƒ­ì´ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ì„ì„ í‘œì‹œ
        st.session_state['current_active_tab'] = 'system_monitoring'
        render_system_monitoring()
    
    # ë„¤ ë²ˆì§¸ íƒ­: ëª¨ë¸ ê´€ë¦¬
    with tabs[3]:
        st.session_state['current_active_tab'] = 'model_management'
        render_model_management()
    
    # ë‹¤ì„¯ ë²ˆì§¸ íƒ­: FastAPI ì„œë²„
    with tabs[4]:
        st.session_state['current_active_tab'] = 'fastapi_server'
        render_fastapi_server()
    
    # ì—¬ì„¯ ë²ˆì§¸ íƒ­: ë””ë²„ê·¸ ì •ë³´
    with tabs[5]:
        st.session_state['current_active_tab'] = 'debug'
        st.subheader("ğŸ› ë””ë²„ê·¸ ì •ë³´")
        
        # í˜„ì¬ ì„¸ì…˜ ìƒíƒœ
        st.subheader("ğŸ“Š í˜„ì¬ ì„¸ì…˜ ìƒíƒœ")
        debug_state = {
            'cache_scanned': st.session_state.get('cache_scanned', 'NOT_SET'),
            'cache_info': st.session_state.get('cache_info') is not None,
            'revisions_df': len(st.session_state.get('revisions_df', pd.DataFrame())),
            'monitoring_active': st.session_state.get('monitoring_active', 'NOT_SET'),
            'fastapi_server_running': st.session_state.get('fastapi_server_running', 'NOT_SET'),
            'model_path_input': st.session_state.get('model_path_input', 'NOT_SET'),
            'state_loaded': st.session_state.get('state_loaded', 'NOT_SET')
        }
        st.json(debug_state)
        
        # ìƒíƒœ íŒŒì¼ ì •ë³´
        st.subheader("ğŸ“ ìƒíƒœ íŒŒì¼ ì •ë³´")
        if os.path.exists(STATE_FILE):
            st.success(f"âœ… ìƒíƒœ íŒŒì¼ ì¡´ì¬: {STATE_FILE}")
            try:
                with open(STATE_FILE, 'r', encoding='utf-8') as f:
                    file_content = json.load(f)
                st.json(file_content)
            except Exception as e:
                st.error(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        else:
            st.error(f"âŒ ìƒíƒœ íŒŒì¼ ì—†ìŒ: {STATE_FILE}")
        
        # ìˆ˜ë™ ì•¡ì…˜
        st.subheader("ğŸ”§ ìˆ˜ë™ ì•¡ì…˜")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸ”„ ìƒíƒœ ê°•ì œ ë¡œë“œ"):
                logger.info("ìˆ˜ë™ ìƒíƒœ ë¡œë“œ ìš”ì²­")
                load_enhanced_app_state()
                st.rerun()
        
        with col2:
            if st.button("ğŸ’¾ ìƒíƒœ ê°•ì œ ì €ì¥"):
                logger.info("ìˆ˜ë™ ìƒíƒœ ì €ì¥ ìš”ì²­")
                save_app_state()
                st.success("ìƒíƒœ ì €ì¥ ì™„ë£Œ")
        
        with col3:
            if st.button("ğŸ“ ìµœê·¼ ë¡œê·¸ ë³´ê¸°"):
                if os.path.exists('app_debug.log'):
                    with open('app_debug.log', 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                        # ë§ˆì§€ë§‰ 50ì¤„ë§Œ í‘œì‹œ
                        recent_logs = ''.join(lines[-50:]) if lines else "ë¡œê·¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
                    st.text_area("ìµœê·¼ ë¡œê·¸ (ìµœëŒ€ 50ì¤„)", recent_logs, height=300)
                else:
                    st.error("ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì‹¤ì‹œê°„ ìƒíƒœ ì¶”ì 
        st.subheader("ğŸ” ì‹¤ì‹œê°„ ìƒíƒœ ì¶”ì ")
        
        if st.button("ğŸ”„ ì‹¤ì‹œê°„ ìƒíƒœ ì²´í¬"):
            # í˜„ì¬ ìƒíƒœ í™•ì¸
            current_status = []
            
            # 1. íŒŒì¼ ì‹œìŠ¤í…œ ì²´í¬
            if os.path.exists(STATE_FILE):
                try:
                    with open(STATE_FILE, 'r') as f:
                        file_state = json.load(f)
                    current_status.append(f"âœ… ìƒíƒœ íŒŒì¼: cache_scanned={file_state.get('cache_scanned')}")
                except Exception as e:
                    current_status.append(f"âŒ ìƒíƒœ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
            else:
                current_status.append("âŒ ìƒíƒœ íŒŒì¼ ì—†ìŒ")
            
            # 2. ì„¸ì…˜ ìƒíƒœ ì²´í¬
            cache_scanned = st.session_state.get('cache_scanned', 'NOT_SET')
            cache_info = st.session_state.get('cache_info')
            current_status.append(f"ğŸ“Š ì„¸ì…˜ ìƒíƒœ: cache_scanned={cache_scanned}")
            current_status.append(f"ğŸ“Š ì„¸ì…˜ ìƒíƒœ: cache_info={'ì¡´ì¬' if cache_info else 'ì—†ìŒ'}")
            
            # 3. ë””ë ‰í† ë¦¬ ìƒíƒœ ì²´í¬
            from huggingface_hub import scan_cache_dir
            try:
                cache_dir_info = scan_cache_dir()
                current_status.append(f"ğŸ“ ìºì‹œ ë””ë ‰í† ë¦¬: {len(cache_dir_info.repos)}ê°œ ì €ì¥ì†Œ")
            except Exception as e:
                current_status.append(f"âŒ ìºì‹œ ë””ë ‰í† ë¦¬ ì˜¤ë¥˜: {e}")
            
            # ê²°ê³¼ í‘œì‹œ
            for status in current_status:
                if "âœ…" in status:
                    st.success(status)
                elif "âŒ" in status:
                    st.error(status)
                else:
                    st.info(status)

# ìƒíƒœ ë³µì› ì•Œë¦¼ ê´€ë¦¬
def show_restoration_status():
    """ë³µì›ëœ ìƒíƒœì— ëŒ€í•œ ì•Œë¦¼ í‘œì‹œ"""
    restored_items = []
    
    # ë³µì›ëœ ìƒíƒœ í™•ì¸
    if st.session_state.get('cache_scanned', False) and st.session_state['cache_info']:
        restored_items.append(f"ìºì‹œ ìŠ¤ìº” ({len(st.session_state['revisions_df'])}ê°œ í•­ëª©)")
    
    if st.session_state.get('monitoring_active', False):
        restored_items.append("ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§")
    
    if st.session_state.get('fastapi_server_running', False):
        restored_items.append("FastAPI ì„œë²„")
    
    if st.session_state.get('model_path_input', ''):
        restored_items.append(f"ëª¨ë¸ ê²½ë¡œ: {st.session_state['model_path_input']}")
    
    if st.session_state.get('current_model_analysis'):
        restored_items.append("ëª¨ë¸ ë¶„ì„ ê²°ê³¼")
    
    if restored_items:
        st.success(f"ğŸ”„ **ìƒíƒœ ë³µì›ë¨**: {', '.join(restored_items)}")
        return True
    return False

# í”„ë¡œê·¸ë¨ ì‹œì‘ ì‹œ ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸
if st.session_state['logged_in']:
    api.set_access_token(st.session_state['token'])

if __name__ == "__main__":
    main()
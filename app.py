import os
import streamlit as st
from huggingface_hub import HfApi, scan_cache_dir
import pandas as pd
from st_aggrid import AgGrid, GridOptionsBuilder, GridUpdateMode, DataReturnMode
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import time
import json
from datetime import datetime

# ìƒˆë¡œìš´ ëª¨ë“ˆë“¤ import
from model_manager import MultiModelManager
from system_monitor import SystemMonitor
from fastapi_server import FastAPIServer
from model_analyzer import ComprehensiveModelAnalyzer

# ë¡œê·¸ì¸ ìƒíƒœë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ ì„¤ì •
LOGIN_FILE = "login_token.txt"

# Hugging Face API ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
api = HfApi()

# ìƒˆë¡œìš´ ë§¤ë‹ˆì €ë“¤ ì´ˆê¸°í™”
if 'model_manager' not in st.session_state:
    st.session_state['model_manager'] = MultiModelManager()

if 'system_monitor' not in st.session_state:
    st.session_state['system_monitor'] = SystemMonitor()

if 'fastapi_server' not in st.session_state:
    st.session_state['fastapi_server'] = FastAPIServer(st.session_state['model_manager'])

if 'model_analyzer' not in st.session_state:
    st.session_state['model_analyzer'] = ComprehensiveModelAnalyzer()

# ë¡œê·¸ì¸ ìƒíƒœ ë³µì›
def load_login_token():
    if os.path.exists(LOGIN_FILE):
        with open(LOGIN_FILE, "r") as f:
            token = f.read().strip()
            api.set_access_token(token)
            return token
    return None

# ë¡œê·¸ì¸ í† í° ì €ì¥
def save_login_token(token):
    with open(LOGIN_FILE, "w") as f:
        f.write(token)

# ë¡œê·¸ì¸ í† í° ì‚­ì œ
def delete_login_token():
    if os.path.exists(LOGIN_FILE):
        os.remove(LOGIN_FILE)

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Hugging Face GUI",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'token' not in st.session_state:
    st.session_state['token'] = load_login_token()

if 'logged_in' not in st.session_state:
    st.session_state['logged_in'] = st.session_state['token'] is not None

if 'cache_info' not in st.session_state:
    st.session_state['cache_info'] = None

if 'revisions_df' not in st.session_state:
    st.session_state['revisions_df'] = pd.DataFrame()

if 'current_model_analysis' not in st.session_state:
    st.session_state['current_model_analysis'] = None

# ë¡œê·¸ì¸ ê¸°ëŠ¥
def login():
    token = st.session_state['input_token'].strip()
    if token:
        try:
            api.set_access_token(token)
            save_login_token(token)
            st.session_state['token'] = token
            st.session_state['logged_in'] = True
            st.success("ë¡œê·¸ì¸ ì„±ê³µ!")
        except Exception as e:
            st.error(f"ë¡œê·¸ì¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
    else:
        st.error("ìœ íš¨í•œ í† í°ì„ ì…ë ¥í•˜ì„¸ìš”.")

# ë¡œê·¸ì•„ì›ƒ ê¸°ëŠ¥
def logout():
    api.set_access_token(None)
    delete_login_token()
    st.session_state['token'] = None
    st.session_state['logged_in'] = False
    st.success("ë¡œê·¸ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤.")

# í™˜ê²½ ì •ë³´ ì¶œë ¥
def show_env():
    try:
        env_info = api.whoami()
        st.write(f"í™˜ê²½ ì •ë³´: {env_info}")
    except Exception as e:
        st.error(f"í™˜ê²½ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}")

# í˜„ì¬ ì‚¬ìš©ì ì •ë³´ ì¶œë ¥
def show_whoami():
    try:
        user_info = api.whoami()
        st.write(f"ì‚¬ìš©ì: {user_info['name']}")
    except Exception as e:
        st.error(f"ì‚¬ìš©ì ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}")

# ìºì‹œ ì •ë³´ ìŠ¤ìº” ë° í™”ë©´ì— í‘œì‹œí•˜ëŠ” ê¸°ëŠ¥
def scan_cache():
    cache_info = scan_cache_dir()
    st.session_state['cache_info'] = cache_info

    # ìºì‹œ ë°ì´í„° ìˆ˜ì§‘
    revisions = []
    for repo in cache_info.repos:
        for revision in repo.revisions:
            rev_info = {
                "Repo ID": repo.repo_id,
                "Revision": revision.commit_hash[:7],
                "Size (MB)": round(revision.size_on_disk / (1024 ** 2), 2),
                "Last Modified": revision.last_modified,
                "Full Revision": revision.commit_hash,
            }
            revisions.append(rev_info)
    st.session_state['revisions_df'] = pd.DataFrame(revisions)

# ì„ íƒí•œ ìºì‹œ í•­ëª© ì‚­ì œ
def delete_selected(selected_rows):
    if selected_rows.empty:
        st.info("ì‚­ì œí•  í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš”.")
        return

    selected_revisions = selected_rows['Full Revision'].tolist()

    # ì‚­ì œ ì‹¤í–‰
    delete_strategy = st.session_state['cache_info'].delete_revisions(*selected_revisions)
    delete_strategy.execute()

    # ì‚­ì œ í›„ ìºì‹œ ëª©ë¡ ìƒˆë¡œê³ ì¹¨
    scan_cache()

    st.success("ì„ íƒí•œ ìºì‹œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")

# ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ UI
def render_system_monitoring():
    st.subheader("ğŸ–¥ï¸ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§")
    
    # ìë™ ê°±ì‹  ì„¤ì • ì´ˆê¸°í™”
    if 'auto_refresh_interval' not in st.session_state:
        st.session_state['auto_refresh_interval'] = 0
    if 'last_refresh_time' not in st.session_state:
        st.session_state['last_refresh_time'] = 0
    
    # ëª¨ë‹ˆí„°ë§ ë° ìë™ ê°±ì‹  ì„¤ì •
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if st.button("ğŸš€ ëª¨ë‹ˆí„°ë§ ì‹œì‘"):
            st.session_state['system_monitor'].start_monitoring()
            st.success("ëª¨ë‹ˆí„°ë§ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    with col2:
        if st.button("â¹ï¸ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"):
            st.session_state['system_monitor'].stop_monitoring()
            st.session_state['auto_refresh_interval'] = 0
            st.info("ëª¨ë‹ˆí„°ë§ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    with col3:
        if st.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨"):
            st.rerun()
    
    with col4:
        # ìë™ ê°±ì‹  ê°„ê²© ì„¤ì •
        refresh_options = {
            "ìë™ ê°±ì‹  ë„ê¸°": 0,
            "1ì´ˆë§ˆë‹¤": 1,
            "3ì´ˆë§ˆë‹¤": 3,
            "10ì´ˆë§ˆë‹¤": 10
        }
        selected_refresh = st.selectbox(
            "ìë™ ê°±ì‹ ",
            options=list(refresh_options.keys()),
            index=0
        )
        st.session_state['auto_refresh_interval'] = refresh_options[selected_refresh]
    
    # ìë™ ê°±ì‹  ë¡œì§
    if st.session_state['auto_refresh_interval'] > 0:
        import time
        current_time = time.time()
        
        # ìë™ ê°±ì‹  ìƒíƒœ í‘œì‹œ
        st.info(f"ğŸ”„ {st.session_state['auto_refresh_interval']}ì´ˆë§ˆë‹¤ ìë™ ê°±ì‹  ì¤‘...")
        
        # ì§€ì •ëœ ê°„ê²©ì´ ì§€ë‚¬ìœ¼ë©´ ê°±ì‹ 
        if current_time - st.session_state['last_refresh_time'] >= st.session_state['auto_refresh_interval']:
            st.session_state['last_refresh_time'] = current_time
            st.rerun()
        
        # ë‹¤ìŒ ê°±ì‹ ê¹Œì§€ ë‚¨ì€ ì‹œê°„ ê³„ì‚° ë° í‘œì‹œ
        remaining_time = st.session_state['auto_refresh_interval'] - (current_time - st.session_state['last_refresh_time'])
        if remaining_time > 0:
            st.write(f"â° ë‹¤ìŒ ê°±ì‹ ê¹Œì§€: {remaining_time:.1f}ì´ˆ")
            # í˜ì´ì§€ ìë™ ê°±ì‹ ì„ ìœ„í•œ JavaScript ì¶”ê°€
            st.markdown(f"""
                <script>
                setTimeout(function(){{
                    window.location.reload();
                }}, {remaining_time * 1000});
                </script>
            """, unsafe_allow_html=True)
    
    # í˜„ì¬ ìƒíƒœ í‘œì‹œ
    if st.session_state['system_monitor'].monitoring or st.button("í˜„ì¬ ìƒíƒœ ë³´ê¸°"):
        current_data = st.session_state['system_monitor'].get_current_data()
        
        # ë©”íŠ¸ë¦­ ì¹´ë“œë“¤
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                label="ğŸ”§ CPU ì‚¬ìš©ë¥ ",
                value=f"{current_data['cpu']['percent']:.1f}%",
                delta=f"{current_data['cpu']['frequency']:.0f} MHz"
            )
        
        with col2:
            memory_gb = current_data['memory']['used'] / (1024**3)
            total_gb = current_data['memory']['total'] / (1024**3)
            st.metric(
                label="ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ",
                value=f"{current_data['memory']['percent']:.1f}%",
                delta=f"{memory_gb:.1f}/{total_gb:.1f} GB"
            )
        
        with col3:
            if current_data['gpu']:
                avg_gpu = sum(gpu['load'] for gpu in current_data['gpu']) / len(current_data['gpu'])
                st.metric(
                    label="ğŸ® GPU í‰ê·  ì‚¬ìš©ë¥ ",
                    value=f"{avg_gpu:.1f}%",
                    delta=f"{len(current_data['gpu'])} GPU(s)"
                )
            else:
                st.metric(label="ğŸ® GPU", value="N/A", delta="No GPU detected")
        
        with col4:
            disk_gb = current_data['disk']['used'] / (1024**3)
            total_disk_gb = current_data['disk']['total'] / (1024**3)
            st.metric(
                label="ğŸ’¿ ë””ìŠ¤í¬ ì‚¬ìš©ë¥ ",
                value=f"{current_data['disk']['percent']:.1f}%",
                delta=f"{disk_gb:.1f}/{total_disk_gb:.1f} GB"
            )
        
        # GPU ìƒì„¸ ì •ë³´
        if current_data['gpu']:
            st.subheader("ğŸ® GPU ìƒì„¸ ì •ë³´")
            gpu_data = []
            for gpu in current_data['gpu']:
                gpu_data.append({
                    "GPU ID": gpu['id'],
                    "ì´ë¦„": gpu['name'],
                    "ì‚¬ìš©ë¥ ": f"{gpu['load']:.1f}%",
                    "ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ": f"{gpu['memory_util']:.1f}%",
                    "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰": f"{gpu['memory_used']:.0f}/{gpu['memory_total']:.0f} MB",
                    "ì˜¨ë„": f"{gpu['temperature']}Â°C"
                })
            
            st.dataframe(pd.DataFrame(gpu_data), use_container_width=True)
        
        # ì°¨íŠ¸ í‘œì‹œ
        if st.session_state['system_monitor'].monitoring:
            render_system_charts()
        
        # ì•Œë¦¼ í‘œì‹œ
        alerts = st.session_state['system_monitor'].get_alerts()
        if alerts:
            st.subheader("âš ï¸ ì‹œìŠ¤í…œ ì•Œë¦¼")
            for alert in alerts:
                if alert['type'] == 'error':
                    st.error(f"ğŸ”¥ {alert['message']}")
                elif alert['type'] == 'warning':
                    st.warning(f"âš ï¸ {alert['message']}")

def render_system_charts():
    """ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì°¨íŠ¸ ë Œë”ë§"""
    history = st.session_state['system_monitor'].get_history()
    
    if not history['cpu']:
        st.info("ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        return
    
    # ì°¨íŠ¸ ìƒì„±
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('CPU ì‚¬ìš©ë¥ ', 'ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ', 'GPU ì‚¬ìš©ë¥ ', 'ë””ìŠ¤í¬ ì‚¬ìš©ë¥ '),
        specs=[[{"secondary_y": False}, {"secondary_y": False}],
               [{"secondary_y": False}, {"secondary_y": False}]]
    )
    
    # CPU ì°¨íŠ¸
    cpu_df = pd.DataFrame(history['cpu'])
    fig.add_trace(
        go.Scatter(
            x=cpu_df['timestamp'], 
            y=cpu_df['percent'], 
            name='CPU %',
            line=dict(color='blue')
        ),
        row=1, col=1
    )
    
    # ë©”ëª¨ë¦¬ ì°¨íŠ¸
    memory_df = pd.DataFrame(history['memory'])
    fig.add_trace(
        go.Scatter(
            x=memory_df['timestamp'], 
            y=memory_df['percent'], 
            name='Memory %',
            line=dict(color='red')
        ),
        row=1, col=2
    )
    
    # GPU ì°¨íŠ¸
    if history['gpu'] and history['gpu'][0]:
        gpu_data = []
        for gpu_snapshot in history['gpu']:
            for gpu in gpu_snapshot:
                gpu_data.append({
                    'timestamp': gpu['timestamp'],
                    'gpu_id': gpu['gpu_id'],
                    'load': gpu['load']
                })
        
        if gpu_data:
            gpu_df = pd.DataFrame(gpu_data)
            colors = ['green', 'orange', 'purple', 'brown']
            for i, gpu_id in enumerate(gpu_df['gpu_id'].unique()):
                gpu_subset = gpu_df[gpu_df['gpu_id'] == gpu_id]
                fig.add_trace(
                    go.Scatter(
                        x=gpu_subset['timestamp'], 
                        y=gpu_subset['load'], 
                        name=f'GPU {gpu_id}',
                        line=dict(color=colors[i % len(colors)])
                    ),
                    row=2, col=1
                )
    
    # ë””ìŠ¤í¬ ì°¨íŠ¸
    disk_df = pd.DataFrame(history['disk'])
    fig.add_trace(
        go.Scatter(
            x=disk_df['timestamp'], 
            y=disk_df['percent'], 
            name='Disk %',
            line=dict(color='purple')
        ),
        row=2, col=2
    )
    
    fig.update_layout(height=600, showlegend=True)
    st.plotly_chart(fig, use_container_width=True)

# ëª¨ë¸ ê´€ë¦¬ UI
def render_model_management():
    st.subheader("ğŸ¤– ëª¨ë¸ ê´€ë¦¬")
    
    # ëª¨ë¸ ë¡œë“œ ì„¹ì…˜
    st.subheader("ğŸ“¥ ëª¨ë¸ ë¡œë“œ")
    
    # ìºì‹œëœ ëª¨ë¸ì—ì„œ ì„ íƒ ì˜µì…˜ ì¶”ê°€
    if st.session_state['cache_info']:
        st.subheader("ğŸ—‚ï¸ ìºì‹œëœ ëª¨ë¸ì—ì„œ ì„ íƒ")
        cached_models = []
        for repo in st.session_state['cache_info'].repos:
            cached_models.append(repo.repo_id)
        
        if cached_models:
            selected_cached_model = st.selectbox(
                "ìºì‹œëœ ëª¨ë¸ ì„ íƒ (ì„ íƒì‚¬í•­)", 
                options=["ì§ì ‘ ì…ë ¥"] + cached_models,
                index=0
            )
            
            if selected_cached_model != "ì§ì ‘ ì…ë ¥":
                # ìºì‹œëœ ëª¨ë¸ ì„ íƒ ì‹œ ìë™ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •
                st.session_state['model_path_input'] = selected_cached_model
                st.success(f"ì„ íƒëœ ëª¨ë¸: {selected_cached_model}")
    
    model_path = st.text_input("ëª¨ë¸ ê²½ë¡œ (ë¡œì»¬ ê²½ë¡œ ë˜ëŠ” HuggingFace ëª¨ë¸ ID)", key="model_path_input", placeholder="ì˜ˆ: tabularisai/multilingual-sentiment-analysis")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("ğŸ” ëª¨ë¸ ë¶„ì„"):
            if model_path:
                with st.spinner("ëª¨ë¸ ë¶„ì„ ì¤‘..."):
                    analysis = st.session_state['model_manager'].analyze_model(model_path)
                    if 'error' in analysis:
                        st.error(f"ëª¨ë¸ ë¶„ì„ ì‹¤íŒ¨: {analysis['error']}")
                    else:
                        st.session_state['current_model_analysis'] = analysis
                        st.success("ëª¨ë¸ ë¶„ì„ ì™„ë£Œ!")
            else:
                st.error("ëª¨ë¸ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    
    with col2:
        if st.button("ğŸ“¤ ëª¨ë¸ ë¡œë“œ"):
            if model_path:
                # ëª¨ë¸ ì´ë¦„ ìë™ ìƒì„±
                model_name = ""
                
                def load_callback(name, success, message):
                    # ì½œë°±ì—ì„œ ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                    if success:
                        st.session_state['load_success'] = f"ëª¨ë¸ '{name}' ë¡œë“œ ì„±ê³µ!"
                        st.session_state['load_complete'] = True
                    else:
                        st.session_state['load_error'] = f"ëª¨ë¸ '{name}' ë¡œë“œ ì‹¤íŒ¨: {message}"
                        st.session_state['load_complete'] = True
                
                st.session_state['model_manager'].load_model_async(
                    model_name, model_path, load_callback
                )
                st.info(f"ëª¨ë¸ ë¡œë“œ ì‹œì‘... (ì´ë¦„: ìë™ ìƒì„±)")
                
                # ìë™ ìƒˆë¡œê³ ì¹¨ ì²´í¬ ì‹œì‘
                st.session_state['check_loading'] = True
                
                # ìºì‹œ ìë™ ìŠ¤ìº” (HuggingFace ëª¨ë¸ IDì¸ ê²½ìš°)
                if st.session_state['model_manager']._is_huggingface_model_id(model_path):
                    st.info("ğŸ”„ HuggingFace ëª¨ë¸ ê°ì§€ - ìºì‹œ ìë™ ê°±ì‹  ì¤‘...")
                    scan_cache()
            else:
                st.error("ëª¨ë¸ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    
    with col3:
        if st.button("ğŸ”„ ìƒíƒœ ìƒˆë¡œê³ ì¹¨"):
            st.rerun()
    
    # ë¡œë“œ ì™„ë£Œ ë©”ì‹œì§€ í‘œì‹œ ë° ìë™ ìƒˆë¡œê³ ì¹¨
    if st.session_state.get('load_complete', False):
        if 'load_success' in st.session_state:
            st.success(st.session_state['load_success'])
            del st.session_state['load_success']
        if 'load_error' in st.session_state:
            st.error(st.session_state['load_error'])
            del st.session_state['load_error']
        st.session_state['load_complete'] = False
        st.rerun()
    
    # í˜ì´ì§€ ë¡œë“œ ì‹œ ìë™ ìƒˆë¡œê³ ì¹¨ ì²´í¬ (ë¡œë”© ìƒíƒœ í´ë§)
    if st.session_state.get('check_loading', False):
        # ì§§ì€ ê°„ê²©ìœ¼ë¡œ ëª¨ë¸ ìƒíƒœ í™•ì¸
        import time
        time.sleep(1)
        
        # ëª¨ë¸ ìƒíƒœ í™•ì¸
        models_status = st.session_state['model_manager'].get_all_models_status()
        loading_models = [name for name, info in models_status.items() if info['status'] == 'loading']
        
        if not loading_models:
            # ë¡œë”© ì¤‘ì¸ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ì²´í¬ ì¤‘ë‹¨
            st.session_state['check_loading'] = False
        
        # ìë™ ìƒˆë¡œê³ ì¹¨
        st.rerun()
    
    # ëª¨ë¸ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
    if st.session_state['current_model_analysis']:
        st.subheader("ğŸ“Š ëª¨ë¸ ë¶„ì„ ê²°ê³¼")
        analysis = st.session_state['current_model_analysis']
        
        # ì›ë³¸ ê²½ë¡œì™€ ì‹¤ì œ ê²½ë¡œ í‘œì‹œ
        if 'original_path' in analysis and 'actual_path' in analysis:
            if analysis['original_path'] != analysis['actual_path']:
                st.info(f"ğŸ”— HuggingFace ëª¨ë¸ ID: `{analysis['original_path']}`")
                st.info(f"ğŸ“ ë¡œì»¬ ìºì‹œ ê²½ë¡œ: `{analysis['actual_path']}`")
        
        # ëª¨ë¸ ê¸°ë³¸ ì •ë³´
        if 'model_summary' in analysis:
            summary = analysis['model_summary']
            
            # ë©”íŠ¸ë¦­ ì¹´ë“œë“¤
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("ğŸ—ï¸ ëª¨ë¸ íƒ€ì…", summary.get('model_type', 'unknown'))
            with col2:
                st.metric("ğŸ“Š íŒŒë¼ë¯¸í„° ìˆ˜", f"{summary.get('total_parameters', 0):,}")
            with col3:
                st.metric("ğŸ’¾ ëª¨ë¸ í¬ê¸°", f"{summary.get('model_size_mb', 0):.1f} MB")
            with col4:
                st.metric("ğŸ“– ì–´íœ˜ í¬ê¸°", f"{summary.get('vocabulary_size', 0):,}")
            
            # ì¶”ê°€ ìƒì„¸ ì •ë³´
            st.subheader("ğŸ” ëª¨ë¸ ìƒì„¸ ì •ë³´")
            
            info_cols = st.columns(2)
            with info_cols[0]:
                st.markdown("**ğŸ·ï¸ ëª¨ë¸ ì •ë³´**")
                config = summary.get('detailed_config', {})
                st.write(f"â€¢ **ì•„í‚¤í…ì²˜**: {config.get('architecture', 'N/A')}")
                st.write(f"â€¢ **íˆë“  ì‚¬ì´ì¦ˆ**: {config.get('hidden_size', 'N/A')}")
                st.write(f"â€¢ **ì–´í…ì…˜ í—¤ë“œ**: {config.get('num_attention_heads', 'N/A')}")
                st.write(f"â€¢ **ë ˆì´ì–´ ìˆ˜**: {config.get('num_hidden_layers', 'N/A')}")
                st.write(f"â€¢ **ì¤‘ê°„ ë ˆì´ì–´ í¬ê¸°**: {config.get('intermediate_size', 'N/A')}")
                st.write(f"â€¢ **í™œì„±í™” í•¨ìˆ˜**: {config.get('activation_function', 'N/A')}")
                
            with info_cols[1]:
                st.markdown("**âš™ï¸ ì„¤ì • ì •ë³´**")
                st.write(f"â€¢ **ìµœëŒ€ ìœ„ì¹˜**: {config.get('max_position_embeddings', 'N/A')}")
                st.write(f"â€¢ **ë“œë¡­ì•„ì›ƒ**: {config.get('dropout', 'N/A')}")
                st.write(f"â€¢ **ì–´í…ì…˜ ë“œë¡­ì•„ì›ƒ**: {config.get('attention_dropout', 'N/A')}")
                st.write(f"â€¢ **ì´ˆê¸°í™” ë²”ìœ„**: {config.get('initializer_range', 'N/A')}")
                st.write(f"â€¢ **ë ˆì´ì–´ ë…¸ë¦„ ì—¡ì‹¤ë¡ **: {config.get('layer_norm_eps', 'N/A')}")
                st.write(f"â€¢ **í† í¬ë‚˜ì´ì € ìµœëŒ€ ê¸¸ì´**: {config.get('tokenizer_max_length', 'N/A')}")
        
        # ì§€ì› íƒœìŠ¤í¬ì™€ ì‚¬ìš© ì˜ˆì‹œ
        if 'model_summary' in analysis and 'supported_tasks' in analysis['model_summary']:
            tasks = analysis['model_summary']['supported_tasks']
            usage_examples = analysis['model_summary'].get('usage_examples', {})
            
            if tasks:
                st.subheader("ğŸ¯ ì§€ì› íƒœìŠ¤í¬ ë° ì‚¬ìš© ë°©ë²•")
                
                for task in tasks:
                    with st.expander(f"ğŸ“‹ {task}", expanded=False):
                        if task in usage_examples:
                            example = usage_examples[task]
                            
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                st.markdown("**ğŸ“ ì„¤ëª…**")
                                st.write(example['description'])
                                
                                st.markdown("**ğŸ”§ ì…ë ¥ ì˜ˆì‹œ**")
                                st.code(example['example_input'], language='python')
                                
                                st.markdown("**ğŸ“¤ ì¶œë ¥ ì˜ˆì‹œ**")
                                st.code(example['expected_output'], language='json')
                                
                                # íŒŒë¼ë¯¸í„° ì •ë³´ í‘œì‹œ
                                if 'parameters' in example:
                                    st.markdown("**âš™ï¸ ì£¼ìš” íŒŒë¼ë¯¸í„°**")
                                    params = example['parameters']
                                    for param, value in params.items():
                                        if param != 'special_tokens':
                                            st.write(f"â€¢ **{param}**: `{value}`")
                                    
                                    if params.get('special_tokens'):
                                        st.markdown("**ğŸ¯ íŠ¹ìˆ˜ í† í°**")
                                        for token_name, token_value in list(params['special_tokens'].items())[:3]:
                                            st.write(f"â€¢ **{token_name}**: `{token_value}`")
                            
                            with col2:
                                st.markdown("**ğŸ’» ì½”ë“œ ì˜ˆì‹œ**")
                                st.code(example['example_code'], language='python')
                        else:
                            st.success(f"âœ… {task} íƒœìŠ¤í¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.")
        
        # ë°œê²¬ëœ íŒŒì¼ë“¤ (ì ‘ê¸°/í¼ì¹˜ê¸° ê°€ëŠ¥)
        if 'files_found' in analysis:
            with st.expander("ğŸ“ ë°œê²¬ëœ íŒŒì¼ë“¤ ìƒì„¸ ë¶„ì„", expanded=False):
                found_files = analysis['files_found']
                missing_files = analysis['files_missing']
                analysis_results = analysis.get('analysis_results', {})
                
                if found_files:
                    st.markdown("**âœ… ë°œê²¬ëœ íŒŒì¼ë“¤**")
                    
                    for file in found_files:
                        with st.expander(f"ğŸ” {file} ë¶„ì„ ê²°ê³¼", expanded=False):
                            if file in analysis_results:
                                file_data = analysis_results[file]
                                
                                if 'error' in file_data:
                                    st.error(f"âŒ ë¶„ì„ ì˜¤ë¥˜: {file_data['error']}")
                                else:
                                    # íŒŒì¼ë³„ ìƒì„¸ ì •ë³´ í‘œì‹œ
                                    if file == 'config.json':
                                        st.markdown("**ğŸ“‹ ëª¨ë¸ ì„¤ì • ì •ë³´**")
                                        col1, col2 = st.columns(2)
                                        with col1:
                                            st.write(f"â€¢ **ëª¨ë¸ íƒ€ì…**: {file_data.get('model_type', 'N/A')}")
                                            st.write(f"â€¢ **ì•„í‚¤í…ì²˜**: {', '.join(file_data.get('architectures', ['N/A']))}")
                                            st.write(f"â€¢ **ì–´íœ˜ í¬ê¸°**: {file_data.get('vocab_size', 'N/A'):,}")
                                            st.write(f"â€¢ **íˆë“  ì‚¬ì´ì¦ˆ**: {file_data.get('hidden_size', 'N/A')}")
                                        with col2:
                                            st.write(f"â€¢ **ë ˆì´ì–´ ìˆ˜**: {file_data.get('num_hidden_layers', 'N/A')}")
                                            st.write(f"â€¢ **ì–´í…ì…˜ í—¤ë“œ**: {file_data.get('num_attention_heads', 'N/A')}")
                                            st.write(f"â€¢ **ìµœëŒ€ ìœ„ì¹˜**: {file_data.get('max_position_embeddings', 'N/A')}")
                                            st.write(f"â€¢ **ì¶”ì • íŒŒë¼ë¯¸í„°**: {file_data.get('model_parameters', 0):,}")
                                    
                                    elif file == 'tokenizer_config.json':
                                        st.markdown("**ğŸ”¤ í† í¬ë‚˜ì´ì € ì„¤ì •**")
                                        col1, col2 = st.columns(2)
                                        with col1:
                                            st.write(f"â€¢ **í† í¬ë‚˜ì´ì € í´ë˜ìŠ¤**: {file_data.get('tokenizer_class', 'N/A')}")
                                            st.write(f"â€¢ **ìµœëŒ€ ê¸¸ì´**: {file_data.get('model_max_length', 'N/A')}")
                                            st.write(f"â€¢ **íŒ¨ë”© ë°©í–¥**: {file_data.get('padding_side', 'N/A')}")
                                        with col2:
                                            st.write(f"â€¢ **ìë¥´ê¸° ë°©í–¥**: {file_data.get('truncation_side', 'N/A')}")
                                            st.write(f"â€¢ **ì •ë¦¬ ê³µë°±**: {file_data.get('clean_up_tokenization_spaces', 'N/A')}")
                                        
                                        if file_data.get('special_tokens'):
                                            st.markdown("**ğŸ¯ íŠ¹ìˆ˜ í† í°ë“¤**")
                                            for token_name, token_value in file_data['special_tokens'].items():
                                                st.write(f"â€¢ **{token_name}**: `{token_value}`")
                                    
                                    elif file == 'tokenizer.json':
                                        st.markdown("**ğŸ” í† í¬ë‚˜ì´ì € ìƒì„¸ ì •ë³´**")
                                        col1, col2 = st.columns(2)
                                        with col1:
                                            st.write(f"â€¢ **ë²„ì „**: {file_data.get('version', 'N/A')}")
                                            st.write(f"â€¢ **ëª¨ë¸ íƒ€ì…**: {file_data.get('model_type', 'N/A')}")
                                            st.write(f"â€¢ **ì–´íœ˜ í¬ê¸°**: {file_data.get('vocab_size', 'N/A'):,}")
                                        with col2:
                                            if file_data.get('special_tokens'):
                                                st.write("â€¢ **íŠ¹ìˆ˜ í† í° ìˆ˜**: " + str(len(file_data['special_tokens'])))
                                    
                                    elif file == 'vocab.txt':
                                        st.markdown("**ğŸ“š ì–´íœ˜ ì •ë³´**")
                                        col1, col2 = st.columns(2)
                                        with col1:
                                            st.write(f"â€¢ **ì´ ì–´íœ˜ í¬ê¸°**: {file_data.get('vocab_size', 'N/A'):,}")
                                            st.write(f"â€¢ **íŠ¹ìˆ˜ í† í° ìˆ˜**: {len(file_data.get('special_tokens_found', []))}")
                                        with col2:
                                            if file_data.get('special_tokens_found'):
                                                st.write("â€¢ **ë°œê²¬ëœ íŠ¹ìˆ˜ í† í°ë“¤**:")
                                                for token in file_data['special_tokens_found'][:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                                                    st.write(f"  - `{token}`")
                                        
                                        if file_data.get('sample_tokens'):
                                            st.markdown("**ğŸ“ ìƒ˜í”Œ í† í°ë“¤**")
                                            sample_text = ", ".join([f"`{token}`" for token in file_data['sample_tokens'][:10]])
                                            st.write(sample_text)
                                    
                                    elif file == 'special_tokens_map.json':
                                        st.markdown("**ğŸ¯ íŠ¹ìˆ˜ í† í° ë§µí•‘**")
                                        if file_data:
                                            for token_name, token_info in file_data.items():
                                                if isinstance(token_info, dict):
                                                    st.write(f"â€¢ **{token_name}**: `{token_info.get('content', token_info)}`")
                                                else:
                                                    st.write(f"â€¢ **{token_name}**: `{token_info}`")
                                    
                                    elif file in ['pytorch_model.bin', 'model.safetensors']:
                                        st.markdown("**ğŸ§  ëª¨ë¸ ê°€ì¤‘ì¹˜ ì •ë³´**")
                                        col1, col2 = st.columns(2)
                                        with col1:
                                            st.write(f"â€¢ **íŒŒì¼ í¬ê¸°**: {file_data.get('file_size_mb', 'N/A'):.2f} MB")
                                            st.write(f"â€¢ **ì´ íŒŒë¼ë¯¸í„°**: {file_data.get('total_parameters', 'N/A'):,}")
                                        with col2:
                                            if file == 'pytorch_model.bin':
                                                st.write(f"â€¢ **ë°ì´í„° íƒ€ì…**: {file_data.get('dtype_info', 'N/A')}")
                                            else:  # safetensors
                                                st.write(f"â€¢ **í…ì„œ ê°œìˆ˜**: {file_data.get('tensor_count', 'N/A')}")
                                        
                                        if file_data.get('parameter_keys'):
                                            st.markdown("**ğŸ”‘ íŒŒë¼ë¯¸í„° í‚¤ ìƒ˜í”Œ**")
                                            key_text = ", ".join([f"`{key}`" for key in file_data['parameter_keys'][:5]])
                                            st.write(key_text)
                                    
                                    elif file == 'generation_config.json':
                                        st.markdown("**ğŸ® ìƒì„± ì„¤ì •**")
                                        col1, col2 = st.columns(2)
                                        with col1:
                                            st.write(f"â€¢ **ìµœëŒ€ ê¸¸ì´**: {file_data.get('max_length', 'N/A')}")
                                            st.write(f"â€¢ **ìµœëŒ€ ìƒˆ í† í°**: {file_data.get('max_new_tokens', 'N/A')}")
                                            st.write(f"â€¢ **ì˜¨ë„**: {file_data.get('temperature', 'N/A')}")
                                        with col2:
                                            st.write(f"â€¢ **Top-p**: {file_data.get('top_p', 'N/A')}")
                                            st.write(f"â€¢ **Top-k**: {file_data.get('top_k', 'N/A')}")
                                            st.write(f"â€¢ **ìƒ˜í”Œë§**: {file_data.get('do_sample', 'N/A')}")
                                    
                                    elif file == 'merges.txt':
                                        st.markdown("**ğŸ”€ BPE ë³‘í•© ì •ë³´**")
                                        st.write(f"â€¢ **ë³‘í•© ê·œì¹™ ìˆ˜**: {file_data.get('num_merges', 'N/A'):,}")
                                        if file_data.get('sample_merges'):
                                            st.markdown("**ğŸ“ ìƒ˜í”Œ ë³‘í•© ê·œì¹™**")
                                            for merge in file_data['sample_merges'][:5]:
                                                st.write(f"â€¢ `{merge}`")
                            else:
                                st.info("ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                if missing_files:
                    st.markdown("**âŒ ëˆ„ë½ëœ íŒŒì¼ë“¤**")
                    for file in missing_files:
                        with st.expander(f"âŒ {file} (ëˆ„ë½ë¨)", expanded=False):
                            # ëˆ„ë½ëœ íŒŒì¼ì— ëŒ€í•œ ì„¤ëª…
                            if file == 'pytorch_model.bin':
                                st.info("PyTorch í˜•ì‹ì˜ ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ì…ë‹ˆë‹¤. ëŒ€ì‹  model.safetensors íŒŒì¼ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.")
                            elif file == 'generation_config.json':
                                st.info("í…ìŠ¤íŠ¸ ìƒì„± ì„¤ì • íŒŒì¼ì…ë‹ˆë‹¤. ìƒì„± íƒœìŠ¤í¬ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê¸°ë³¸ ì„¤ì •ì„ ì •ì˜í•©ë‹ˆë‹¤.")
                            elif file == 'merges.txt':
                                st.info("BPE(Byte Pair Encoding) í† í¬ë‚˜ì´ì €ì˜ ë³‘í•© ê·œì¹™ íŒŒì¼ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ë‹¤ë¥¸ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                            else:
                                st.info(f"{file} íŒŒì¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ê¶Œì¥ì‚¬í•­
        if 'recommendations' in analysis and analysis['recommendations']:
            st.subheader("ğŸ’¡ ê¶Œì¥ì‚¬í•­")
            for i, rec in enumerate(analysis['recommendations'], 1):
                st.warning(f"**{i}.** {rec}")
    
    # ë¡œë“œëœ ëª¨ë¸ ëª©ë¡
    st.subheader("ğŸ“‹ ë¡œë“œëœ ëª¨ë¸ ëª©ë¡")
    models_status = st.session_state['model_manager'].get_all_models_status()
    
    if models_status:
        # ê° ëª¨ë¸ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ í‘œì‹œ
        for name, info in models_status.items():
            with st.expander(f"ğŸ¤– {name} - {info['status']}", expanded=info['status'] == 'error'):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**ğŸ“Š ê¸°ë³¸ ì •ë³´**")
                    
                    # ìƒíƒœì— ë”°ë¥¸ ìƒ‰ìƒ í‘œì‹œ
                    if info['status'] == 'loaded':
                        st.success(f"âœ… **ìƒíƒœ**: {info['status']}")
                    elif info['status'] == 'loading':
                        st.info(f"ğŸ”„ **ìƒíƒœ**: {info['status']}")
                    elif info['status'] == 'error':
                        st.error(f"âŒ **ìƒíƒœ**: {info['status']}")
                    else:
                        st.warning(f"âš ï¸ **ìƒíƒœ**: {info['status']}")
                    
                    st.write(f"ğŸ“ **ê²½ë¡œ**: `{info['path']}`")
                    st.write(f"ğŸ’¾ **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: {info['memory_usage']:.1f} MB")
                    st.write(f"â° **ë¡œë“œ ì‹œê°„**: {info['load_time'] if info['load_time'] else 'N/A'}")
                
                with col2:
                    st.markdown("**ğŸ” ìƒì„¸ ì •ë³´**")
                    
                    # ì—ëŸ¬ ì •ë³´ í‘œì‹œ
                    if info['status'] == 'error' and info['error_message']:
                        st.error(f"ğŸš¨ **ì—ëŸ¬ ë‚´ìš©**:")
                        st.code(info['error_message'], language='text')
                    
                    # ëª¨ë¸ ë¶„ì„ ì •ë³´ í‘œì‹œ
                    if info.get('config_analysis') and 'model_summary' in info['config_analysis']:
                        summary = info['config_analysis']['model_summary']
                        st.write(f"ğŸ—ï¸ **ëª¨ë¸ íƒ€ì…**: {summary.get('model_type', 'unknown')}")
                        st.write(f"ğŸ“Š **íŒŒë¼ë¯¸í„° ìˆ˜**: {summary.get('total_parameters', 0):,}")
                        
                        if summary.get('supported_tasks'):
                            st.write(f"ğŸ¯ **ì§€ì› íƒœìŠ¤í¬**: {', '.join(summary['supported_tasks'])}")
                    
                    # ëª¨ë¸ì´ ë¡œë“œëœ ê²½ìš° ì¶”ê°€ ì •ë³´
                    if info['status'] == 'loaded':
                        st.success("ğŸŸ¢ ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì–´ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                        
                        # ì¶”ë¡  ê°€ëŠ¥í•œ íƒœìŠ¤í¬ í‘œì‹œ
                        available_tasks = st.session_state['model_manager'].get_available_tasks(name)
                        if available_tasks:
                            st.write(f"ğŸš€ **ì‚¬ìš© ê°€ëŠ¥í•œ íƒœìŠ¤í¬**: {', '.join(available_tasks)}")
                
                # ëª¨ë¸ ì–¸ë¡œë“œ ë²„íŠ¼
                if info['status'] == 'loaded':
                    if st.button(f"ğŸ—‘ï¸ {name} ì–¸ë¡œë“œ", key=f"unload_{name}"):
                        if st.session_state['model_manager'].unload_model(name):
                            st.success(f"ëª¨ë¸ '{name}' ì–¸ë¡œë“œ ì™„ë£Œ!")
                            st.rerun()
                        else:
                            st.error(f"ëª¨ë¸ '{name}' ì–¸ë¡œë“œ ì‹¤íŒ¨!")
                elif info['status'] == 'error':
                    if st.button(f"ğŸ—‘ï¸ {name} ì œê±°", key=f"remove_{name}"):
                        if st.session_state['model_manager'].remove_model(name):
                            st.success(f"ëª¨ë¸ '{name}' ì œê±° ì™„ë£Œ!")
                            st.rerun()
                        else:
                            st.error(f"ëª¨ë¸ '{name}' ì œê±° ì‹¤íŒ¨!")
    else:
        st.info("ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")

# FastAPI ì„œë²„ UI
def render_fastapi_server():
    st.subheader("ğŸš€ FastAPI ì„œë²„")
    
    # ì„œë²„ ì •ë³´
    server_info = st.session_state['fastapi_server'].get_server_info()
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.metric("ì„œë²„ ìƒíƒœ", "ğŸŸ¢ ì‹¤í–‰ ì¤‘" if server_info['running'] else "ğŸ”´ ì¤‘ì§€ë¨")
    
    with col2:
        st.metric("ë¡œë“œëœ ëª¨ë¸ ìˆ˜", server_info['loaded_models'])
    
    # ì„œë²„ ì œì–´
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("ğŸš€ ì„œë²„ ì‹œì‘"):
            try:
                result = st.session_state['fastapi_server'].start_server()
                st.success(result)
            except Exception as e:
                st.error(f"ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
    
    with col2:
        if st.button("â¹ï¸ ì„œë²„ ì¤‘ì§€"):
            try:
                result = st.session_state['fastapi_server'].stop_server()
                st.info(result)
            except Exception as e:
                st.error(f"ì„œë²„ ì¤‘ì§€ ì‹¤íŒ¨: {e}")
    
    with col3:
        if st.button("ğŸ§¹ ìºì‹œ ì •ë¦¬"):
            st.session_state['fastapi_server'].clear_pipeline_cache()
            st.success("íŒŒì´í”„ë¼ì¸ ìºì‹œê°€ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ì„œë²„ ì •ë³´ í‘œì‹œ
    if server_info['running']:
        st.subheader("ğŸ”— ì„œë²„ ì •ë³´")
        st.info(f"**ì„œë²„ URL:** {server_info['url']}")
        st.info(f"**API ë¬¸ì„œ:** {server_info['docs_url']}")
        
        # ì—”ë“œí¬ì¸íŠ¸ ëª©ë¡
        st.subheader("ğŸ“¡ ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ë“œí¬ì¸íŠ¸")
        endpoints = st.session_state['fastapi_server'].get_available_endpoints()
        endpoints_data = []
        for ep in endpoints:
            endpoints_data.append({
                "ê²½ë¡œ": ep['path'],
                "ë©”ì„œë“œ": ', '.join(ep['methods']),
                "ì´ë¦„": ep['name'] or "N/A"
            })
        
        st.dataframe(pd.DataFrame(endpoints_data), use_container_width=True)
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¸í„°í˜ì´ìŠ¤
        st.subheader("ğŸ§ª API í…ŒìŠ¤íŠ¸")
        loaded_models = st.session_state['model_manager'].get_loaded_models()
        
        if loaded_models:
            selected_model = st.selectbox("í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ì„ íƒ", loaded_models)
            test_text = st.text_area("ì…ë ¥ í…ìŠ¤íŠ¸", "Hello, world!")
            
            if st.button("ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸"):
                import requests
                try:
                    response = requests.post(
                        f"{server_info['url']}/models/{selected_model}/predict",
                        json={"text": test_text}
                    )
                    if response.status_code == 200:
                        result = response.json()
                        st.success("ì˜ˆì¸¡ ì„±ê³µ!")
                        st.json(result)
                    else:
                        st.error(f"ì˜ˆì¸¡ ì‹¤íŒ¨: {response.status_code}")
                        st.error(response.text)
                except Exception as e:
                    st.error(f"ìš”ì²­ ì‹¤íŒ¨: {e}")
        else:
            st.info("í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´ ë¨¼ì € ëª¨ë¸ì„ ë¡œë“œí•˜ì„¸ìš”.")

# ë©”ì¸ í•¨ìˆ˜
def main():
    st.title("ğŸš€ Hugging Face GUI")
    st.markdown("---")
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ“Š ì‹œìŠ¤í…œ ìš”ì•½")
        
        # ì‹œìŠ¤í…œ ìš”ì•½ ì •ë³´
        system_summary = st.session_state['model_manager'].get_system_summary()
        
        st.metric("ë¡œë“œëœ ëª¨ë¸", system_summary['loaded_models_count'])
        st.metric("ì´ ëª¨ë¸", system_summary['total_models_count'])
        st.metric("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰", f"{system_summary['total_memory_usage_mb']:.1f} MB")
        
        # ëª¨ë‹ˆí„°ë§ ìƒíƒœ
        if st.session_state['system_monitor'].monitoring:
            st.success("ğŸŸ¢ ëª¨ë‹ˆí„°ë§ ì‹¤í–‰ ì¤‘")
        else:
            st.info("â¸ï¸ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€ë¨")
        
        # ì„œë²„ ìƒíƒœ
        if st.session_state['fastapi_server'].is_running():
            st.success("ğŸŸ¢ API ì„œë²„ ì‹¤í–‰ ì¤‘")
        else:
            st.info("â¸ï¸ API ì„œë²„ ì¤‘ì§€ë¨")
    
    # íƒ­ ìƒì„±
    tabs = st.tabs([
        "ğŸ” ë¡œê·¸ì¸ ë° ì‚¬ìš©ì",
        "ğŸ“ ìºì‹œ ê´€ë¦¬",
        "ğŸ–¥ï¸ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§",
        "ğŸ¤– ëª¨ë¸ ê´€ë¦¬",
        "ğŸš€ FastAPI ì„œë²„"
    ])
    
    # ì²« ë²ˆì§¸ íƒ­: ë¡œê·¸ì¸/ë¡œê·¸ì•„ì›ƒ ë° ì‚¬ìš©ì ì •ë³´
    with tabs[0]:
        st.subheader("ğŸ” ë¡œê·¸ì¸ ë° ì‚¬ìš©ì ì •ë³´")
        
        if not st.session_state['logged_in']:
            st.text_input("Hugging Face í† í°:", key='input_token')
            st.button("ë¡œê·¸ì¸", on_click=login)
        else:
            st.success("âœ… ë¡œê·¸ì¸ ìƒíƒœ ìœ ì§€ë¨")
            st.button("ë¡œê·¸ì•„ì›ƒ", on_click=logout)
        
        if st.session_state['logged_in']:
            col1, col2 = st.columns(2)
            with col1:
                st.button("ğŸ‘¤ í˜„ì¬ ì‚¬ìš©ì ì •ë³´", on_click=show_whoami)
            with col2:
                st.button("ğŸŒ í™˜ê²½ ì •ë³´ ë³´ê¸°", on_click=show_env)
    
    # ë‘ ë²ˆì§¸ íƒ­: ìºì‹œ ê´€ë¦¬
    with tabs[1]:
        st.subheader("ğŸ“ ìºì‹œ ê´€ë¦¬")
        if st.button("ğŸ” ìºì‹œ ìŠ¤ìº”"):
            scan_cache()
        
        if st.session_state['cache_info']:
            # AgGrid ì„¤ì •
            gb = GridOptionsBuilder.from_dataframe(st.session_state['revisions_df'])
            gb.configure_selection("multiple", use_checkbox=True, groupSelectsChildren=True)
            gb.configure_pagination(paginationAutoPageSize=True)
            gb.configure_side_bar()
            gb.configure_default_column(enablePivot=True, enableValue=True, enableRowGroup=True)
            
            gridOptions = gb.build()
            
            grid_response = AgGrid(
                st.session_state['revisions_df'],
                gridOptions=gridOptions,
                data_return_mode=DataReturnMode.FILTERED_AND_SORTED,
                update_mode=GridUpdateMode.SELECTION_CHANGED,
                fit_columns_on_grid_load=True,
                enable_enterprise_modules=True,
                height=400,
                width='100%',
                reload_data=False
            )
            
            selected = grid_response['selected_rows']
            selected_df = pd.DataFrame(selected)
            
            # ì„ íƒ ìš”ì•½
            if not selected_df.empty:
                selected_count = len(selected_df)
                total_size = selected_df['Size (MB)'].sum()
                st.write(f"ì„ íƒëœ í•­ëª©: {selected_count}ê°œ, ì´ ìš©ëŸ‰: {total_size:.2f} MB")
                
                # ì‚­ì œ í™•ì¸ ë° ë²„íŠ¼
                with st.expander("ì„ íƒí•œ ìºì‹œ ì‚­ì œ"):
                    st.warning(f"{selected_count}ê°œì˜ ìˆ˜ì • ë²„ì „ì„ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
                    if st.button("ì‚­ì œ í™•ì¸"):
                        delete_selected(selected_df)
                        st.rerun()
            else:
                st.write("ì„ íƒëœ í•­ëª©: 0ê°œ, ì´ ìš©ëŸ‰: 0.00 MB")
    
    # ì„¸ ë²ˆì§¸ íƒ­: ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§
    with tabs[2]:
        render_system_monitoring()
    
    # ë„¤ ë²ˆì§¸ íƒ­: ëª¨ë¸ ê´€ë¦¬
    with tabs[3]:
        render_model_management()
    
    # ë‹¤ì„¯ ë²ˆì§¸ íƒ­: FastAPI ì„œë²„
    with tabs[4]:
        render_fastapi_server()

# í”„ë¡œê·¸ë¨ ì‹œì‘ ì‹œ ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ ë° ìºì‹œ ìŠ¤ìº”
if st.session_state['logged_in']:
    api.set_access_token(st.session_state['token'])
    if st.session_state['cache_info'] is None:
        scan_cache()

if __name__ == "__main__":
    main()
"""
Refactored comprehensive model analyzer using modular architecture.

This module now serves as a compatibility wrapper that uses the new
modular analysis components from the models.analysis package. The original
743-line monolithic implementation has been broken down into specialized modules
for better maintainability and separation of concerns.

Original file backed up as models/model_analyzer_original.py
"""

import json
import os
from pathlib import Path
from typing import Dict, Any, List

# Import the new modular components
from models.analysis import (
    ConfigAnalyzer,
    WeightAnalyzer,
    TokenizerAnalyzer,
    SummaryGenerator
)


class ComprehensiveModelAnalyzer:
    """
    Backward compatibility wrapper for comprehensive model analysis.

    This class maintains the original API while delegating to the new
    modular analysis components.
    """

    def __init__(self):
        """Initialize the modular model analyzer."""
        # Initialize specialized analyzers
        self.config_analyzer = ConfigAnalyzer()
        self.weight_analyzer = WeightAnalyzer()
        self.tokenizer_analyzer = TokenizerAnalyzer()
        self.summary_generator = SummaryGenerator()

        # Maintain backward compatibility with supported_files mapping
        self.supported_files = {
            'config.json': self._analyze_config,
            'tokenizer.json': self._analyze_tokenizer,
            'tokenizer_config.json': self._analyze_tokenizer_config,
            'vocab.txt': self._analyze_vocab,
            'merges.txt': self._analyze_merges,
            'special_tokens_map.json': self._analyze_special_tokens,
            'pytorch_model.bin': self._analyze_pytorch_model,
            'model.safetensors': self._analyze_safetensors,
            'generation_config.json': self._analyze_generation_config
        }

    def analyze_model_directory(self, model_path: str, model_name: str = "") -> Dict[str, Any]:
        """Analyze model directory - maintains backward compatibility."""
        analysis = {
            'model_path': model_path,
            'model_name': model_name,
            'files_found': [],
            'files_missing': [],
            'analysis_results': {},
            'model_summary': {},
            'recommendations': []
        }

        # Check file existence and analyze
        for filename in self.supported_files.keys():
            file_path = os.path.join(model_path, filename)
            if os.path.exists(file_path):
                analysis['files_found'].append(filename)
                try:
                    analysis['analysis_results'][filename] = self.supported_files[filename](file_path)
                except Exception as e:
                    analysis['analysis_results'][filename] = {'error': str(e)}
            else:
                analysis['files_missing'].append(filename)

        # Generate model summary using new modular summary generator
        analysis['model_summary'] = self.summary_generator.generate_model_summary(
            analysis['analysis_results'], model_name, model_path
        )

        # Generate recommendations using new modular generator
        analysis['recommendations'] = self.summary_generator.generate_recommendations(analysis)

        return analysis

    # Backward compatibility methods - delegate to appropriate analyzers

    def _analyze_config(self, file_path: str) -> Dict[str, Any]:
        """Analyze config.json - delegates to ConfigAnalyzer."""
        return self.config_analyzer.analyze_config(file_path)

    def _analyze_generation_config(self, file_path: str) -> Dict[str, Any]:
        """Analyze generation_config.json - delegates to ConfigAnalyzer."""
        return self.config_analyzer.analyze_generation_config(file_path)

    def _analyze_tokenizer(self, file_path: str) -> Dict[str, Any]:
        """Analyze tokenizer.json - delegates to TokenizerAnalyzer."""
        return self.tokenizer_analyzer.analyze_tokenizer(file_path)

    def _analyze_tokenizer_config(self, file_path: str) -> Dict[str, Any]:
        """Analyze tokenizer_config.json - delegates to TokenizerAnalyzer."""
        return self.tokenizer_analyzer.analyze_tokenizer_config(file_path)

    def _analyze_vocab(self, file_path: str) -> Dict[str, Any]:
        """Analyze vocab.txt - delegates to TokenizerAnalyzer."""
        return self.tokenizer_analyzer.analyze_vocab(file_path)

    def _analyze_merges(self, file_path: str) -> Dict[str, Any]:
        """Analyze merges.txt - delegates to TokenizerAnalyzer."""
        return self.tokenizer_analyzer.analyze_merges(file_path)

    def _analyze_special_tokens(self, file_path: str) -> Dict[str, Any]:
        """Analyze special_tokens_map.json - delegates to TokenizerAnalyzer."""
        return self.tokenizer_analyzer.analyze_special_tokens(file_path)

    def _analyze_pytorch_model(self, file_path: str) -> Dict[str, Any]:
        """Analyze pytorch_model.bin - delegates to WeightAnalyzer."""
        return self.weight_analyzer.analyze_pytorch_model(file_path)

    def _analyze_safetensors(self, file_path: str) -> Dict[str, Any]:
        """Analyze model.safetensors - delegates to WeightAnalyzer."""
        return self.weight_analyzer.analyze_safetensors(file_path)

    # Legacy methods for backward compatibility

    def _generate_model_summary(self, analysis_results: Dict, model_name: str = "", model_path: str = "") -> Dict[str, Any]:
        """Generate model summary - delegates to SummaryGenerator."""
        return self.summary_generator.generate_model_summary(analysis_results, model_name, model_path)

    def _generate_recommendations(self, analysis: Dict) -> List[str]:
        """Generate recommendations - delegates to SummaryGenerator."""
        return self.summary_generator.generate_recommendations(analysis)

    def _infer_tasks_from_config(self, config: Dict, model_name: str = "", model_path: str = "") -> List[str]:
        """Infer tasks from config - delegates to ConfigAnalyzer."""
        return self.config_analyzer.infer_tasks_from_config(config, model_name, model_path)

    def _estimate_parameters(self, config: Dict) -> int:
        """Estimate parameters - delegates to ConfigAnalyzer."""
        return self.config_analyzer.estimate_parameters(config)

    def _generate_usage_examples(self, tasks: List[str], model_type: str, analysis_results: Dict = None) -> Dict[str, Dict]:
        """Generate usage examples - delegates to SummaryGenerator."""
        return self.summary_generator._generate_usage_examples(tasks, model_type, analysis_results)

    def _extract_embedding_info(self, summary: Dict, analysis_results: Dict) -> Dict[str, Any]:
        """Extract embedding info - delegates to SummaryGenerator."""
        return self.summary_generator._extract_embedding_info(summary, analysis_results)

    def _analyze_model_capabilities(self, summary: Dict, analysis_results: Dict) -> List[str]:
        """Analyze model capabilities - delegates to SummaryGenerator."""
        return self.summary_generator._analyze_model_capabilities(summary, analysis_results)


# Convenience functions for backward compatibility
def create_model_analyzer() -> ComprehensiveModelAnalyzer:
    """Create comprehensive model analyzer."""
    return ComprehensiveModelAnalyzer()


def analyze_model(model_path: str, model_name: str = "") -> Dict[str, Any]:
    """Analyze model directory - convenience function."""
    analyzer = ComprehensiveModelAnalyzer()
    return analyzer.analyze_model_directory(model_path, model_name)


# Logging setup for backward compatibility
import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
logger.info("Model analyzer module loaded with modular architecture")
logger.info("Original implementation backed up as model_analyzer_original.py")
logger.info("New architecture: 743 lines â†’ ~150 lines (80% reduction)")
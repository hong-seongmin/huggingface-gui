"""
ê·¹í•œ ì„±ëŠ¥ ìµœì í™” - transformers ë³‘ëª© í•´ê²°
"""
import os
import time
import torch
import warnings
from typing import Any, Optional

class ExtremeOptimizer:
    """transformers ë³‘ëª©ì„ í•´ê²°í•˜ëŠ” ê·¹í•œ ìµœì í™”"""
    
    def __init__(self):
        self.setup_extreme_environment()
    
    def setup_extreme_environment(self):
        """ê·¹í•œ í™˜ê²½ ìµœì í™”"""
        # ëª¨ë“  ê²½ê³  ì–µì œ
        warnings.filterwarnings("ignore")
        os.environ["PYTHONWARNINGS"] = "ignore"
        
        # Transformers ê·¹í•œ ìµœì í™”
        os.environ["TRANSFORMERS_VERBOSITY"] = "error"
        os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "1"
        os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"
        os.environ["TOKENIZERS_PARALLELISM"] = "false"
        os.environ["HF_DATASETS_OFFLINE"] = "1"
        os.environ["HF_HUB_OFFLINE"] = "1"
        
        # PyTorch ê·¹í•œ ìµœì í™”
        os.environ["PYTORCH_JIT"] = "1"
        os.environ["PYTORCH_JIT_USE_NNC"] = "1"
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"
        
        # CPU ìµœì í™”
        cpu_count = os.cpu_count()
        os.environ["OMP_NUM_THREADS"] = str(cpu_count)
        os.environ["MKL_NUM_THREADS"] = str(cpu_count)
        os.environ["NUMEXPR_NUM_THREADS"] = str(cpu_count)
        
        print(f"[EXTREME] ê·¹í•œ í™˜ê²½ ìµœì í™” ì™„ë£Œ ({cpu_count} ì½”ì–´)")
    
    def optimize_transformers_loading(self):
        """transformers ë¡œë”© ìµœì í™”"""
        try:
            # transformers ë‚´ë¶€ ìµœì í™”
            import transformers
            
            # ìºì‹œ ë¹„í™œì„±í™” (ë””ìŠ¤í¬ I/O ê°ì†Œ)
            transformers.utils.WEIGHTS_NAME = None
            transformers.utils.CONFIG_NAME = None
            
            # ê²€ì¦ ë¹„í™œì„±í™”
            transformers.modeling_utils.PreTrainedModel._load_pretrained_model = self._fast_load_pretrained_model
            
            print("[EXTREME] transformers ë¡œë”© ìµœì í™” ì™„ë£Œ")
            
        except Exception as e:
            print(f"[EXTREME] transformers ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def _fast_load_pretrained_model(self, *args, **kwargs):
        """ë¹ ë¥¸ pretrained ëª¨ë¸ ë¡œë”© (ê²€ì¦ ìš°íšŒ)"""
        # ì›ë³¸ ë©”ì„œë“œ í˜¸ì¶œí•˜ë˜ ê²€ì¦ ë‹¨ê³„ ìƒëµ
        try:
            return self._original_load_pretrained_model(*args, **kwargs)
        except:
            # ê²€ì¦ ì‹¤íŒ¨ì‹œì—ë„ ê³„ì† ì§„í–‰
            pass
    
    def patch_huggingface_hub(self):
        """HuggingFace Hub ìš”ì²­ ìµœì í™”"""
        try:
            import huggingface_hub
            
            # ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ìš°íšŒ (ë¡œì»¬ íŒŒì¼ë§Œ ì‚¬ìš©)
            original_hf_hub_download = huggingface_hub.hf_hub_download
            
            def fast_hf_hub_download(*args, **kwargs):
                kwargs['local_files_only'] = True
                return original_hf_hub_download(*args, **kwargs)
            
            huggingface_hub.hf_hub_download = fast_hf_hub_download
            
            print("[EXTREME] HuggingFace Hub íŒ¨ì¹˜ ì™„ë£Œ")
            
        except Exception as e:
            print(f"[EXTREME] Hub íŒ¨ì¹˜ ì‹¤íŒ¨: {e}")
    
    def ultra_fast_model_loading(self, model_path: str, device: str = "cpu"):
        """Ultra-Fast ëª¨ë¸ ë¡œë”© (ëª¨ë“  ìµœì í™” ì ìš©)"""
        start_time = time.time()
        
        try:
            print("[EXTREME] Ultra-Fast ë¡œë”© ì‹œì‘")
            
            # 1. í™˜ê²½ ìµœì í™”
            self.setup_extreme_environment()
            self.optimize_transformers_loading()
            self.patch_huggingface_hub()
            
            # 2. ë©”ëª¨ë¦¬ ì •ë¦¬
            import gc
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # 3. ì„¤ì • í™•ì¸ (ë¹ ë¥¸ ë¡œë”©)
            from transformers import AutoConfig
            config = AutoConfig.from_pretrained(
                model_path,
                local_files_only=True,
                trust_remote_code=True,
                _fast_init=True  # ë¹ ë¥¸ ì´ˆê¸°í™”
            )
            
            is_classification = (
                hasattr(config, 'architectures') and 
                config.architectures and
                any('Classification' in arch for arch in config.architectures)
            )
            
            # 4. ê·¹í•œ ìµœì í™”ëœ ë¡œë”© ì„¤ì •
            load_kwargs = {
                "trust_remote_code": True,
                "local_files_only": True,
                "torch_dtype": torch.float32,
                "low_cpu_mem_usage": True,
                "use_safetensors": True,
                "_fast_init": True,  # ë¹ ë¥¸ ì´ˆê¸°í™”
                "device_map": None,  # ìë™ ë””ë°”ì´ìŠ¤ ë§¤í•‘ ë¹„í™œì„±í™”
            }
            
            # 5. ëª¨ë¸ ë¡œë”© (ê²€ì¦ ìµœì†Œí™”)
            if is_classification:
                from transformers import AutoModelForSequenceClassification
                
                print("[EXTREME] Classification ëª¨ë¸ ê·¹í•œ ë¡œë”©...")
                with torch.no_grad():
                    model = AutoModelForSequenceClassification.from_pretrained(
                        model_path, **load_kwargs
                    )
            else:
                from transformers import AutoModel
                
                print("[EXTREME] ì¼ë°˜ ëª¨ë¸ ê·¹í•œ ë¡œë”©...")
                with torch.no_grad():
                    model = AutoModel.from_pretrained(
                        model_path, **load_kwargs
                    )
            
            # 6. í† í¬ë‚˜ì´ì € ë¡œë”© (ë³‘ë ¬)
            from transformers import AutoTokenizer
            
            print("[EXTREME] í† í¬ë‚˜ì´ì € ê·¹í•œ ë¡œë”©...")
            tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                use_fast=True,
                local_files_only=True,
                trust_remote_code=True,
                _fast_init=True
            )
            
            # 7. ë””ë°”ì´ìŠ¤ ì´ë™ ë° ìµœì í™”
            model = model.to(device)
            model.eval()
            
            # 8. ê·¸ë˜ë””ì–¸íŠ¸ ë¹„í™œì„±í™”
            for param in model.parameters():
                param.requires_grad = False
            
            load_time = time.time() - start_time
            print(f"[EXTREME] Ultra-Fast ë¡œë”© ì™„ë£Œ: {load_time:.1f}ì´ˆ")
            
            return model, tokenizer, load_time
            
        except Exception as e:
            print(f"[EXTREME] Ultra-Fast ë¡œë”© ì‹¤íŒ¨: {e}")
            return None, None, 0.0
    
    def benchmark_optimization(self, model_path: str):
        """ìµœì í™” ë²¤ì¹˜ë§ˆí¬"""
        print("=" * 50)
        print("ğŸ”¥ EXTREME ìµœì í™” ë²¤ì¹˜ë§ˆí¬")
        print("=" * 50)
        
        # 1. ê¸°ë³¸ ë¡œë”© (ë¹„êµìš©)
        print("1. ê¸°ë³¸ ë¡œë”© (ì°¸ê³ ìš©)")
        start_time = time.time()
        
        try:
            from transformers import AutoModel, AutoTokenizer
            model = AutoModel.from_pretrained(model_path, local_files_only=True)
            tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
            basic_time = time.time() - start_time
            print(f"   ê¸°ë³¸ ë¡œë”©: {basic_time:.1f}ì´ˆ")
        except Exception as e:
            print(f"   ê¸°ë³¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            basic_time = 999.0
        
        # 2. EXTREME ìµœì í™” ë¡œë”©
        print("2. EXTREME ìµœì í™” ë¡œë”©")
        model, tokenizer, extreme_time = self.ultra_fast_model_loading(model_path)
        
        if model and tokenizer:
            speedup = basic_time / extreme_time if extreme_time > 0 else float('inf')
            print(f"   EXTREME ë¡œë”©: {extreme_time:.1f}ì´ˆ")
            print(f"   ì„±ëŠ¥ í–¥ìƒ: {speedup:.1f}ë°°")
            
            if extreme_time < 60:
                print("   ğŸ† ë“±ê¸‰: ULTRA-FAST!")
            elif extreme_time < 120:
                print("   ğŸ¥‡ ë“±ê¸‰: FAST!")
            else:
                print("   ğŸ¥ˆ ë“±ê¸‰: GOOD")
        else:
            print("   âŒ EXTREME ë¡œë”© ì‹¤íŒ¨")
        
        print("=" * 50)

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
extreme_optimizer = ExtremeOptimizer()
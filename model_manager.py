import threading
import psutil
import torch
import os
import re
import hashlib
from typing import Dict, List, Optional, Callable
from dataclasses import dataclass, field
from datetime import datetime
from model_analyzer import ComprehensiveModelAnalyzer
from model_optimization import optimizer
from model_cache import model_cache
from device_manager import device_manager
from detailed_profiler import profiler
from huggingface_hub import hf_hub_download, snapshot_download, HfApi

@dataclass
class ModelInfo:
    name: str
    path: str
    model: Optional[object] = None
    tokenizer: Optional[object] = None
    config_analysis: Dict = field(default_factory=dict)
    memory_usage: float = 0.0
    load_time: Optional[datetime] = None
    status: str = "unloaded"  # unloaded, loading, loaded, error
    error_message: str = ""

class MultiModelManager:
    def __init__(self):
        self.models: Dict[str, ModelInfo] = {}
        self.loading_locks = {}
        self.max_memory_threshold = 0.8  # 80% ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì œí•œ
        self.load_queue = []
        self.model_analyzer = ComprehensiveModelAnalyzer()
        self.callbacks: List[Callable] = []
        self.hf_api = HfApi()
        
    def add_callback(self, callback: Callable):
        """ëª¨ë¸ ìƒíƒœ ë³€ê²½ ì½œë°± ë“±ë¡"""
        self.callbacks.append(callback)
        
    def _notify_callbacks(self, model_name: str, event_type: str, data: Dict = None):
        """ì½œë°± í•¨ìˆ˜ë“¤ì—ê²Œ ì•Œë¦¼"""
        for callback in self.callbacks:
            try:
                callback(model_name, event_type, data or {})
            except Exception as e:
                print(f"Callback error: {e}")
    
    def get_memory_info(self):
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´"""
        memory = psutil.virtual_memory()
        gpu_memory = []
        
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                gpu_memory.append({
                    'device': i,
                    'name': torch.cuda.get_device_name(i),
                    'total': torch.cuda.get_device_properties(i).total_memory,
                    'allocated': torch.cuda.memory_allocated(i),
                    'reserved': torch.cuda.memory_reserved(i)
                })
        
        return {
            'system_memory': {
                'total': memory.total,
                'available': memory.available,
                'used': memory.used,
                'percent': memory.percent
            },
            'gpu_memory': gpu_memory
        }
    
    def can_load_model(self, estimated_size: float) -> bool:
        """ëª¨ë¸ ë¡œë“œ ê°€ëŠ¥ ì—¬ë¶€ ì²´í¬"""
        memory_info = self.get_memory_info()
        current_usage = memory_info['system_memory']['percent'] / 100
        return current_usage + (estimated_size / memory_info['system_memory']['total']) < self.max_memory_threshold
    
    def analyze_model(self, model_path: str) -> Dict:
        """ëª¨ë¸ ë¶„ì„ (ë¡œë“œ ì—†ì´, HuggingFace ëª¨ë¸ ID ì§€ì›)"""
        try:
            actual_model_path = model_path
            
            # HuggingFace ëª¨ë¸ IDì¸ê²½ìš° ë‹¤ìš´ë¡œë“œ
            if self._is_huggingface_model_id(model_path):
                actual_model_path = self._download_huggingface_model(model_path)
            
            analysis = self.model_analyzer.analyze_model_directory(actual_model_path)
            analysis['original_path'] = model_path
            analysis['actual_path'] = actual_model_path
            return analysis
        except Exception as e:
            return {'error': str(e)}
    
    def _is_huggingface_model_id(self, model_path: str) -> bool:
        """HuggingFace ëª¨ë¸ ID í˜•ì‹ì¸ì§€ í™•ì¸"""
        # ë¡œì»¬ ê²½ë¡œê°€ ì•„ë‹Œ ê²½ìš° (ì ˆëŒ€ê²½ë¡œë‚˜ ìƒëŒ€ê²½ë¡œ)
        if os.path.isabs(model_path) or os.path.exists(model_path):
            return False
        
        # URLì´ ì•„ë‹Œ ê²½ìš°
        if model_path.startswith(('http://', 'https://', 'file://')):
            return False
        
        # HuggingFace ëª¨ë¸ ID í˜•ì‹ í™•ì¸ (username/model-name)
        if '/' in model_path and not model_path.startswith('/'):
            return True
        
        return False
    
    def _download_huggingface_model(self, model_id: str) -> str:
        """HuggingFace Hubì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        try:
            # ëª¨ë¸ ì •ë³´ í™•ì¸
            model_info = self.hf_api.model_info(model_id)
            
            # ë¡œì»¬ ìºì‹œ ë””ë ‰í† ë¦¬ë¡œ ë‹¤ìš´ë¡œë“œ (ì´ë¯¸ ìºì‹œëœ ê²½ìš° ë‹¤ìš´ë¡œë“œ ë°©ì§€)
            try:
                # ë¨¼ì € ë¡œì»¬ ìºì‹œì—ì„œ ì°¾ê¸° ì‹œë„
                local_path = snapshot_download(
                    repo_id=model_id,
                    repo_type="model", 
                    cache_dir=None,  # ê¸°ë³¸ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
                    local_files_only=True  # ì´ë¯¸ ìºì‹œëœ íŒŒì¼ë§Œ ì‚¬ìš©
                )
                print(f"[DEBUG] ìºì‹œì—ì„œ ëª¨ë¸ ì°¾ìŒ: {model_id}")
            except Exception:
                # ìºì‹œì— ì—†ëŠ” ê²½ìš°ì—ë§Œ ë‹¤ìš´ë¡œë“œ
                print(f"[DEBUG] ìºì‹œ ë¯¸ìŠ¤ - ë‹¤ìš´ë¡œë“œ ì‹œì‘: {model_id}")
                local_path = snapshot_download(
                    repo_id=model_id,
                    repo_type="model",
                    cache_dir=None,  # ê¸°ë³¸ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
                    local_files_only=False
                )
            
            return local_path
            
        except Exception as e:
            raise Exception(f"HuggingFace ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    
    def _generate_model_name(self, model_path: str) -> str:
        """ëª¨ë¸ ê²½ë¡œì—ì„œ ìë™ìœ¼ë¡œ ëª¨ë¸ ì´ë¦„ ìƒì„±"""
        if self._is_huggingface_model_id(model_path):
            # HuggingFace ëª¨ë¸ IDì—ì„œ ëª¨ë¸ ì´ë¦„ ì¶”ì¶œ
            return model_path.split('/')[-1]
        else:
            # ë¡œì»¬ ê²½ë¡œì—ì„œ ëª¨ë¸ ì´ë¦„ ì¶”ì¶œ
            return os.path.basename(model_path.rstrip('/'))
    
    def load_model_async(self, model_name: str, model_path: str, callback: Optional[Callable] = None):
        """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ (HuggingFace ëª¨ë¸ ID ì§€ì›, ëª¨ë¸ ì´ë¦„ ìë™ ìƒì„±)"""
        # ëª¨ë¸ ì´ë¦„ì´ ë¹„ì–´ìˆìœ¼ë©´ ìë™ ìƒì„±
        if not model_name or not model_name.strip():
            model_name = self._generate_model_name(model_path)
        
        # ì¤‘ë³µ ëª¨ë¸ ì´ë¦„ ì²˜ë¦¬
        original_name = model_name
        counter = 1
        while model_name in self.models:
            model_name = f"{original_name}_{counter}"
            counter += 1
        
        print(f"[DEBUG] load_model_async ì‹œì‘: {model_name}, {model_path}")
        
        # ë¡œë”© ë½ ì„¤ì •
        if model_name not in self.loading_locks:
            self.loading_locks[model_name] = threading.Lock()
        
        # ìŠ¤ë ˆë“œ ì‹œì‘
        print(f"[DEBUG] ìŠ¤ë ˆë“œ ìƒì„± ì¤‘: {model_name}")
        thread = threading.Thread(
            target=self._load_model_sync, 
            args=(model_name, model_path, callback),
            name=f"ModelLoad-{model_name}"
        )
        thread.daemon = True
        print(f"[DEBUG] ìŠ¤ë ˆë“œ ì‹œì‘ ì „: {model_name}")
        thread.start()
        print(f"[DEBUG] ìŠ¤ë ˆë“œ ì‹œì‘ë¨: {model_name}, thread={thread}")
        
        return thread
    
    def _load_model_sync(self, model_name: str, model_path: str, callback: Optional[Callable] = None):
        """ì‹¤ì œ ëª¨ë¸ ë¡œë”© ì‘ì—… (ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)"""
        import time
        import threading
        import queue
        
        start_time = time.time()
        
        def load_model_with_transformers(actual_model_path, device):
            """Fast ëª¨ë¸ ë¡œë”©"""
            print(f"[FAST] ëª¨ë¸ ë¡œë”© ì‹œì‘")
            
            # í”„ë¡œíŒŒì¼ë§ ì‹œì‘ (í”„ë¡œíŒŒì¼ëŸ¬ ë‚´ë¶€ì—ì„œ í™œì„±í™” ì—¬ë¶€ í™•ì¸)
            profiler.start_profiling("ëª¨ë¸ ë¡œë”©")
            profiler.memory_snapshot("ì´ˆê¸° ìƒíƒœ")
            
            # ì§ì ‘ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©ìœ¼ë¡œ ì‹¤ì œ BGE-M3 ëª¨ë¸ ë¡œë”©
            print("[DEBUG] ì‹¤ì œ transformers ëª¨ë¸ ë¡œë”© ì‹œì‘")
            
            import time
            load_start = time.time()
            
            try:
                from transformers import AutoModel, AutoTokenizer, AutoConfig
                
                print(f"[DEBUG] Config ë¡œë”© ì‹œì‘: {model_name}")
                
                # ë¹ ë¥¸ ë¡œì»¬ config í™•ì¸
                try:
                    import json
                    config_path = os.path.join(actual_model_path, "config.json")
                    if os.path.exists(config_path):
                        with open(config_path, 'r') as f:
                            config_dict = json.load(f)
                        print(f"[DEBUG] ë¡œì»¬ config ë¡œë”© ì™„ë£Œ: {model_name}")
                    else:
                        config = AutoConfig.from_pretrained(actual_model_path, local_files_only=True)
                        print(f"[DEBUG] AutoConfig ë¡œë”© ì™„ë£Œ: {model_name}")
                except Exception as e:
                    print(f"[DEBUG] Config ë¡œë”© ì˜¤ë¥˜, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
                
                print(f"[DEBUG] ì‹¤ì œ ëª¨ë¸ ë¡œë”© ì‹œì‘: {model_name}")
                
                # ëª¨ë¸ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                print(f"[DEBUG] ëª¨ë¸ íŒŒì¼ í™•ì¸ ì¤‘: {actual_model_path}")
                model_files = [
                    "config.json",
                    "pytorch_model.bin", 
                    "model.safetensors",
                    "tokenizer.json",
                    "tokenizer_config.json"
                ]
                
                for file in model_files:
                    file_path = os.path.join(actual_model_path, file)
                    exists = os.path.exists(file_path)
                    if exists:
                        size_mb = os.path.getsize(file_path) / (1024*1024)
                        print(f"[DEBUG] âœ… {file}: {size_mb:.1f}MB")
                    else:
                        print(f"[DEBUG] âŒ {file}: íŒŒì¼ ì—†ìŒ")
                
                # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
                import psutil
                mem = psutil.virtual_memory()
                print(f"[DEBUG] ë©”ëª¨ë¦¬ ìƒíƒœ - ì‚¬ìš©ë¥ : {mem.percent}%, ì‚¬ìš©ê°€ëŠ¥: {mem.available/1024**3:.1f}GB")
                
                print(f"[DEBUG] AutoModel.from_pretrained í˜¸ì¶œ ì‹œì‘ (í° ëª¨ë¸ì´ë¯€ë¡œ ì‹œê°„ ì†Œìš” ì˜ˆìƒ)")
                
                # í™˜ê²½ ë³€ìˆ˜ ìƒíƒœ í™•ì¸
                print(f"[DEBUG] í™˜ê²½ ë³€ìˆ˜ í™•ì¸:")
                env_vars = {
                    'HF_HUB_OFFLINE': os.getenv('HF_HUB_OFFLINE', 'None'),
                    'TRANSFORMERS_OFFLINE': os.getenv('TRANSFORMERS_OFFLINE', 'None'),
                    'HF_HUB_DISABLE_TELEMETRY': os.getenv('HF_HUB_DISABLE_TELEMETRY', 'None'),
                    'TOKENIZERS_PARALLELISM': os.getenv('TOKENIZERS_PARALLELISM', 'None')
                }
                for key, value in env_vars.items():
                    print(f"[DEBUG]   {key}={value}")
                
                model_start = time.time()
                
                # AutoModel ë¡œë”©ì„ ë‹¨ê³„ë³„ë¡œ ë¶„í• í•˜ì—¬ ì§„í–‰ ìƒíƒœ ì¶”ì 
                print(f"[DEBUG] 1/5: transformers AutoModel ì„í¬íŠ¸ í™•ì¸")
                from transformers import AutoModel
                print(f"[DEBUG] 2/5: AutoConfig ì‚¬ì „ ë¡œë”©")
                
                # Config ë¨¼ì € ë¡œë”©í•˜ì—¬ ëª¨ë¸ êµ¬ì¡° í™•ì¸
                try:
                    from transformers import AutoConfig
                    print(f"[DEBUG] Config ë¡œë”© ì‹œë„: {actual_model_path}")
                    config = AutoConfig.from_pretrained(
                        actual_model_path,
                        local_files_only=True,
                        trust_remote_code=True
                    )
                    print(f"[DEBUG] âœ… Config ë¡œë”© ì„±ê³µ: {config.__class__.__name__}")
                    print(f"[DEBUG] ëª¨ë¸ íƒ€ì…: {getattr(config, 'model_type', 'Unknown')}")
                    print(f"[DEBUG] ì–´íœ˜ í¬ê¸°: {getattr(config, 'vocab_size', 'Unknown')}")
                    print(f"[DEBUG] ìˆ¨ê²¨ì§„ í¬ê¸°: {getattr(config, 'hidden_size', 'Unknown')}")
                except Exception as config_e:
                    print(f"[DEBUG] âš ï¸ Config ë¡œë”© ì‹¤íŒ¨, ê³„ì† ì§„í–‰: {config_e}")
                
                print(f"[DEBUG] 3/5: ì‹¤ì œ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë”© ì‹œì‘ (ê°€ì¥ ì‹œê°„ ì†Œìš” ë‹¨ê³„)")
                
                # ë¡œë”© íƒ€ì„ì•„ì›ƒ ë° ì§„í–‰ìƒí™© ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ìŠ¤ë ˆë“œ ìƒì„±
                import threading
                import queue
                
                loading_result = queue.Queue()
                loading_error = queue.Queue()
                
                def load_model_with_progress():
                    """ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ëª¨ë¸ ë¡œë”© ìˆ˜í–‰"""
                    try:
                        print(f"[DEBUG] ëª¨ë¸ ë¡œë”© ìŠ¤ë ˆë“œ ì‹œì‘")
                        
                        # BGE-M3ëŠ” embedding ëª¨ë¸ì´ë¯€ë¡œ AutoModel ì‚¬ìš©
                        model = AutoModel.from_pretrained(
                            actual_model_path, 
                            local_files_only=True,
                            torch_dtype=torch.float32,
                            trust_remote_code=True
                        )
                        
                        print(f"[DEBUG] ëª¨ë¸ ë¡œë”© ìŠ¤ë ˆë“œ ì™„ë£Œ")
                        loading_result.put(model)
                        
                    except Exception as e:
                        print(f"[DEBUG] ëª¨ë¸ ë¡œë”© ìŠ¤ë ˆë“œ ì˜¤ë¥˜: {e}")
                        loading_error.put(e)
                
                # ë¡œë”© ìŠ¤ë ˆë“œ ì‹œì‘
                loading_thread = threading.Thread(target=load_model_with_progress)
                loading_thread.daemon = True
                loading_thread.start()
                
                # ì§„í–‰ìƒí™© ëª¨ë‹ˆí„°ë§ (30ì´ˆë§ˆë‹¤ ìƒíƒœ ì¶œë ¥)
                timeout_seconds = 300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                check_interval = 30    # 30ì´ˆë§ˆë‹¤ ì²´í¬
                elapsed_checks = 0
                
                while loading_thread.is_alive():
                    loading_thread.join(timeout=check_interval)
                    
                    if loading_thread.is_alive():
                        elapsed_checks += 1
                        elapsed_time = elapsed_checks * check_interval
                        
                        # ë©”ëª¨ë¦¬ ìƒíƒœ ì²´í¬
                        try:
                            mem = psutil.virtual_memory()
                            print(f"[DEBUG] ë¡œë”© ì§„í–‰ì¤‘... {elapsed_time}ì´ˆ ê²½ê³¼")
                            print(f"[DEBUG] ë©”ëª¨ë¦¬ ìƒíƒœ: {mem.percent}% ì‚¬ìš©, {mem.available/1024**3:.1f}GB ì‚¬ìš©ê°€ëŠ¥")
                            
                            # í”„ë¡œì„¸ìŠ¤ë³„ ë©”ëª¨ë¦¬ í™•ì¸
                            process = psutil.Process()
                            proc_mem_mb = process.memory_info().rss / 1024 / 1024
                            print(f"[DEBUG] í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬: {proc_mem_mb:.1f}MB")
                            
                        except Exception as mem_e:
                            print(f"[DEBUG] ë©”ëª¨ë¦¬ ì²´í¬ ì‹¤íŒ¨: {mem_e}")
                        
                        # íƒ€ì„ì•„ì›ƒ ì²´í¬
                        if elapsed_time >= timeout_seconds:
                            print(f"[DEBUG] âŒ ëª¨ë¸ ë¡œë”© íƒ€ì„ì•„ì›ƒ ({timeout_seconds}ì´ˆ)")
                            loading_error.put(TimeoutError(f"ëª¨ë¸ ë¡œë”©ì´ {timeout_seconds}ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤"))
                            break
                
                # ê²°ê³¼ í™•ì¸
                if not loading_error.empty():
                    error = loading_error.get()
                    raise error
                
                if not loading_result.empty():
                    model = loading_result.get()
                    print(f"[DEBUG] 4/5: ëª¨ë¸ ë¡œë”© ì™„ë£Œ, í›„ì²˜ë¦¬ ì‹œì‘")
                else:
                    raise Exception("ëª¨ë¸ ë¡œë”©ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                
                model_load_time = time.time() - model_start
                print(f"[DEBUG] 5/5: ëª¨ë¸ ë¡œë”© í›„ì²˜ë¦¬ ì™„ë£Œ")
                print(f"[DEBUG] âœ… ì‹¤ì œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name} ({model_load_time:.1f}ì´ˆ)")
                
                # ëª¨ë¸ ìƒíƒœ ê²€ì¦
                print(f"[DEBUG] ëª¨ë¸ ìƒíƒœ ê²€ì¦:")
                print(f"[DEBUG]   ëª¨ë¸ í´ë˜ìŠ¤: {model.__class__.__name__}")
                print(f"[DEBUG]   ëª¨ë¸ ìƒíƒœ: {'eval' if not model.training else 'train'}")
                print(f"[DEBUG]   ëª¨ë¸ ìƒ€ê³  ëª¨ë“œ: {next(model.parameters()).requires_grad}")
                
                # ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
                param_count = sum(p.numel() for p in model.parameters())
                param_size_mb = param_count * 4 / 1024 / 1024  # float32 = 4bytes
                print(f"[DEBUG] ëª¨ë¸ íŒŒë¼ë¯¸í„°: {param_count:,}ê°œ ({param_size_mb:.1f}MB)")
                
                # ëª¨ë¸ ë ˆì´ì–´ êµ¬ì¡° ê°„ëµ ë¶„ì„
                layer_count = 0
                for name, module in model.named_modules():
                    layer_count += 1
                    if layer_count <= 5:  # ì²˜ìŒ 5ê°œ ë ˆì´ì–´ë§Œ ìƒì„¸ ì •ë³´
                        print(f"[DEBUG]   ë ˆì´ì–´ {layer_count}: {name} ({module.__class__.__name__})")
                print(f"[DEBUG] ì´ ë ˆì´ì–´ ìˆ˜: {layer_count}")
                
                print(f"[DEBUG] í† í¬ë‚˜ì´ì € ë¡œë”© ì‹œì‘: {model_name}")
                tokenizer_start = time.time()
                
                tokenizer = AutoTokenizer.from_pretrained(
                    actual_model_path, 
                    local_files_only=True,
                    trust_remote_code=True
                )
                
                tokenizer_load_time = time.time() - tokenizer_start
                print(f"[DEBUG] âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ: {model_name} ({tokenizer_load_time:.1f}ì´ˆ)")
                
                # í† í¬ë‚˜ì´ì € ì •ë³´ í™•ì¸
                vocab_size = tokenizer.vocab_size if hasattr(tokenizer, 'vocab_size') else 'Unknown'
                print(f"[DEBUG] í† í¬ë‚˜ì´ì € ì–´íœ˜ í¬ê¸°: {vocab_size}")
                
                # í†µí•© ë””ë°”ì´ìŠ¤ ê´€ë¦¬ìë¡œ ì¼ê´€ì„± ë³´ì¥
                print(f"[DEBUG] ë””ë°”ì´ìŠ¤ ì¼ê´€ì„± ë³´ì¥ ì‹œì‘: {model_name}")
                device_start = time.time()
                
                model, tokenizer = device_manager.ensure_device_consistency(model, tokenizer)
                model.eval()
                
                device_time = time.time() - device_start
                print(f"[DEBUG] âœ… ë””ë°”ì´ìŠ¤ ì¼ê´€ì„± ë³´ì¥ ì™„ë£Œ: {model_name} ({device_time:.1f}ì´ˆ)")
                
                # ìµœì¢… ëª¨ë¸ ìƒíƒœ í™•ì¸
                model_device = next(model.parameters()).device
                print(f"[DEBUG] ìµœì¢… ëª¨ë¸ ë””ë°”ì´ìŠ¤: {model_device}")
                print(f"[DEBUG] ëª¨ë¸ í‰ê°€ ëª¨ë“œ: {not model.training}")
                
                # ë””ë°”ì´ìŠ¤ ì¼ê´€ì„± ìµœì¢… ê²€ì¦
                devices = set(param.device for param in model.parameters())
                if len(devices) == 1:
                    print(f"[DEBUG] âœ… ë””ë°”ì´ìŠ¤ ì¼ê´€ì„± í™•ì¸: {list(devices)[0]}")
                else:
                    print(f"[DEBUG] âš ï¸ ë””ë°”ì´ìŠ¤ ë¶ˆì¼ì¹˜ ê°ì§€: {devices}")
                
                load_time = time.time() - load_start
                print(f"[DEBUG] ì‹¤ì œ ëª¨ë¸ ë¡œë”© ì´ ì‹œê°„: {load_time:.1f}ì´ˆ")
                
                # ë¡œë”© ì„±ê³µ ë©”ì‹œì§€
                print(f"[DEBUG] ğŸ‰ BGE-M3 ëª¨ë¸ ë¡œë”© ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
                
                profiler.print_detailed_report()
                return model, tokenizer, load_time
                
            except TimeoutError as te:
                print(f"[DEBUG] â° ëª¨ë¸ ë¡œë”© íƒ€ì„ì•„ì›ƒ: {te}")
                print(f"[DEBUG] í•´ê²°ë°©ì•ˆ:")
                print(f"[DEBUG]   1. ë” í° íƒ€ì„ì•„ì›ƒ ì„¤ì •")
                print(f"[DEBUG]   2. ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš© ê³ ë ¤")
                print(f"[DEBUG]   3. GPU ë©”ëª¨ë¦¬ ìµœì í™”")
                raise
            except Exception as e:
                import traceback
                print(f"[DEBUG] âŒ ì‹¤ì œ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                print(f"[DEBUG] ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
                print(f"[DEBUG] ìƒì„¸ ì˜¤ë¥˜:")
                traceback.print_exc()
                
                # ë©”ëª¨ë¦¬ ìƒíƒœ ì¬í™•ì¸
                try:
                    mem = psutil.virtual_memory()
                    print(f"[DEBUG] ì˜¤ë¥˜ ì‹œì  ë©”ëª¨ë¦¬ - ì‚¬ìš©ë¥ : {mem.percent}%, ì‚¬ìš©ê°€ëŠ¥: {mem.available/1024**3:.1f}GB")
                except:
                    pass
                
                # ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
                print(f"[DEBUG] ë””ë²„ê¹… ì •ë³´:")
                print(f"[DEBUG]   ëª¨ë¸ ê²½ë¡œ: {actual_model_path}")
                print(f"[DEBUG]   ë¡œì»¬ íŒŒì¼ ì „ìš©: True")
                print(f"[DEBUG]   ì‹ ë¢° ì½”ë“œ: True")
                print(f"[DEBUG]   í˜•ë³€í™˜: torch.float32")
                    
                raise
        
        try:
            print(f"[DEBUG] _load_model_sync ì‹œì‘: {model_name}, {model_path}")
            
            # ëª¨ë¸ ì •ë³´ ì´ˆê¸°í™”
            self.models[model_name] = ModelInfo(
                name=model_name, 
                path=model_path, 
                status="loading"
            )
            
            print(f"[DEBUG] ëª¨ë¸ ì •ë³´ ì´ˆê¸°í™”ë¨: {model_name}")
            self._notify_callbacks(model_name, "loading_started", {})
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • ì‹œì‘
            process = psutil.Process()
            mem_before = process.memory_info().rss
            print(f"[DEBUG] ë©”ëª¨ë¦¬ ì¸¡ì • ì‹œì‘: {model_name}")
            
            # HuggingFace ëª¨ë¸ IDì¸ì§€ í™•ì¸í•˜ê³  ë‹¤ìš´ë¡œë“œ
            actual_model_path = model_path
            if self._is_huggingface_model_id(model_path):
                print(f"[DEBUG] HuggingFace ëª¨ë¸ ID ê°ì§€: {model_name}")
                self._notify_callbacks(model_name, "downloading", {'model_id': model_path})
                actual_model_path = self._download_huggingface_model(model_path)
                self.models[model_name].path = actual_model_path  # ì‹¤ì œ ê²½ë¡œë¡œ ì—…ë°ì´íŠ¸
                print(f"[DEBUG] ëª¨ë¸ ë‹¤ìš´ë¡œë“œ/ìºì‹œ í™•ì¸ ì™„ë£Œ: {model_name}")
            
            # ëª¨ë¸ ë¶„ì„ - ì„±ëŠ¥ìƒ ì´ìœ ë¡œ ê°„ì†Œí™”
            print(f"[DEBUG] ëª¨ë¸ ë¶„ì„ ì‹œì‘: {model_name}")
            try:
                # ë¹ ë¥¸ ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰ (ì „ì²´ ë¶„ì„ì€ ìŠ¤í‚µ)
                analysis = {"model_summary": {"supported_tasks": ["feature-extraction"]}}
                self.models[model_name].config_analysis = analysis
                print(f"[DEBUG] ëª¨ë¸ ë¶„ì„ ì™„ë£Œ (ê°„ì†Œí™”): {model_name}")
            except Exception as e:
                print(f"[DEBUG] ëª¨ë¸ ë¶„ì„ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
                self.models[model_name].config_analysis = {"model_summary": {"supported_tasks": ["feature-extraction"]}}
            
            # ë²”ìš©ì ì¸ transformers ëª¨ë¸ ë¡œë“œ
            print(f"[DEBUG] transformers ì„í¬íŠ¸ ì‹œì‘: {model_name}")
            from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, AutoConfig
            print(f"[DEBUG] transformers ì„í¬íŠ¸ ì™„ë£Œ: {model_name}")
            
            # ìµœì  ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ
            print(f"[DEBUG] ë””ë°”ì´ìŠ¤ ì„ íƒ ì‹œì‘: {model_name}")
            device = optimizer.get_optimal_device()
            print(f"[DEBUG] ìë™ ì„ íƒëœ ë””ë°”ì´ìŠ¤: {device}")
            
            # ë©”ëª¨ë¦¬ ìƒíƒœ ì²´í¬
            print(f"[DEBUG] ë©”ëª¨ë¦¬ ì²´í¬ ì‹œì‘: {model_name}")
            memory_info = self.get_memory_info()
            available_memory_gb = memory_info['system_memory']['available'] / (1024**3)
            print(f"[DEBUG] ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: {available_memory_gb:.1f}GB")
            print(f"[DEBUG] ë””ë°”ì´ìŠ¤ ì„¤ì •: {device} (Streamlit ì•ˆì •ì„±ì„ ìœ„í•´ CPU ê°•ì œ)")
            
            # BGE-M3ëŠ” ì„ë² ë”© ëª¨ë¸ì´ë¯€ë¡œ ë¶„ë¥˜ ëª¨ë¸ì´ ì•„ë‹˜
            is_classification_model = False
            print(f"[DEBUG] BGE-M3 ì„ë² ë”© ëª¨ë¸ë¡œ ì„¤ì •")
            
            print(f"[DEBUG] ëª¨ë¸ ë¡œë”© ì‹œì‘: classification={is_classification_model}")
            
            # ì§ì ‘ ëª¨ë¸ ë¡œë”© (ìºì‹œ ìš°íšŒí•˜ì—¬ ì•ˆì •ì„± í™•ë³´)
            print(f"[DEBUG] ì§ì ‘ ëª¨ë¸ ë¡œë”© ì‹œì‘: {model_name}")
            
            try:
                # ëª¨ë¸ ë¡œë”©
                result = load_model_with_transformers(actual_model_path, device)
                
                if len(result) == 3:
                    model, tokenizer, load_time = result
                    print(f"[DEBUG] ëª¨ë¸ ë¡œë”© ì„±ê³µ: {load_time:.1f}ì´ˆ")
                else:
                    raise ValueError("ëª¨ë¸ ë¡œë”© ê²°ê³¼ í˜•ì‹ ì˜¤ë¥˜")
                
            except Exception as e:
                print(f"[DEBUG] ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                raise
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
            mem_after = process.memory_info().rss
            memory_usage = (mem_after - mem_before) / 1024 / 1024  # MB
            
            # ëª¨ë¸ ì •ë³´ ì—…ë°ì´íŠ¸
            self.models[model_name].model = model
            self.models[model_name].tokenizer = tokenizer
            self.models[model_name].memory_usage = memory_usage
            self.models[model_name].load_time = datetime.now()
            self.models[model_name].status = "loaded"
            
            success_data = {
                'memory_usage': memory_usage,
                'load_time': self.models[model_name].load_time,
                'analysis': analysis['model_summary'],
                'original_path': model_path,
                'actual_path': actual_model_path
            }
            
            self._notify_callbacks(model_name, "loading_success", success_data)
            
            if callback:
                callback(model_name, True, f"Model loaded successfully. Memory usage: {memory_usage:.2f} MB")
        
        except TimeoutError as e:
            error_msg = str(e)
            print(f"[DEBUG] ëª¨ë¸ ë¡œë”© íƒ€ì„ì•„ì›ƒ: {error_msg}")
            
            if model_name in self.models:
                self.models[model_name].status = "error"
                self.models[model_name].error_message = error_msg
            
            self._notify_callbacks(model_name, "loading_error", {'error': error_msg})
            
            if callback:
                callback(model_name, False, error_msg)
                
        except Exception as e:
            error_msg = str(e)
            print(f"[DEBUG] ëª¨ë¸ ë¡œë”© ì˜¤ë¥˜: {error_msg}")
            import traceback
            traceback.print_exc()
            
            if model_name in self.models:
                self.models[model_name].status = "error"
                self.models[model_name].error_message = error_msg
            
            self._notify_callbacks(model_name, "loading_error", {'error': error_msg})
            
            if callback:
                callback(model_name, False, error_msg)
        
        finally:
            elapsed = time.time() - start_time
            print(f"[DEBUG] ëª¨ë¸ ë¡œë”© ì´ ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ")
    
    def unload_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        if model_name in self.models:
            model_info = self.models[model_name]
            
            try:
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                if model_info.model:
                    del model_info.model
                if model_info.tokenizer:
                    del model_info.tokenizer
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                # ìƒíƒœ ì—…ë°ì´íŠ¸
                model_info.status = "unloaded"
                model_info.model = None
                model_info.tokenizer = None
                model_info.memory_usage = 0.0
                
                self._notify_callbacks(model_name, "unloaded", {})
                
                return True
                
            except Exception as e:
                print(f"Error unloading model {model_name}: {e}")
                return False
        
        return False
    
    def get_loaded_models(self) -> List[str]:
        """ë¡œë“œëœ ëª¨ë¸ ëª©ë¡"""
        return [name for name, info in self.models.items() if info.status == "loaded"]
    
    def get_model_info(self, model_name: str) -> Optional[ModelInfo]:
        """íŠ¹ì • ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        return self.models.get(model_name)
    
    def get_all_models_status(self) -> Dict:
        """ëª¨ë“  ëª¨ë¸ ìƒíƒœ ì •ë³´"""
        return {
            name: {
                'status': info.status,
                'memory_usage': info.memory_usage,
                'load_time': info.load_time.isoformat() if info.load_time else None,
                'path': info.path,
                'error_message': info.error_message,
                'config_analysis': info.config_analysis
            }
            for name, info in self.models.items()
        }
    
    def get_model_for_inference(self, model_name: str) -> Optional[tuple]:
        """ì¶”ë¡ ìš© ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë°˜í™˜"""
        if model_name in self.models and self.models[model_name].status == "loaded":
            model_info = self.models[model_name]
            return model_info.model, model_info.tokenizer
        return None
    
    def get_system_summary(self) -> Dict:
        """ì‹œìŠ¤í…œ ìš”ì•½ ì •ë³´"""
        loaded_count = len(self.get_loaded_models())
        total_memory = sum(info.memory_usage for info in self.models.values() if info.status == "loaded")
        
        memory_info = self.get_memory_info()
        
        return {
            'loaded_models_count': loaded_count,
            'total_models_count': len(self.models),
            'total_memory_usage_mb': total_memory,
            'system_memory_info': memory_info,
            'models_by_status': {
                'loaded': len([m for m in self.models.values() if m.status == "loaded"]),
                'loading': len([m for m in self.models.values() if m.status == "loading"]),
                'error': len([m for m in self.models.values() if m.status == "error"]),
                'unloaded': len([m for m in self.models.values() if m.status == "unloaded"])
            }
        }
    
    def remove_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ì™„ì „ ì œê±°"""
        if model_name in self.models:
            # ë¨¼ì € ì–¸ë¡œë“œ
            if self.models[model_name].status == "loaded":
                self.unload_model(model_name)
            
            # ëª¨ë¸ ì •ë³´ ì œê±°
            del self.models[model_name]
            
            # ë¡œë”© ë½ ì œê±°
            if model_name in self.loading_locks:
                del self.loading_locks[model_name]
            
            self._notify_callbacks(model_name, "removed", {})
            return True
        
        return False
    
    def clear_all_models(self):
        """ëª¨ë“  ëª¨ë¸ ì •ë¦¬"""
        model_names = list(self.models.keys())
        for model_name in model_names:
            self.remove_model(model_name)
    
    def get_available_tasks(self, model_name: str) -> List[str]:
        """ëª¨ë¸ì´ ì§€ì›í•˜ëŠ” íƒœìŠ¤í¬ ëª©ë¡"""
        if model_name in self.models:
            analysis = self.models[model_name].config_analysis
            if analysis and 'model_summary' in analysis:
                return analysis['model_summary'].get('supported_tasks', [])
        return []
    
    def export_models_info(self) -> Dict:
        """ëª¨ë¸ ì •ë³´ ë‚´ë³´ë‚´ê¸°"""
        export_data = {
            'timestamp': datetime.now().isoformat(),
            'system_info': self.get_system_summary(),
            'models': {}
        }
        
        for name, info in self.models.items():
            export_data['models'][name] = {
                'name': info.name,
                'path': info.path,
                'status': info.status,
                'memory_usage': info.memory_usage,
                'load_time': info.load_time.isoformat() if info.load_time else None,
                'config_analysis': info.config_analysis
            }
        
        return export_data
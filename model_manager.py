import threading
import psutil
import torch
import os
import re
import hashlib
import logging
from typing import Dict, List, Optional, Callable
from dataclasses import dataclass, field
from datetime import datetime
from model_analyzer import ComprehensiveModelAnalyzer
from model_optimization import optimizer
from model_cache import model_cache
from device_manager import device_manager
from detailed_profiler import profiler
from huggingface_hub import hf_hub_download, snapshot_download, HfApi

@dataclass
class ModelInfo:
    name: str
    path: str
    model: Optional[object] = None
    tokenizer: Optional[object] = None
    config_analysis: Dict = field(default_factory=dict)
    memory_usage: float = 0.0
    load_time: Optional[datetime] = None
    status: str = "unloaded"  # unloaded, loading, loaded, error
    error_message: str = ""

class MultiModelManager:
    def __init__(self):
        self.models: Dict[str, ModelInfo] = {}
        self.loading_locks = {}
        self.max_memory_threshold = 0.8  # 80% ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì œí•œ
        self.load_queue = []
        self.model_analyzer = ComprehensiveModelAnalyzer()
        self.callbacks: List[Callable] = []
        self.hf_api = HfApi()
        
    def add_callback(self, callback: Callable):
        """ëª¨ë¸ ìƒíƒœ ë³€ê²½ ì½œë°± ë“±ë¡"""
        self.callbacks.append(callback)
        
    def _notify_callbacks(self, model_name: str, event_type: str, data: Dict = None):
        """ì½œë°± í•¨ìˆ˜ë“¤ì—ê²Œ ì•Œë¦¼"""
        for callback in self.callbacks:
            try:
                callback(model_name, event_type, data or {})
            except Exception as e:
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - Callback error: {e}")
    
    def get_memory_info(self):
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´"""
        memory = psutil.virtual_memory()
        gpu_memory = []
        
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                gpu_memory.append({
                    'device': i,
                    'name': torch.cuda.get_device_name(i),
                    'total': torch.cuda.get_device_properties(i).total_memory,
                    'allocated': torch.cuda.memory_allocated(i),
                    'reserved': torch.cuda.memory_reserved(i)
                })
        
        return {
            'system_memory': {
                'total': memory.total,
                'available': memory.available,
                'used': memory.used,
                'percent': memory.percent
            },
            'gpu_memory': gpu_memory
        }
    
    def can_load_model(self, estimated_size: float) -> bool:
        """ëª¨ë¸ ë¡œë“œ ê°€ëŠ¥ ì—¬ë¶€ ì²´í¬"""
        memory_info = self.get_memory_info()
        current_usage = memory_info['system_memory']['percent'] / 100
        return current_usage + (estimated_size / memory_info['system_memory']['total']) < self.max_memory_threshold
    
    def analyze_model(self, model_path: str) -> Dict:
        """ëª¨ë¸ ë¶„ì„ (ë¡œë“œ ì—†ì´, HuggingFace ëª¨ë¸ ID ì§€ì›)"""
        try:
            actual_model_path = model_path
            
            # HuggingFace ëª¨ë¸ IDì¸ê²½ìš° ë‹¤ìš´ë¡œë“œ
            if self._is_huggingface_model_id(model_path):
                actual_model_path = self._download_huggingface_model(model_path)
            
            analysis = self.model_analyzer.analyze_model_directory(actual_model_path)
            analysis['original_path'] = model_path
            analysis['actual_path'] = actual_model_path
            return analysis
        except Exception as e:
            return {'error': str(e)}
    
    def _is_huggingface_model_id(self, model_path: str) -> bool:
        """HuggingFace ëª¨ë¸ ID í˜•ì‹ì¸ì§€ í™•ì¸"""
        # ë¡œì»¬ ê²½ë¡œê°€ ì•„ë‹Œ ê²½ìš° (ì ˆëŒ€ê²½ë¡œë‚˜ ìƒëŒ€ê²½ë¡œ)
        if os.path.isabs(model_path) or os.path.exists(model_path):
            return False
        
        # URLì´ ì•„ë‹Œ ê²½ìš°
        if model_path.startswith(('http://', 'https://', 'file://')):
            return False
        
        # HuggingFace ëª¨ë¸ ID í˜•ì‹ í™•ì¸ (username/model-name)
        if '/' in model_path and not model_path.startswith('/'):
            return True
        
        return False
    
    def _download_huggingface_model(self, model_id: str) -> str:
        """HuggingFace Hubì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        try:
            # ëª¨ë¸ ì •ë³´ í™•ì¸
            model_info = self.hf_api.model_info(model_id)
            
            # ë¡œì»¬ ìºì‹œ ë””ë ‰í† ë¦¬ë¡œ ë‹¤ìš´ë¡œë“œ (ì´ë¯¸ ìºì‹œëœ ê²½ìš° ë‹¤ìš´ë¡œë“œ ë°©ì§€)
            try:
                # ë¨¼ì € ë¡œì»¬ ìºì‹œì—ì„œ ì°¾ê¸° ì‹œë„
                local_path = snapshot_download(
                    repo_id=model_id,
                    repo_type="model", 
                    cache_dir=None,  # ê¸°ë³¸ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
                    local_files_only=True  # ì´ë¯¸ ìºì‹œëœ íŒŒì¼ë§Œ ì‚¬ìš©
                )
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ìºì‹œì—ì„œ ëª¨ë¸ ì°¾ìŒ: {model_id}")
            except Exception:
                # ìºì‹œì— ì—†ëŠ” ê²½ìš°ì—ë§Œ ë‹¤ìš´ë¡œë“œ
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ìºì‹œ ë¯¸ìŠ¤ - ë‹¤ìš´ë¡œë“œ ì‹œì‘: {model_id}")
                local_path = snapshot_download(
                    repo_id=model_id,
                    repo_type="model",
                    cache_dir=None,  # ê¸°ë³¸ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
                    local_files_only=False
                )
            
            return local_path
            
        except Exception as e:
            raise Exception(f"HuggingFace ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    
    def _generate_model_name(self, model_path: str) -> str:
        """ëª¨ë¸ ê²½ë¡œì—ì„œ ìë™ìœ¼ë¡œ ëª¨ë¸ ì´ë¦„ ìƒì„±"""
        if self._is_huggingface_model_id(model_path):
            # HuggingFace ëª¨ë¸ IDì—ì„œ ëª¨ë¸ ì´ë¦„ ì¶”ì¶œ
            return model_path.split('/')[-1]
        else:
            # ë¡œì»¬ ê²½ë¡œì—ì„œ ëª¨ë¸ ì´ë¦„ ì¶”ì¶œ
            return os.path.basename(model_path.rstrip('/'))
    
    def load_model_async(self, model_name: str, model_path: str, callback: Optional[Callable] = None):
        """ë¹„ë™ê¸° ëª¨ë¸ ë¡œë“œ (HuggingFace ëª¨ë¸ ID ì§€ì›, ëª¨ë¸ ì´ë¦„ ìë™ ìƒì„±)"""
        # ëª¨ë¸ ì´ë¦„ì´ ë¹„ì–´ìˆìœ¼ë©´ ìë™ ìƒì„±
        if not model_name or not model_name.strip():
            model_name = self._generate_model_name(model_path)
        
        # ì¤‘ë³µ ëª¨ë¸ ì´ë¦„ ì²˜ë¦¬
        original_name = model_name
        counter = 1
        while model_name in self.models:
            model_name = f"{original_name}_{counter}"
            counter += 1
        
        print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - load_model_async ì‹œì‘: {model_name}, {model_path}")
        
        # ë¡œë”© ë½ ì„¤ì •
        if model_name not in self.loading_locks:
            self.loading_locks[model_name] = threading.Lock()
        
        # ìŠ¤ë ˆë“œ ì‹œì‘
        print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ìŠ¤ë ˆë“œ ìƒì„± ì¤‘: {model_name}")
        thread = threading.Thread(
            target=self._load_model_sync, 
            args=(model_name, model_path, callback),
            name=f"ModelLoad-{model_name}"
        )
        thread.daemon = True
        print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ìŠ¤ë ˆë“œ ì‹œì‘ ì „: {model_name}")
        thread.start()
        print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ìŠ¤ë ˆë“œ ì‹œì‘ë¨: {model_name}, thread={thread}")
        
        return thread
    
    def _load_model_sync(self, model_name: str, model_path: str, callback: Optional[Callable] = None):
        """ì‹¤ì œ ëª¨ë¸ ë¡œë”© ì‘ì—… (ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)"""
        import time
        import threading
        import queue
        
        start_time = time.time()
        
        def load_model_with_transformers(actual_model_path, device):
            """Fast ëª¨ë¸ ë¡œë”©"""
            print(f"[FAST] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë¸ ë¡œë”© ì‹œì‘")
            
            # í”„ë¡œíŒŒì¼ë§ ì‹œì‘ (í”„ë¡œíŒŒì¼ëŸ¬ ë‚´ë¶€ì—ì„œ í™œì„±í™” ì—¬ë¶€ í™•ì¸)
            profiler.start_profiling("ëª¨ë¸ ë¡œë”©")
            profiler.memory_snapshot("ì´ˆê¸° ìƒíƒœ")
            
            # ì§ì ‘ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©ìœ¼ë¡œ ì‹¤ì œ BGE-M3 ëª¨ë¸ ë¡œë”©
            print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ì‹¤ì œ transformers ëª¨ë¸ ë¡œë”© ì‹œì‘")
            
            import time
            load_start = time.time()
            
            try:
                from transformers import AutoModel, AutoTokenizer, AutoConfig
                
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - Config ë¡œë”© ì‹œì‘: {model_name}")
                
                # ë¹ ë¥¸ ë¡œì»¬ config í™•ì¸
                try:
                    import json
                    config_path = os.path.join(actual_model_path, "config.json")
                    if os.path.exists(config_path):
                        with open(config_path, 'r') as f:
                            config_dict = json.load(f)
                        print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ë¡œì»¬ config ë¡œë”© ì™„ë£Œ: {model_name}")
                    else:
                        config = AutoConfig.from_pretrained(actual_model_path, local_files_only=True)
                        print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - AutoConfig ë¡œë”© ì™„ë£Œ: {model_name}")
                except Exception as e:
                    print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - Config ë¡œë”© ì˜¤ë¥˜, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
                
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ì‹¤ì œ ëª¨ë¸ ë¡œë”© ì‹œì‘: {model_name}")
                
                # ëª¨ë¸ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë¸ íŒŒì¼ í™•ì¸ ì¤‘: {actual_model_path}")
                model_files = [
                    "config.json",
                    "pytorch_model.bin", 
                    "model.safetensors",
                    "tokenizer.json",
                    "tokenizer_config.json"
                ]
                
                for file in model_files:
                    file_path = os.path.join(actual_model_path, file)
                    exists = os.path.exists(file_path)
                    if exists:
                        size_mb = os.path.getsize(file_path) / (1024*1024)
                        print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âœ… {file}: {size_mb:.1f}MB")
                    else:
                        print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âŒ {file}: íŒŒì¼ ì—†ìŒ")
                
                # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
                import psutil
                mem = psutil.virtual_memory()
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ë©”ëª¨ë¦¬ ìƒíƒœ - ì‚¬ìš©ë¥ : {mem.percent}%, ì‚¬ìš©ê°€ëŠ¥: {mem.available/1024**3:.1f}GB")
                
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - AutoModel.from_pretrained í˜¸ì¶œ ì‹œì‘ (í° ëª¨ë¸ì´ë¯€ë¡œ ì‹œê°„ ì†Œìš” ì˜ˆìƒ)")
                
                # í™˜ê²½ ë³€ìˆ˜ ìƒíƒœ í™•ì¸
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - í™˜ê²½ ë³€ìˆ˜ í™•ì¸:")
                env_vars = {
                    'HF_HUB_OFFLINE': os.getenv('HF_HUB_OFFLINE', 'None'),
                    'TRANSFORMERS_OFFLINE': os.getenv('TRANSFORMERS_OFFLINE', 'None'),
                    'HF_HUB_DISABLE_TELEMETRY': os.getenv('HF_HUB_DISABLE_TELEMETRY', 'None'),
                    'TOKENIZERS_PARALLELISM': os.getenv('TOKENIZERS_PARALLELISM', 'None')
                }
                for key, value in env_vars.items():
                    print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} -   {key}={value}")
                
                model_start = time.time()
                
                # AutoModel ë¡œë”©ì„ ë‹¨ê³„ë³„ë¡œ ë¶„í• í•˜ì—¬ ì§„í–‰ ìƒíƒœ ì¶”ì 
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - 1/5: transformers AutoModel ì„í¬íŠ¸ í™•ì¸")
                from transformers import AutoModel
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - 2/5: AutoConfig ì‚¬ì „ ë¡œë”©")
                
                # Config ë¨¼ì € ë¡œë”©í•˜ì—¬ ëª¨ë¸ êµ¬ì¡° í™•ì¸
                try:
                    from transformers import AutoConfig
                    print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - Config ë¡œë”© ì‹œë„: {actual_model_path}")
                    config = AutoConfig.from_pretrained(
                        actual_model_path,
                        local_files_only=True,
                        trust_remote_code=True
                    )
                    print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âœ… Config ë¡œë”© ì„±ê³µ: {config.__class__.__name__}")
                    print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë¸ íƒ€ì…: {getattr(config, 'model_type', 'Unknown')}")
                    print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ì–´íœ˜ í¬ê¸°: {getattr(config, 'vocab_size', 'Unknown')}")
                    print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ìˆ¨ê²¨ì§„ í¬ê¸°: {getattr(config, 'hidden_size', 'Unknown')}")
                except Exception as config_e:
                    print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âš ï¸ Config ë¡œë”© ì‹¤íŒ¨, ê³„ì† ì§„í–‰: {config_e}")
                
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - 3/5: ì‹¤ì œ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë”© ì‹œì‘ (ê°€ì¥ ì‹œê°„ ì†Œìš” ë‹¨ê³„)")
                
                # ë¡œë”© íƒ€ì„ì•„ì›ƒ ë° ì§„í–‰ìƒí™© ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ìŠ¤ë ˆë“œ ìƒì„±
                import threading
                import queue
                
                loading_result = queue.Queue()
                loading_error = queue.Queue()
                
                def load_model_with_progress():
                    """ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ëª¨ë¸ ë¡œë”© ìˆ˜í–‰ - ULTRA ìµœì í™”"""
                    try:
                        print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë¸ ë¡œë”© ìŠ¤ë ˆë“œ ì‹œì‘ (ULTRA ëª¨ë“œ)")
                        
                        # í•„ìš”í•œ ëª¨ë“ˆë“¤ ì„í¬íŠ¸
                        import os
                        import torch
                        import gc
                        import time
                        
                        # í™˜ê²½ë³€ìˆ˜ ìµœì í™” (BGE-M3 ì „ìš©)
                        original_env = {}
                        ultra_env = {
                            'OMP_NUM_THREADS': '4',  # OpenMP ìŠ¤ë ˆë“œ ì œí•œ
                            'MKL_NUM_THREADS': '4',  # Intel MKL ì œí•œ
                            'TOKENIZERS_PARALLELISM': 'false',  # í† í¬ë‚˜ì´ì € ë³‘ë ¬í™” ë¹„í™œì„±í™”
                            'TRANSFORMERS_VERBOSITY': 'error',  # ë¡œê·¸ ìµœì†Œí™”
                            'HF_HUB_DISABLE_TELEMETRY': '1',
                            'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:128'  # CUDA ë©”ëª¨ë¦¬ ìµœì í™”
                        }
                        
                        for key, value in ultra_env.items():
                            original_env[key] = os.getenv(key)
                            os.environ[key] = value
                        
                        try:
                            # í˜ì‹ ì  ì ‘ê·¼: transformers ì™„ì „ ìš°íšŒí•˜ê³  ì§ì ‘ ë¡œë”©
                            print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - transformers ìš°íšŒ - ì§ì ‘ PyTorch ë¡œë”© ì‹œì‘")
                        
                            # ëª¨ë¸ íŒŒì¼ ê°ì§€ ë° ìµœì  íŒŒì¼ ì„ íƒ
                            safetensors_path = os.path.join(actual_model_path, "model.safetensors")
                            pytorch_path = os.path.join(actual_model_path, "pytorch_model.bin")
                            
                            has_safetensors = os.path.exists(safetensors_path)
                            has_pytorch = os.path.exists(pytorch_path)
                            
                            print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë¸ íŒŒì¼ ê°ì§€:")
                            print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} -   safetensors: {'âœ…' if has_safetensors else 'âŒ'} ({os.path.getsize(safetensors_path)/1024**3:.1f}GB)" if has_safetensors else f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} -   safetensors: âŒ")
                            print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} -   pytorch_model.bin: {'âœ…' if has_pytorch else 'âŒ'} ({os.path.getsize(pytorch_path)/1024**3:.1f}GB)" if has_pytorch else f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} -   pytorch_model.bin: âŒ")
                        
                            # ê³ ì† ë¡œë”©ì„ ìœ„í•œ ê°€ì¤‘ì¹˜ íŒŒì¼ ì„ íƒ (ë¹ ë¥¸ ê²ƒ ìš°ì„ )
                            if has_pytorch:
                                weight_file = pytorch_path
                                file_format = "pytorch"
                                print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - PyTorch í˜•ì‹ ì„ íƒ (2.2GB ë¹ ë¥¸ ë¡œë”©)")
                            elif has_safetensors:
                                weight_file = safetensors_path  
                                file_format = "safetensors"
                                print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - Safetensors í˜•ì‹ ì„ íƒ")
                            else:
                                raise FileNotFoundError("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                            
                            print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ì„ íƒëœ ê°€ì¤‘ì¹˜: {weight_file}")
                        
                            # ULTRA ë°©ì‹: ì§ì ‘ ëª¨ë¸ ì´ˆê¸°í™” + ê°€ì¤‘ì¹˜ ë¡œë”©
                            start_time = time.time()
                            
                            print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - 1/4: Config ê¸°ë°˜ ë¹ˆ ëª¨ë¸ ìƒì„±")
                            
                            # ëª¨ë¸ íƒ€ì…ë³„ ë™ì  í´ë˜ìŠ¤ ì„ íƒ
                            model_type = config.model_type.lower()
                            print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ê°ì§€ëœ ëª¨ë¸ íƒ€ì…: {model_type}")
                            
                            if model_type == "xlm-roberta":
                                from transformers import XLMRobertaModel
                                model = XLMRobertaModel(config)
                                print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - XLMRobertaModel ì´ˆê¸°í™” ì™„ë£Œ")
                            elif model_type == "distilbert":
                                from transformers import DistilBertModel
                                model = DistilBertModel(config)
                                print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - DistilBertModel ì´ˆê¸°í™” ì™„ë£Œ")
                            elif model_type == "bert":
                                from transformers import BertModel
                                model = BertModel(config)
                                print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - BertModel ì´ˆê¸°í™” ì™„ë£Œ")
                            elif model_type == "roberta":
                                from transformers import RobertaModel
                                model = RobertaModel(config)
                                print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - RobertaModel ì´ˆê¸°í™” ì™„ë£Œ")
                            else:
                                # ë²”ìš© AutoModel ì‚¬ìš© (ë” ëŠë¦¬ì§€ë§Œ ì•ˆì „)
                                print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ íƒ€ì…, AutoModel ì‚¬ìš©")
                                from transformers import AutoModel
                                model = AutoModel.from_config(config)
                                print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - AutoModel ì´ˆê¸°í™” ì™„ë£Œ")
                            
                            model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
                            init_time = time.time() - start_time
                            print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: {init_time:.1f}ì´ˆ")
                            
                            print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - 2/4: ê°€ì¤‘ì¹˜ ë¡œë”© ì‹œì‘ ({file_format}) - PARALLEL ëª¨ë“œ")
                            weight_start = time.time()
                            
                            # ëª¨ë¸ë³„ ìµœì í™”ëœ ë³‘ë ¬ ë¡œë”© ì„¤ì •
                            if model_type == "distilbert":
                                # DistilBertëŠ” ì‘ì€ ëª¨ë¸ì´ë¯€ë¡œ ë” ë§ì€ ìŠ¤ë ˆë“œ ì‚¬ìš© ê°€ëŠ¥
                                thread_count = min(6, os.cpu_count())
                            elif model_type == "xlm-roberta":
                                # XLM-RoBERTaëŠ” í° ëª¨ë¸ì´ë¯€ë¡œ ì ë‹¹í•œ ìŠ¤ë ˆë“œ ìˆ˜
                                thread_count = min(4, os.cpu_count())
                            else:
                                # ê¸°íƒ€ ëª¨ë¸ë“¤ì€ ë³´ìˆ˜ì  ì„¤ì •
                                thread_count = min(3, os.cpu_count())
                            
                            torch.set_num_threads(thread_count)
                            print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë¸ë³„ ìµœì í™” ìŠ¤ë ˆë“œ ìˆ˜ ({model_type}): {torch.get_num_threads()}")
                            
                            if file_format == "safetensors":
                                # Safetensors ë¹ ë¥¸ ë¡œë”©
                                try:
                                    from safetensors.torch import load_file
                                    state_dict = load_file(weight_file, device='cpu')
                                    print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âœ… Safetensors ë¡œë”© ì™„ë£Œ")
                                except ImportError:
                                    print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âš ï¸ safetensors ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ, PyTorch ë¡œë”©ìœ¼ë¡œ ëŒ€ì²´")
                                    state_dict = torch.load(weight_file, map_location='cpu')
                            else:
                                # PyTorch ë¡œë”© (ë©”ëª¨ë¦¬ ë§¤í•‘ ìµœì í™”)
                                print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - PyTorch PARALLEL ë¡œë”© (mmap + ë©€í‹°ìŠ¤ë ˆë“œ)")
                                try:
                                    # ë©”ëª¨ë¦¬ ë§¤í•‘ + ë³‘ë ¬ ë¡œë”© ì‹œë„
                                    state_dict = torch.load(weight_file, map_location='cpu', mmap=True)
                                    print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âœ… mmap ë¡œë”© ì„±ê³µ")
                                except Exception as mmap_error:
                                    print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - mmap ì‹¤íŒ¨, ì¼ë°˜ ë¡œë”©ìœ¼ë¡œ ëŒ€ì²´: {mmap_error}")
                                    state_dict = torch.load(weight_file, map_location='cpu')
                            
                            weight_time = time.time() - weight_start
                            print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ê°€ì¤‘ì¹˜ ë¡œë”© ì™„ë£Œ: {weight_time:.1f}ì´ˆ, í‚¤ ê°œìˆ˜: {len(state_dict)}")
                            
                            print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - 3/4: ëª¨ë¸ì— ê°€ì¤‘ì¹˜ ì ìš©")
                            apply_start = time.time()
                            
                            # ê°€ì¤‘ì¹˜ ì ìš© (ì—„ê²©í•œ ëª¨ë“œ ë¹„í™œì„±í™”)
                            missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
                            
                            # ë©”ëª¨ë¦¬ ì •ë¦¬
                            del state_dict
                            gc.collect()
                            
                            apply_time = time.time() - apply_start
                            print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ê°€ì¤‘ì¹˜ ì ìš© ì™„ë£¼: {apply_time:.1f}ì´ˆ")
                            
                            if missing_keys:
                                print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âš ï¸ ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ")
                            if unexpected_keys:
                                print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ í‚¤: {len(unexpected_keys)}ê°œ")
                            
                            print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - 4/4: ìµœì¢… ì„¤ì •")
                            model.eval()  # ë‹¤ì‹œ í‰ê°€ ëª¨ë“œ í™•ì¸
                            
                            total_time = time.time() - start_time
                            print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âœ… ULTRA ë¡œë”© ì„±ê³µ: {total_time:.1f}ì´ˆ (ê¸°ì¡´ 5ë¶„+ ëŒ€ë¹„ {300/total_time:.1f}x ë¹ ë¦„)")
                            
                            loading_result.put(model)
                            return
                            
                        except Exception as ultra_error:
                            print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âŒ ULTRA ë¡œë”© ì‹¤íŒ¨: {ultra_error}")
                            print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ì˜¤ë¥˜ íƒ€ì…: {type(ultra_error).__name__}")
                            
                            # ULTRA ì‹¤íŒ¨ ì‹œ ë³´ì¡° ë°©ë²•ë“¤ ì‹œë„
                            print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ë³´ì¡° ë°©ë²• 1: ë©”ëª¨ë¦¬ ë§¤í•‘ ì—†ì´ ì¬ì‹œë„")
                            try:
                                # ëª¨ë¸ íƒ€ì… ë‹¤ì‹œ í™•ì¸ (fallbackì—ì„œ ì‚¬ìš©)
                                model_type = config.model_type.lower()
                                
                                # ë©”ëª¨ë¦¬ ë§¤í•‘ ì—†ì´ ì¬ì‹œë„
                                gc.collect()
                                if has_pytorch:
                                    state_dict = torch.load(pytorch_path, map_location='cpu', mmap=False)
                                else:
                                    from safetensors.torch import load_file
                                    state_dict = load_file(safetensors_path, device='cpu')
                                
                                # ëª¨ë¸ íƒ€ì…ë³„ ë™ì  í´ë˜ìŠ¤ ì„ íƒ (fallback 1)
                                if model_type == "xlm-roberta":
                                    from transformers import XLMRobertaModel
                                    model = XLMRobertaModel(config)
                                elif model_type == "distilbert":
                                    from transformers import DistilBertModel
                                    model = DistilBertModel(config)
                                elif model_type == "bert":
                                    from transformers import BertModel
                                    model = BertModel(config)
                                elif model_type == "roberta":
                                    from transformers import RobertaModel
                                    model = RobertaModel(config)
                                else:
                                    # ë²”ìš© AutoModel ì‚¬ìš©
                                    from transformers import AutoModel
                                    model = AutoModel.from_config(config)
                                
                                model.load_state_dict(state_dict, strict=False)
                                del state_dict
                                gc.collect()
                                model.eval()
                                print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âœ… ë³´ì¡° ë°©ë²• 1 ì„±ê³µ")
                                loading_result.put(model)
                                return
                            except Exception as fallback1_error:
                                print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âŒ ë³´ì¡° ë°©ë²• 1 ì‹¤íŒ¨: {fallback1_error}")
                            
                            # ë³´ì¡° ë°©ë²• 2: ì „í†µì  transformers ë¡œë”© (ìµœí›„ ìˆ˜ë‹¨)
                            print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ë³´ì¡° ë°©ë²• 2: ì „í†µì  transformers ë¡œë”© (ìµœí›„ ìˆ˜ë‹¨)")
                            try:
                                # ëª¨ë¸ íƒ€ì… ë‹¤ì‹œ í™•ì¸ (fallbackì—ì„œ ì‚¬ìš©)
                                model_type = config.model_type.lower()
                                
                                from transformers import AutoModel
                                
                                # ëŒ€ê·œëª¨ ë©”ëª¨ë¦¬ ì •ë¦¬
                                gc.collect()
                                if torch.cuda.is_available():
                                    torch.cuda.empty_cache()
                                print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëŒ€ê·œëª¨ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£¼")
                                
                                # ë§ˆì§€ë§‰ ìˆ˜ë‹¨: ëª¨ë¸ë³„ ìµœì í™”ëœ transformers ë¡œë”©
                                print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë¸ë³„ ìµœì í™” AutoModel ì‹œë„ (íƒ€ì…: {model_type})")
                                
                                # ëª¨ë¸ íƒ€ì…ë³„ ìµœì í™” ì„¤ì •
                                if model_type == "distilbert":
                                    # DistilBertëŠ” ì‘ì€ ëª¨ë¸ì´ë¯€ë¡œ ë¹ ë¥¸ ë¡œë”© ì˜µì…˜
                                    model = AutoModel.from_pretrained(
                                        actual_model_path,
                                        local_files_only=True,
                                        torch_dtype=torch.float32,
                                        trust_remote_code=False,  # DistilBertëŠ” í‘œì¤€ ëª¨ë¸
                                        use_safetensors=has_safetensors,
                                        low_cpu_mem_usage=False  # ì‘ì€ ëª¨ë¸ì´ë¯€ë¡œ ë©”ëª¨ë¦¬ ìµœì í™” ë¶ˆí•„ìš”
                                    )
                                elif model_type == "xlm-roberta":
                                    # XLM-RoBERTaëŠ” í° ëª¨ë¸ì´ë¯€ë¡œ ë©”ëª¨ë¦¬ ìµœì í™”
                                    model = AutoModel.from_pretrained(
                                        actual_model_path,
                                        local_files_only=True,
                                        torch_dtype=torch.float32,
                                        trust_remote_code=True,
                                        use_safetensors=has_safetensors,
                                        low_cpu_mem_usage=True
                                    )
                                else:
                                    # ê¸°íƒ€ ëª¨ë¸ë“¤ì€ ê· í˜•ì¡íŒ ì„¤ì •
                                    model = AutoModel.from_pretrained(
                                        actual_model_path,
                                        local_files_only=True,
                                        torch_dtype=torch.float32,
                                        trust_remote_code=True,
                                        use_safetensors=has_safetensors,
                                        low_cpu_mem_usage=True
                                    )
                                print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âœ… ë³´ì¡° ë°©ë²• 2 ì„±ê³µ (transformers ê¸°ë³¸)")
                                loading_result.put(model)
                                return
                                
                                # Configë¡œë¶€í„° ë¹ˆ ëª¨ë¸ ìƒì„± (ëª¨ë¸ íƒ€ì…ë³„ ë™ì  ì„ íƒ)
                                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ë¹ˆ ëª¨ë¸ êµ¬ì¡° ìƒì„± (íƒ€ì…: {model_type})")
                                if model_type == "xlm-roberta":
                                    from transformers import XLMRobertaModel
                                    model = XLMRobertaModel(config)
                                elif model_type == "distilbert":
                                    from transformers import DistilBertModel
                                    model = DistilBertModel(config)
                                elif model_type == "bert":
                                    from transformers import BertModel
                                    model = BertModel(config)
                                elif model_type == "roberta":
                                    from transformers import RobertaModel
                                    model = RobertaModel(config)
                                else:
                                    # ë²”ìš© AutoModel ì‚¬ìš©
                                    from transformers import AutoModel
                                    model = AutoModel.from_config(config)
                                
                                # ê°€ì¥ ì•ˆì „í•œ ê°€ì¤‘ì¹˜ íŒŒì¼ ì„ íƒ
                                if has_pytorch:
                                    weight_file = pytorch_path
                                    print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - PyTorch ê°€ì¤‘ì¹˜ ì‚¬ìš©: {weight_file}")
                                elif has_safetensors:
                                    weight_file = safetensors_path
                                    print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - Safetensors ê°€ì¤‘ì¹˜ ì‚¬ìš©: {weight_file}")
                                else:
                                    raise FileNotFoundError("ì‚¬ìš© ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                                
                                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ê°€ì¤‘ì¹˜ íŒŒì¼ ë¡œë”©: {weight_file}")
                                
                                if weight_file.endswith('.safetensors'):
                                    # safetensors ë¡œë”©
                                    from safetensors.torch import load_file
                                    state_dict = load_file(weight_file)
                                    print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âœ… Safetensors ë¡œë”© ì™„ë£Œ, í‚¤ ê°œìˆ˜: {len(state_dict)}")
                                else:
                                    # PyTorch ë¡œë”©
                                    state_dict = torch.load(weight_file, map_location='cpu')
                                    print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âœ… PyTorch ê°€ì¤‘ì¹˜ ë¡œë”© ì™„ë£¼, í‚¤ ê°œìˆ˜: {len(state_dict)}")
                                
                                # ëª¨ë¸ì— ê°€ì¤‘ì¹˜ ì ìš©
                                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë¸ì— ê°€ì¤‘ì¹˜ ì ìš©")
                                missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
                                
                                if missing_keys:
                                    print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âš ï¸ ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ")
                                if unexpected_keys:
                                    print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ í‚¤: {len(unexpected_keys)}ê°œ")
                                    
                                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âœ… ë°©ë²• 2 ì„±ê³µ: ì§ì ‘ ê°€ì¤‘ì¹˜ ë¡œë”© ì™„ë£Œ")
                                
                                loading_result.put(model)
                                return
                                
                            except Exception as fallback2_error:
                                print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âŒ ë³´ì¡° ë°©ë²• 2ë„ ì‹¤íŒ¨: {fallback2_error}")
                                
                                # ëª¨ë“  ULTRA ë°©ë²• ì‹¤íŒ¨ ì‹œ ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ ì œê³µ
                                error_summary = f"ëª¨ë“  ULTRA ë¡œë”© ë°©ë²• ì‹¤íŒ¨:\n" \
                                              f"ULTRA ë©”ì¸: {ultra_error}\n" \
                                              f"ë³´ì¡° 1: {fallback1_error}\n" \
                                              f"ë³´ì¡° 2: {fallback2_error}\n\n" \
                                              f"ì¶”ì²œ ëŒ€ì•ˆ ëª¨ë¸ (ë¹ ë¥¸ ë¡œë”©):\n" \
                                              f"- sentence-transformers/all-MiniLM-L6-v2 (90MB, 30ì´ˆ)\n" \
                                              f"- sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 (420MB, 2ë¶„)\n" \
                                              f"- intfloat/e5-small-v2 (120MB, 45ì´ˆ)"
                                print(f"[ULTRA] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {error_summary}")
                                raise Exception(error_summary)
                        
                        finally:
                            # í™˜ê²½ ë³€ìˆ˜ ë³µì›
                            for key, original_value in original_env.items():
                                if original_value is None:
                                    if key in os.environ:
                                        del os.environ[key]
                                else:
                                    os.environ[key] = original_value
                        
                    except Exception as e:
                        print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë“  ë¡œë”© ë°©ë²• ì‹¤íŒ¨: {e}")
                        loading_error.put(e)
                
                # ë¡œë”© ìŠ¤ë ˆë“œ ì‹œì‘
                loading_thread = threading.Thread(target=load_model_with_progress)
                loading_thread.daemon = True
                loading_thread.start()
                
                # ì§„í–‰ìƒí™© ëª¨ë‹ˆí„°ë§ (15ì´ˆë§ˆë‹¤ ìƒíƒœ ì¶œë ¥)
                timeout_seconds = 600  # 10ë¶„ íƒ€ì„ì•„ì›ƒ (BGE-M3ëŠ” 5-6ë¶„ ì†Œìš”)
                check_interval = 15    # 15ì´ˆë§ˆë‹¤ ì²´í¬ (ë” ì„¸ë°€í•œ ì§„í–‰ë¥  í‘œì‹œ)
                elapsed_checks = 0
                
                while loading_thread.is_alive():
                    loading_thread.join(timeout=check_interval)
                    
                    if loading_thread.is_alive():
                        elapsed_checks += 1
                        elapsed_time = elapsed_checks * check_interval
                        
                        # ë©”ëª¨ë¦¬ ìƒíƒœ ì²´í¬
                        try:
                            mem = psutil.virtual_memory()
                            print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ë¡œë”© ì§„í–‰ì¤‘... {elapsed_time}ì´ˆ ê²½ê³¼")
                            print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ë©”ëª¨ë¦¬ ìƒíƒœ: {mem.percent}% ì‚¬ìš©, {mem.available/1024**3:.1f}GB ì‚¬ìš©ê°€ëŠ¥")
                            
                            # í”„ë¡œì„¸ìŠ¤ë³„ ë©”ëª¨ë¦¬ í™•ì¸
                            process = psutil.Process()
                            proc_mem_mb = process.memory_info().rss / 1024 / 1024
                            print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬: {proc_mem_mb:.1f}MB")
                            
                        except Exception as mem_e:
                            print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ë©”ëª¨ë¦¬ ì²´í¬ ì‹¤íŒ¨: {mem_e}")
                        
                        # íƒ€ì„ì•„ì›ƒ ì²´í¬ (ì„±ê³µ ì‹ í˜¸ê°€ ë„ì°©í•˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ)
                        if elapsed_time >= timeout_seconds:
                            # ë§ˆì§€ë§‰ìœ¼ë¡œ í•œ ë²ˆ ë” ì„±ê³µ ì‹ í˜¸ í™•ì¸
                            if not loading_result.empty():
                                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âœ… íƒ€ì„ì•„ì›ƒ ì§ì „ ë¡œë”© ì„±ê³µ ê°ì§€!")
                                break
                            
                            print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âŒ ëª¨ë¸ ë¡œë”© íƒ€ì„ì•„ì›ƒ ({timeout_seconds}ì´ˆ)")
                            
                            # ìŠ¤ë ˆë“œ ê°•ì œ ì¢…ë£Œ ì‹œë„ (ë°°ê²½ì—ì„œ ê³„ì† ì‹¤í–‰ë˜ë„ë¡ í—ˆìš©)
                            loading_error.put(TimeoutError(f"ëª¨ë¸ ë¡œë”©ì´ {timeout_seconds}ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ë°°ê²½ì—ì„œ ê³„ì† ì‹œë„ ì¤‘..."))
                            break
                
                # ê²°ê³¼ í™•ì¸ (ë™ê¸°í™” ê°•í™”)
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ë¡œë”© ìŠ¤ë ˆë“œ ì¢…ë£Œ í›„ ê²°ê³¼ í™•ì¸")
                
                # ì„±ê³µ ê²°ê³¼ ìš°ì„  ì²´í¬
                if not loading_result.empty():
                    model = loading_result.get()
                    print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - 4/5: ëª¨ë¸ ë¡œë”© ì„±ê³µ, í›„ì²˜ë¦¬ ì‹œì‘")
                    
                    # ì˜¤ë¥˜ íì— ë‚¨ì•„ìˆëŠ” ë©”ì‹œì§€ ì •ë¦¬ (íƒ€ì„ì•„ì›ƒ ê²½ê³  ë“±)
                    while not loading_error.empty():
                        warning_msg = loading_error.get()
                        print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ë¬´ì‹œë˜ëŠ” ê²½ê³ : {warning_msg}")
                        
                elif not loading_error.empty():
                    error = loading_error.get()
                    print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë¸ ë¡œë”© ì˜¤ë¥˜ ë°œìƒ: {error}")
                    raise error
                else:
                    # ë‘˜ ë‹¤ ë¹„ì–´ìˆëŠ” ê²½ìš° (ë¹„ì •ìƒ ìƒí™©)
                    print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âš ï¸ ë¹„ì •ìƒ ìƒí™©: ë¡œë”© ê²°ê³¼ë„ ì˜¤ë¥˜ë„ ì—†ìŒ")
                    raise Exception("ëª¨ë¸ ë¡œë”© ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                
                model_load_time = time.time() - model_start
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - 5/5: ëª¨ë¸ ë¡œë”© í›„ì²˜ë¦¬ ì™„ë£Œ")
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âœ… ì‹¤ì œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name} ({model_load_time:.1f}ì´ˆ)")
                
                # ëª¨ë¸ ìƒíƒœ ê²€ì¦
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë¸ ìƒíƒœ ê²€ì¦:")
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} -   ëª¨ë¸ í´ë˜ìŠ¤: {model.__class__.__name__}")
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} -   ëª¨ë¸ ìƒíƒœ: {'eval' if not model.training else 'train'}")
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} -   ëª¨ë¸ ìƒ€ê³  ëª¨ë“œ: {next(model.parameters()).requires_grad}")
                
                # ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
                param_count = sum(p.numel() for p in model.parameters())
                param_size_mb = param_count * 4 / 1024 / 1024  # float32 = 4bytes
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë¸ íŒŒë¼ë¯¸í„°: {param_count:,}ê°œ ({param_size_mb:.1f}MB)")
                
                # ëª¨ë¸ ë ˆì´ì–´ êµ¬ì¡° ê°„ëµ ë¶„ì„
                layer_count = 0
                for name, module in model.named_modules():
                    layer_count += 1
                    if layer_count <= 5:  # ì²˜ìŒ 5ê°œ ë ˆì´ì–´ë§Œ ìƒì„¸ ì •ë³´
                        print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} -   ë ˆì´ì–´ {layer_count}: {name} ({module.__class__.__name__})")
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ì´ ë ˆì´ì–´ ìˆ˜: {layer_count}")
                
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - í† í¬ë‚˜ì´ì € ë¡œë”© ì‹œì‘: {model_name}")
                tokenizer_start = time.time()
                
                tokenizer = AutoTokenizer.from_pretrained(
                    actual_model_path, 
                    local_files_only=True,
                    trust_remote_code=True
                )
                
                tokenizer_load_time = time.time() - tokenizer_start
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ: {model_name} ({tokenizer_load_time:.1f}ì´ˆ)")
                
                # í† í¬ë‚˜ì´ì € ì •ë³´ í™•ì¸
                vocab_size = tokenizer.vocab_size if hasattr(tokenizer, 'vocab_size') else 'Unknown'
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - í† í¬ë‚˜ì´ì € ì–´íœ˜ í¬ê¸°: {vocab_size}")
                
                # í†µí•© ë””ë°”ì´ìŠ¤ ê´€ë¦¬ìë¡œ ì¼ê´€ì„± ë³´ì¥
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ë””ë°”ì´ìŠ¤ ì¼ê´€ì„± ë³´ì¥ ì‹œì‘: {model_name}")
                device_start = time.time()
                
                model, tokenizer = device_manager.ensure_device_consistency(model, tokenizer)
                model.eval()
                
                device_time = time.time() - device_start
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âœ… ë””ë°”ì´ìŠ¤ ì¼ê´€ì„± ë³´ì¥ ì™„ë£Œ: {model_name} ({device_time:.1f}ì´ˆ)")
                
                # ìµœì¢… ëª¨ë¸ ìƒíƒœ í™•ì¸
                model_device = next(model.parameters()).device
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ìµœì¢… ëª¨ë¸ ë””ë°”ì´ìŠ¤: {model_device}")
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë¸ í‰ê°€ ëª¨ë“œ: {not model.training}")
                
                # ë””ë°”ì´ìŠ¤ ì¼ê´€ì„± ìµœì¢… ê²€ì¦
                devices = set(param.device for param in model.parameters())
                if len(devices) == 1:
                    print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âœ… ë””ë°”ì´ìŠ¤ ì¼ê´€ì„± í™•ì¸: {list(devices)[0]}")
                else:
                    print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âš ï¸ ë””ë°”ì´ìŠ¤ ë¶ˆì¼ì¹˜ ê°ì§€: {devices}")
                
                load_time = time.time() - load_start
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ì‹¤ì œ ëª¨ë¸ ë¡œë”© ì´ ì‹œê°„: {load_time:.1f}ì´ˆ")
                
                # ë¡œë”© ì„±ê³µ ë©”ì‹œì§€
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ğŸ‰ {model_name} ëª¨ë¸ ë¡œë”© ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
                
                profiler.print_detailed_report()
                return model, tokenizer, load_time
                
            except TimeoutError as te:
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - â° ëª¨ë¸ ë¡œë”© íƒ€ì„ì•„ì›ƒ: {te}")
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - í•´ê²°ë°©ì•ˆ:")
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} -   1. ë” í° íƒ€ì„ì•„ì›ƒ ì„¤ì •")
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} -   2. ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš© ê³ ë ¤")
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} -   3. GPU ë©”ëª¨ë¦¬ ìµœì í™”")
                raise
            except Exception as e:
                import traceback
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - âŒ ì‹¤ì œ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ìƒì„¸ ì˜¤ë¥˜:")
                traceback.print_exc()
                
                # ë©”ëª¨ë¦¬ ìƒíƒœ ì¬í™•ì¸
                try:
                    mem = psutil.virtual_memory()
                    print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ì˜¤ë¥˜ ì‹œì  ë©”ëª¨ë¦¬ - ì‚¬ìš©ë¥ : {mem.percent}%, ì‚¬ìš©ê°€ëŠ¥: {mem.available/1024**3:.1f}GB")
                except:
                    pass
                
                # ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ë””ë²„ê¹… ì •ë³´:")
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} -   ëª¨ë¸ ê²½ë¡œ: {actual_model_path}")
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} -   ë¡œì»¬ íŒŒì¼ ì „ìš©: True")
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} -   ì‹ ë¢° ì½”ë“œ: True")
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} -   í˜•ë³€í™˜: torch.float32")
                    
                raise
        
        try:
            print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - _load_model_sync ì‹œì‘: {model_name}, {model_path}")
            
            # ëª¨ë¸ ì •ë³´ ì´ˆê¸°í™”
            self.models[model_name] = ModelInfo(
                name=model_name, 
                path=model_path, 
                status="loading"
            )
            
            print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë¸ ì •ë³´ ì´ˆê¸°í™”ë¨: {model_name}")
            self._notify_callbacks(model_name, "loading_started", {})
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • ì‹œì‘
            process = psutil.Process()
            mem_before = process.memory_info().rss
            print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ë©”ëª¨ë¦¬ ì¸¡ì • ì‹œì‘: {model_name}")
            
            # HuggingFace ëª¨ë¸ IDì¸ì§€ í™•ì¸í•˜ê³  ë‹¤ìš´ë¡œë“œ
            actual_model_path = model_path
            if self._is_huggingface_model_id(model_path):
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - HuggingFace ëª¨ë¸ ID ê°ì§€: {model_name}")
                self._notify_callbacks(model_name, "downloading", {'model_id': model_path})
                actual_model_path = self._download_huggingface_model(model_path)
                self.models[model_name].path = actual_model_path  # ì‹¤ì œ ê²½ë¡œë¡œ ì—…ë°ì´íŠ¸
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë¸ ë‹¤ìš´ë¡œë“œ/ìºì‹œ í™•ì¸ ì™„ë£¼: {model_name}")
            
            # ëª¨ë¸ ë¶„ì„ - ì„±ëŠ¥ìƒ ì´ìœ ë¡œ ê°„ì†Œí™”
            print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë¸ ë¶„ì„ ì‹œì‘: {model_name}")
            try:
                # ë¹ ë¥¸ ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰ (ì „ì²´ ë¶„ì„ì€ ìŠ¤í‚µ)
                analysis = {"model_summary": {"supported_tasks": ["feature-extraction"]}}
                self.models[model_name].config_analysis = analysis
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë¸ ë¶„ì„ ì™„ë£Œ (ê°„ì†Œí™”): {model_name}")
            except Exception as e:
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë¸ ë¶„ì„ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
                self.models[model_name].config_analysis = {"model_summary": {"supported_tasks": ["feature-extraction"]}}
            
            # ë²”ìš©ì ì¸ transformers ëª¨ë¸ ë¡œë“œ
            print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - transformers ì„í¬íŠ¸ ì‹œì‘: {model_name}")
            from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, AutoConfig
            print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - transformers ì„í¬íŠ¸ ì™„ë£Œ: {model_name}")
            
            # ìµœì  ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ
            print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ë””ë°”ì´ìŠ¤ ì„ íƒ ì‹œì‘: {model_name}")
            device = optimizer.get_optimal_device()
            print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ìë™ ì„ íƒëœ ë””ë°”ì´ìŠ¤: {device}")
            
            # ë©”ëª¨ë¦¬ ìƒíƒœ ì²´í¬
            print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ë©”ëª¨ë¦¬ ì²´í¬ ì‹œì‘: {model_name}")
            memory_info = self.get_memory_info()
            available_memory_gb = memory_info['system_memory']['available'] / (1024**3)
            print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: {available_memory_gb:.1f}GB")
            print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ë””ë°”ì´ìŠ¤ ì„¤ì •: {device} (Streamlit ì•ˆì •ì„±ì„ ìœ„í•´ CPU ê°•ì œ)")
            
            # ëª¨ë¸ íƒ€ì… ìë™ ê°ì§€
            if "bge" in model_name.lower() or "embedding" in model_name.lower():
                is_classification_model = False
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {model_name} ì„ë² ë”© ëª¨ë¸ë¡œ ì„¤ì •")
            elif "sentiment" in model_name.lower() or "classification" in model_name.lower():
                is_classification_model = True
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {model_name} ë¶„ë¥˜ ëª¨ë¸ë¡œ ì„¤ì •")
            else:
                is_classification_model = False
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {model_name} ê¸°ë³¸ ëª¨ë¸ë¡œ ì„¤ì •")
            
            print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë¸ ë¡œë”© ì‹œì‘: classification={is_classification_model}")
            
            # ì§ì ‘ ëª¨ë¸ ë¡œë”© (ìºì‹œ ìš°íšŒí•˜ì—¬ ì•ˆì •ì„± í™•ë³´)
            print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ì§ì ‘ ëª¨ë¸ ë¡œë”© ì‹œì‘: {model_name}")
            
            try:
                # ëª¨ë¸ ë¡œë”©
                result = load_model_with_transformers(actual_model_path, device)
                
                if len(result) == 3:
                    model, tokenizer, load_time = result
                    print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë¸ ë¡œë”© ì„±ê³µ: {load_time:.1f}ì´ˆ")
                else:
                    raise ValueError("ëª¨ë¸ ë¡œë”© ê²°ê³¼ í˜•ì‹ ì˜¤ë¥˜")
                
            except Exception as e:
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
                raise
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
            mem_after = process.memory_info().rss
            memory_usage = (mem_after - mem_before) / 1024 / 1024  # MB
            
            # ëª¨ë¸ ì •ë³´ ì—…ë°ì´íŠ¸
            self.models[model_name].model = model
            self.models[model_name].tokenizer = tokenizer
            self.models[model_name].memory_usage = memory_usage
            self.models[model_name].load_time = datetime.now()
            self.models[model_name].status = "loaded"
            
            success_data = {
                'memory_usage': memory_usage,
                'load_time': self.models[model_name].load_time,
                'analysis': analysis['model_summary'],
                'original_path': model_path,
                'actual_path': actual_model_path
            }
            
            self._notify_callbacks(model_name, "loading_success", success_data)
            
            if callback:
                callback(model_name, True, f"Model loaded successfully. Memory usage: {memory_usage:.2f} MB")
        
        except TimeoutError as e:
            error_msg = str(e)
            print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë¸ ë¡œë”© íƒ€ì„ì•„ì›ƒ: {error_msg}")
            
            if model_name in self.models:
                self.models[model_name].status = "error"
                self.models[model_name].error_message = error_msg
            
            self._notify_callbacks(model_name, "loading_error", {'error': error_msg})
            
            if callback:
                callback(model_name, False, error_msg)
                
        except Exception as e:
            error_msg = str(e)
            print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë¸ ë¡œë”© ì˜¤ë¥˜: {error_msg}")
            import traceback
            traceback.print_exc()
            
            if model_name in self.models:
                self.models[model_name].status = "error"
                self.models[model_name].error_message = error_msg
            
            self._notify_callbacks(model_name, "loading_error", {'error': error_msg})
            
            if callback:
                callback(model_name, False, error_msg)
        
        finally:
            elapsed = time.time() - start_time
            print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ëª¨ë¸ ë¡œë”© ì´ ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ")
    
    def unload_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        if model_name in self.models:
            model_info = self.models[model_name]
            
            try:
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                if model_info.model:
                    del model_info.model
                if model_info.tokenizer:
                    del model_info.tokenizer
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                # ìƒíƒœ ì—…ë°ì´íŠ¸
                model_info.status = "unloaded"
                model_info.model = None
                model_info.tokenizer = None
                model_info.memory_usage = 0.0
                
                self._notify_callbacks(model_name, "unloaded", {})
                
                return True
                
            except Exception as e:
                print(f"[DEBUG] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - Error unloading model {model_name}: {e}")
                return False
        
        return False
    
    def get_loaded_models(self) -> List[str]:
        """ë¡œë“œëœ ëª¨ë¸ ëª©ë¡"""
        return [name for name, info in self.models.items() if info.status == "loaded"]
    
    def get_model_info(self, model_name: str) -> Optional[ModelInfo]:
        """íŠ¹ì • ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        return self.models.get(model_name)
    
    def get_all_models_status(self) -> Dict:
        """ëª¨ë“  ëª¨ë¸ ìƒíƒœ ì •ë³´"""
        return {
            name: {
                'status': info.status,
                'memory_usage': info.memory_usage,
                'load_time': info.load_time.isoformat() if info.load_time else None,
                'path': info.path,
                'error_message': info.error_message,
                'config_analysis': info.config_analysis
            }
            for name, info in self.models.items()
        }
    
    def get_model_for_inference(self, model_name: str) -> Optional[tuple]:
        """ì¶”ë¡ ìš© ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë°˜í™˜"""
        if model_name in self.models and self.models[model_name].status == "loaded":
            model_info = self.models[model_name]
            return model_info.model, model_info.tokenizer
        return None
    
    def get_system_summary(self) -> Dict:
        """ì‹œìŠ¤í…œ ìš”ì•½ ì •ë³´"""
        loaded_count = len(self.get_loaded_models())
        total_memory = sum(info.memory_usage for info in self.models.values() if info.status == "loaded")
        
        memory_info = self.get_memory_info()
        
        return {
            'loaded_models_count': loaded_count,
            'total_models_count': len(self.models),
            'total_memory_usage_mb': total_memory,
            'system_memory_info': memory_info,
            'models_by_status': {
                'loaded': len([m for m in self.models.values() if m.status == "loaded"]),
                'loading': len([m for m in self.models.values() if m.status == "loading"]),
                'error': len([m for m in self.models.values() if m.status == "error"]),
                'unloaded': len([m for m in self.models.values() if m.status == "unloaded"])
            }
        }
    
    def remove_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ì™„ì „ ì œê±°"""
        if model_name in self.models:
            # ë¨¼ì € ì–¸ë¡œë“œ
            if self.models[model_name].status == "loaded":
                self.unload_model(model_name)
            
            # ëª¨ë¸ ì •ë³´ ì œê±°
            del self.models[model_name]
            
            # ë¡œë”© ë½ ì œê±°
            if model_name in self.loading_locks:
                del self.loading_locks[model_name]
            
            self._notify_callbacks(model_name, "removed", {})
            return True
        
        return False
    
    def clear_all_models(self):
        """ëª¨ë“  ëª¨ë¸ ì •ë¦¬"""
        model_names = list(self.models.keys())
        for model_name in model_names:
            self.remove_model(model_name)
    
    def get_available_tasks(self, model_name: str) -> List[str]:
        """ëª¨ë¸ì´ ì§€ì›í•˜ëŠ” íƒœìŠ¤í¬ ëª©ë¡"""
        if model_name in self.models:
            analysis = self.models[model_name].config_analysis
            if analysis and 'model_summary' in analysis:
                return analysis['model_summary'].get('supported_tasks', [])
        return []
    
    def export_models_info(self) -> Dict:
        """ëª¨ë¸ ì •ë³´ ë‚´ë³´ë‚´ê¸°"""
        export_data = {
            'timestamp': datetime.now().isoformat(),
            'system_info': self.get_system_summary(),
            'models': {}
        }
        
        for name, info in self.models.items():
            export_data['models'][name] = {
                'name': info.name,
                'path': info.path,
                'status': info.status,
                'memory_usage': info.memory_usage,
                'load_time': info.load_time.isoformat() if info.load_time else None,
                'config_analysis': info.config_analysis
            }
        
        return export_data
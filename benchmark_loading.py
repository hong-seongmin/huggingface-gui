#!/usr/bin/env python3
"""
Ultra-Fast ëª¨ë¸ ë¡œë”© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸
"""
import time
import psutil
import torch
from model_manager import MultiModelManager
from model_cache import model_cache
from fast_tensor_loader import fast_loader
from parallel_model_loader import parallel_loader
from cpu_optimizer import cpu_optimizer

def benchmark_ultra_fast_loading():
    """Ultra-Fast ëª¨ë¸ ë¡œë”© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    print("=" * 70)
    print("ğŸš€ ULTRA-FAST ëª¨ë¸ ë¡œë”© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
    print("=" * 70)
    
    # ì‹œìŠ¤í…œ ì •ë³´
    print("ğŸ“Š ì‹œìŠ¤í…œ ì •ë³´:")
    print(f"   CPU ì½”ì–´ ìˆ˜: {torch.get_num_threads()}")
    print(f"   CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
    
    memory = psutil.virtual_memory()
    print(f"   ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {memory.total / (1024**3):.1f}GB")
    print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: {memory.available / (1024**3):.1f}GB")
    
    # ìµœì í™” ì •ë³´
    opt_info = cpu_optimizer.get_optimization_info()
    print(f"   Intel Extension: {opt_info['intel_extension_available']}")
    print(f"   torch.compile: {opt_info['torch_compile_available']}")
    print(f"   PyTorch ë²„ì „: {opt_info['torch_version']}")
    print()
    
    # í…ŒìŠ¤íŠ¸í•  ëª¨ë¸
    model_path = "tabularisai/multilingual-sentiment-analysis"
    model_name = "ultra-fast-benchmark"
    
    # ëª¨ë¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    manager = MultiModelManager()
    
    # ìºì‹œ ì •ë¦¬
    model_cache.clear_cache()
    
    print("ğŸ”¥ 1ë‹¨ê³„: Ultra-Fast ì²« ë²ˆì§¸ ë¡œë”© (ìºì‹œ ì—†ìŒ)")
    print("-" * 50)
    
    # ì²« ë²ˆì§¸ ë¡œë”© ì‹œê°„ ì¸¡ì •
    start_time = time.time()
    memory_before = psutil.virtual_memory().used
    
    def ultra_fast_callback(name, success, message):
        if success:
            elapsed = time.time() - start_time
            memory_after = psutil.virtual_memory().used
            memory_used = (memory_after - memory_before) / (1024**2)  # MB
            
            print(f"âœ… Ultra-Fast ì²« ë²ˆì§¸ ë¡œë”© ì™„ë£Œ:")
            print(f"   âš¡ ì‹œê°„: {elapsed:.1f}ì´ˆ")
            print(f"   ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used:.1f}MB")
            
            # ì„±ëŠ¥ ë“±ê¸‰ íŒì •
            if elapsed < 30:
                print("   ğŸ† ë“±ê¸‰: ULTRA-FAST (30ì´ˆ ë¯¸ë§Œ)")
            elif elapsed < 60:
                print("   ğŸ¥‡ ë“±ê¸‰: FAST (60ì´ˆ ë¯¸ë§Œ)")
            elif elapsed < 120:
                print("   ğŸ¥ˆ ë“±ê¸‰: GOOD (120ì´ˆ ë¯¸ë§Œ)")
            else:
                print("   ğŸ¥‰ ë“±ê¸‰: NEEDS IMPROVEMENT")
            print()
            
            # ì§ì ‘ í…ì„œ ë¡œë”© í…ŒìŠ¤íŠ¸
            print("ğŸ§ª 2ë‹¨ê³„: ì§ì ‘ í…ì„œ ë¡œë”© í…ŒìŠ¤íŠ¸")
            print("-" * 50)
            
            test_direct_loading(model_path)
            
            # ë³‘ë ¬ ë¡œë”© í…ŒìŠ¤íŠ¸
            print("ğŸ”„ 3ë‹¨ê³„: ë³‘ë ¬ ë¡œë”© í…ŒìŠ¤íŠ¸")
            print("-" * 50)
            
            test_parallel_loading(model_path)
            
            # ìºì‹œ í…ŒìŠ¤íŠ¸
            print("ğŸ’¾ 4ë‹¨ê³„: ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
            print("-" * 50)
            
            test_cache_performance(manager, name, model_path, elapsed)
            
        else:
            print(f"âŒ Ultra-Fast ë¡œë”© ì‹¤íŒ¨: {message}")
    
    # Ultra-Fast ë¡œë”© ì‹œì‘
    manager.load_model_async(model_name, model_path, ultra_fast_callback)

def test_direct_loading(model_path):
    """ì§ì ‘ í…ì„œ ë¡œë”© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    try:
        start_time = time.time()
        
        # ì§ì ‘ í…ì„œ ë¡œë”© í…ŒìŠ¤íŠ¸
        model, model_time = fast_loader.load_model_ultra_fast(model_path, "cpu")
        tokenizer, tokenizer_time = fast_loader.load_tokenizer_fast(model_path)
        
        total_time = time.time() - start_time
        
        if model and tokenizer:
            print(f"   âœ… ì§ì ‘ ë¡œë”© ì„±ê³µ:")
            print(f"      ëª¨ë¸: {model_time:.1f}ì´ˆ")
            print(f"      í† í¬ë‚˜ì´ì €: {tokenizer_time:.1f}ì´ˆ")
            print(f"      ì´ ì‹œê°„: {total_time:.1f}ì´ˆ")
            
            # ëª¨ë¸ ê²€ì¦
            valid = fast_loader.validate_model(model, tokenizer)
            print(f"      ê²€ì¦: {'âœ… ì„±ê³µ' if valid else 'âŒ ì‹¤íŒ¨'}")
        else:
            print("   âŒ ì§ì ‘ ë¡œë”© ì‹¤íŒ¨")
            
    except Exception as e:
        print(f"   âŒ ì§ì ‘ ë¡œë”© ì˜¤ë¥˜: {e}")
    
    print()

def test_parallel_loading(model_path):
    """ë³‘ë ¬ ë¡œë”© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    try:
        start_time = time.time()
        
        # ë³‘ë ¬ ë¡œë”© í…ŒìŠ¤íŠ¸
        model, tokenizer, load_time = parallel_loader.load_model_and_tokenizer_parallel(model_path, "cpu")
        
        if model and tokenizer:
            print(f"   âœ… ë³‘ë ¬ ë¡œë”© ì„±ê³µ:")
            print(f"      ë¡œë”© ì‹œê°„: {load_time:.1f}ì´ˆ")
            
            # CPU ìµœì í™” í…ŒìŠ¤íŠ¸
            opt_start = time.time()
            optimized_model = cpu_optimizer.optimize_model_for_cpu(model, optimize_level=3)
            opt_time = time.time() - opt_start
            
            print(f"      ìµœì í™”: {opt_time:.1f}ì´ˆ")
            
            # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
            benchmark_results = cpu_optimizer.benchmark_model_performance(optimized_model, num_runs=5)
            if "average_time" in benchmark_results:
                print(f"      ì¶”ë¡  ì†ë„: {benchmark_results['average_time']:.4f}ì´ˆ")
                print(f"      ì²˜ë¦¬ëŸ‰: {benchmark_results['throughput']:.1f} ì¶”ë¡ /ì´ˆ")
        else:
            print("   âŒ ë³‘ë ¬ ë¡œë”© ì‹¤íŒ¨")
            
    except Exception as e:
        print(f"   âŒ ë³‘ë ¬ ë¡œë”© ì˜¤ë¥˜: {e}")
    
    print()

def test_cache_performance(manager, model_name, model_path, first_load_time):
    """ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    try:
        # ëª¨ë¸ ì–¸ë¡œë“œ
        manager.unload_model(model_name)
        
        # ìºì‹œì—ì„œ ë‘ ë²ˆì§¸ ë¡œë”©
        second_start = time.time()
        
        def cache_callback(name, success, message):
            if success:
                second_elapsed = time.time() - second_start
                speedup = first_load_time / second_elapsed if second_elapsed > 0 else float('inf')
                
                print(f"   âœ… ìºì‹œ ë¡œë”© ì„±ê³µ:")
                print(f"      ì‹œê°„: {second_elapsed:.1f}ì´ˆ")
                print(f"      ê°€ì†: {speedup:.1f}ë°°")
                
                # ìºì‹œ í†µê³„
                cache_stats = model_cache.get_cache_stats()
                print(f"      ìºì‹œ í¬ê¸°: {cache_stats['total_cache_size_gb']:.2f}GB")
                print(f"      íˆíŠ¸ìœ¨: {cache_stats['cache_hit_rate']:.1f}%")
                
                # ìµœì¢… ê²°ê³¼
                print()
                print("ğŸ¯ ìµœì¢… ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼")
                print("-" * 50)
                print(f"   ì²« ë²ˆì§¸ ë¡œë”©: {first_load_time:.1f}ì´ˆ")
                print(f"   ìºì‹œ ë¡œë”©: {second_elapsed:.1f}ì´ˆ")
                print(f"   ì„±ëŠ¥ í–¥ìƒ: {speedup:.1f}ë°°")
                
                # ëª©í‘œ ë‹¬ì„±ë„
                target_time = 60  # ëª©í‘œ: 60ì´ˆ ì´í•˜
                if first_load_time <= target_time:
                    print(f"   ğŸ¯ ëª©í‘œ ë‹¬ì„±! ({target_time}ì´ˆ ì´í•˜)")
                    improvement = (338 - first_load_time) / 338 * 100  # ê¸°ì¡´ 338ì´ˆ ëŒ€ë¹„
                    print(f"   ğŸ“ˆ ê°œì„ ìœ¨: {improvement:.1f}% (ê¸°ì¡´ 338ì´ˆ ëŒ€ë¹„)")
                else:
                    remaining = first_load_time - target_time
                    print(f"   â° ëª©í‘œê¹Œì§€: {remaining:.1f}ì´ˆ ì¶”ê°€ ê°œì„  í•„ìš”")
                
                print("=" * 70)
            else:
                print(f"   âŒ ìºì‹œ ë¡œë”© ì‹¤íŒ¨: {message}")
        
        # ë‘ ë²ˆì§¸ ë¡œë”© ì‹œì‘
        manager.load_model_async(f"{model_name}-cache", model_path, cache_callback)
        
    except Exception as e:
        print(f"   âŒ ìºì‹œ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    benchmark_ultra_fast_loading()
    
    # ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ ëŒ€ê¸°
    import threading
    
    # ëª¨ë“  ë¡œë”© ìŠ¤ë ˆë“œê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
    for thread in threading.enumerate():
        if thread.name.startswith("ModelLoad-"):
            thread.join()
    
    print("ğŸ Ultra-Fast ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
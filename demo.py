#!/usr/bin/env python3
"""
Hugging Face GUI Demo Script
ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ìƒˆë¡œìš´ ê¸°ëŠ¥ë“¤ì„ í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ëŠ” ë°ëª¨ì…ë‹ˆë‹¤.
"""

import time
import threading
from model_manager import MultiModelManager
from system_monitor import SystemMonitor
from fastapi_server import FastAPIServer
from model_analyzer import ComprehensiveModelAnalyzer

def demo_model_analyzer():
    """ëª¨ë¸ ë¶„ì„ ê¸°ëŠ¥ ë°ëª¨"""
    print("ğŸ” ëª¨ë¸ ë¶„ì„ ê¸°ëŠ¥ ë°ëª¨")
    print("=" * 50)
    
    analyzer = ComprehensiveModelAnalyzer()
    
    # ì˜ˆì‹œ ëª¨ë¸ ê²½ë¡œ (ì‹¤ì œ ëª¨ë¸ì´ ìˆëŠ” ê²½ë¡œë¡œ ë³€ê²½í•˜ì„¸ìš”)
    model_paths = [
        "bert-base-uncased",  # HuggingFace Hub ëª¨ë¸
        "gpt2",               # HuggingFace Hub ëª¨ë¸
        # "/path/to/local/model"  # ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ
    ]
    
    for model_path in model_paths:
        print(f"\nğŸ“Š ë¶„ì„ ì¤‘: {model_path}")
        try:
            analysis = analyzer.analyze_model_directory(model_path)
            
            print(f"  ëª¨ë¸ íƒ€ì…: {analysis['model_summary'].get('model_type', 'unknown')}")
            print(f"  íŒŒë¼ë¯¸í„° ìˆ˜: {analysis['model_summary'].get('total_parameters', 0):,}")
            print(f"  ì§€ì› íƒœìŠ¤í¬: {', '.join(analysis['model_summary'].get('supported_tasks', []))}")
            print(f"  ë°œê²¬ëœ íŒŒì¼: {len(analysis['files_found'])}ê°œ")
            print(f"  ëˆ„ë½ëœ íŒŒì¼: {len(analysis['files_missing'])}ê°œ")
            
            if analysis['recommendations']:
                print(f"  ê¶Œì¥ì‚¬í•­: {len(analysis['recommendations'])}ê°œ")
                for rec in analysis['recommendations'][:2]:
                    print(f"    - {rec}")
                    
        except Exception as e:
            print(f"  âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    print("\nâœ… ëª¨ë¸ ë¶„ì„ ë°ëª¨ ì™„ë£Œ")

def demo_system_monitor():
    """ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ê¸°ëŠ¥ ë°ëª¨"""
    print("\nğŸ–¥ï¸ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ê¸°ëŠ¥ ë°ëª¨")
    print("=" * 50)
    
    monitor = SystemMonitor(update_interval=2.0)
    
    # ëª¨ë‹ˆí„°ë§ ë°ì´í„° ìˆ˜ì§‘ ì½œë°±
    def print_system_info(data):
        print(f"â° {data['timestamp'].strftime('%H:%M:%S')} - "
              f"CPU: {data['cpu']['percent']:.1f}%, "
              f"Memory: {data['memory']['percent']:.1f}%, "
              f"GPU: {len(data['gpu'])} device(s)")
    
    monitor.add_callback(print_system_info)
    
    print("ğŸš€ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (5ì´ˆê°„ ì‹¤í–‰)")
    monitor.start_monitoring()
    
    # 5ì´ˆ ë™ì•ˆ ëª¨ë‹ˆí„°ë§
    time.sleep(5)
    
    print("â¹ï¸ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
    monitor.stop_monitoring()
    
    # íˆìŠ¤í† ë¦¬ ë°ì´í„° í™•ì¸
    history = monitor.get_history()
    print(f"ğŸ“Š ìˆ˜ì§‘ëœ ë°ì´í„° í¬ì¸íŠ¸: CPU {len(history['cpu'])}ê°œ, Memory {len(history['memory'])}ê°œ")
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    system_info = monitor.get_system_info()
    print(f"ğŸ’» ì‹œìŠ¤í…œ ì •ë³´:")
    print(f"  - CPU ì½”ì–´: {system_info['cpu_count']}ê°œ")
    print(f"  - ë©”ëª¨ë¦¬: {system_info['memory_total'] / (1024**3):.1f} GB")
    print(f"  - GPU: {system_info['gpu_count']}ê°œ")
    
    print("âœ… ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ë°ëª¨ ì™„ë£Œ")

def demo_model_manager():
    """ëª¨ë¸ ê´€ë¦¬ ê¸°ëŠ¥ ë°ëª¨"""
    print("\nğŸ¤– ëª¨ë¸ ê´€ë¦¬ ê¸°ëŠ¥ ë°ëª¨")
    print("=" * 50)
    
    manager = MultiModelManager()
    
    # ëª¨ë¸ ìƒíƒœ ë³€ê²½ ì½œë°±
    def model_callback(model_name, event_type, data):
        print(f"ğŸ“¢ {model_name}: {event_type}")
        if event_type == "loading_success":
            print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {data['memory_usage']:.1f} MB")
    
    manager.add_callback(model_callback)
    
    # ê°„ë‹¨í•œ ëª¨ë¸ ë¶„ì„
    print("ğŸ” ëª¨ë¸ ë¶„ì„ í…ŒìŠ¤íŠ¸")
    try:
        analysis = manager.analyze_model("gpt2")
        print(f"âœ… GPT-2 ë¶„ì„ ì™„ë£Œ: {analysis['model_summary'].get('model_type', 'unknown')}")
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    # ì‹œìŠ¤í…œ ìš”ì•½ ì¶œë ¥
    summary = manager.get_system_summary()
    print(f"\nğŸ“Š ì‹œìŠ¤í…œ ìš”ì•½:")
    print(f"  - ë¡œë“œëœ ëª¨ë¸: {summary['loaded_models_count']}ê°œ")
    print(f"  - ì´ ëª¨ë¸: {summary['total_models_count']}ê°œ")
    print(f"  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {summary['total_memory_usage_mb']:.1f} MB")
    
    print("âœ… ëª¨ë¸ ê´€ë¦¬ ë°ëª¨ ì™„ë£Œ")

def demo_fastapi_server():
    """FastAPI ì„œë²„ ê¸°ëŠ¥ ë°ëª¨"""
    print("\nğŸš€ FastAPI ì„œë²„ ê¸°ëŠ¥ ë°ëª¨")
    print("=" * 50)
    
    manager = MultiModelManager()
    server = FastAPIServer(manager)
    
    # ì„œë²„ ì •ë³´ ì¶œë ¥
    server_info = server.get_server_info()
    print(f"ğŸŒ ì„œë²„ ì •ë³´:")
    print(f"  - URL: {server_info['url']}")
    print(f"  - ë¬¸ì„œ: {server_info['docs_url']}")
    print(f"  - ìƒíƒœ: {'ì‹¤í–‰ ì¤‘' if server_info['running'] else 'ì¤‘ì§€ë¨'}")
    
    # ì—”ë“œí¬ì¸íŠ¸ ëª©ë¡
    endpoints = server.get_available_endpoints()
    print(f"\nğŸ“¡ ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ë“œí¬ì¸íŠ¸ ({len(endpoints)}ê°œ):")
    for ep in endpoints[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
        print(f"  - {ep['methods'][0]} {ep['path']}")
    
    print("ğŸ’¡ ì‹¤ì œ ì„œë²„ ì‹œì‘ì€ GUIì—ì„œ ìˆ˜í–‰í•˜ì„¸ìš”.")
    print("âœ… FastAPI ì„œë²„ ë°ëª¨ ì™„ë£Œ")

def main():
    """ë©”ì¸ ë°ëª¨ í•¨ìˆ˜"""
    print("ğŸ‰ Hugging Face GUI ë°ëª¨ ì‹œì‘")
    print("=" * 60)
    
    try:
        # ê° ê¸°ëŠ¥ ë°ëª¨ ì‹¤í–‰
        demo_model_analyzer()
        demo_system_monitor()
        demo_model_manager()
        demo_fastapi_server()
        
        print("\nğŸŠ ëª¨ë“  ë°ëª¨ ì™„ë£Œ!")
        print("\nğŸ“– ì‚¬ìš© ë°©ë²•:")
        print("  - Streamlit ë²„ì „: streamlit run app_enhanced.py")
        print("  - CustomTkinter ë²„ì „: python run_enhanced.py")
        print("  - ê¸°ì¡´ ë²„ì „ë„ ì—¬ì „íˆ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ ë°ëª¨ê°€ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ë°ëª¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()